{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#bem-vindoa-deep-learning-20251","title":"Bem-vindo(a) \u2014 Deep Learning (2025.1)","text":"<p>Autor: Caio Ortega Boa Turma: 2025.2 Reposit\u00f3rio: Este site re\u00fane todo o conte\u00fado produzido por Caio Ortega Boa na disciplina de Deep Learning \u2014 exerc\u00edcios, c\u00f3digos, an\u00e1lises, gr\u00e1ficos e anota\u00e7\u00f5es.</p>"},{"location":"#sobre-esta-pagina","title":"Sobre esta p\u00e1gina","text":"<p>Este \u00e9 o hub do projeto da mat\u00e9ria. Aqui voc\u00ea encontra: - vis\u00e3o geral do que foi feito; - links para notebooks/c\u00f3digos; - entregas da mat\u00e9ria;</p>"},{"location":"template/","title":"Template de Entrega","text":""},{"location":"template/#template-de-entrega","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"template/#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> <li>Maria Oliveira</li> <li>Grupo K<ul> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"template/#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"template/#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"template/#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"template/#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"template/#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"exercicio1/main/","title":"Exercicio 1","text":""},{"location":"exercicio1/main/#deep-learning-data","title":"Deep Learning \u2014 Data","text":"<p>Autor: Caio Ortega Boa Disciplina: Deep Learning Per\u00edodo: 2025.1 Link do Reposit\u00f3rio</p>"},{"location":"exercicio1/main/#sumario","title":"Sum\u00e1rio","text":"<ul> <li>Exerc\u00edcio 1: Separabilidade em 2D (dados sint\u00e9ticos gaussianos)  </li> <li>Exerc\u00edcio 2: N\u00e3o-linearidade em 5D e proje\u00e7\u00e3o PCA \u2192 2D  </li> <li>Exerc\u00edcio 3: Pr\u00e9-processamento do Spaceship Titanic (Kaggle) para MLP com <code>tanh</code></li> </ul>"},{"location":"exercicio1/main/#exercicio-1-class-separability-em-2d","title":"Exerc\u00edcio 1 \u2014 Class Separability em 2D","text":"<p>Objetivo. Explorar como a distribui\u00e7\u00e3o de quatro classes em 2D influencia a complexidade das fronteiras de decis\u00e3o que uma rede neural precisaria aprender.</p>"},{"location":"exercicio1/main/#parametros-utilizados","title":"Par\u00e2metros utilizados","text":"<ul> <li>M\u00e9dias (\u03bcx, \u03bcy): (2,3), (5,6), (8,1), (15,4)</li> <li>Desvios (\u03c3x, \u03c3y): (0,8; 2,5), (1,2; 1,9), (0,9; 0,9), (0,5; 2,0) </li> </ul>"},{"location":"exercicio1/main/#visualizacao","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio1/main/#analise-e-respostas","title":"An\u00e1lise e respostas","text":"<ul> <li>Distribui\u00e7\u00e3o e overlap: Classes 0 e 1 apresentam sobreposi\u00e7\u00e3o consider\u00e1vel enquanto as classes 1 e 2 apresentam leve sobreposi\u00e7\u00e3o; a Classe 3 est\u00e1 deslocada \u00e0 direita sem nenhuma sobreposi\u00e7\u00e3o.  </li> <li>Uma fronteira linear simples separa tudo? N\u00e3o. Com uma \u00fanica fronteira linear n\u00e3o \u00e9 poss\u00edvel separar todas as classes corretamente.  </li> <li>\u201cSketch\u201d das fronteiras que a rede aprenderia: Para separar de maneira eficiente todas as classes seriam necess\u00e1rias pelo menos 3 fronteiras lineares.</li> </ul>"},{"location":"exercicio1/main/#codigo","title":"C\u00f3digo","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Garante reprodutibilidade dos n\u00fameros aleat\u00f3rios\nnp.random.seed(42)\n\ndef draw_line(ax, point1, point2, style='--', color='k', lw=2):\n    \"\"\"\n    Desenha uma linha entre dois pontos no gr\u00e1fico.\n\n    Par\u00e2metros:\n    - ax: objeto matplotlib.axes onde a linha ser\u00e1 desenhada.\n    - point1: tupla (x1, y1) do primeiro ponto.\n    - point2: tupla (x2, y2) do segundo ponto.\n    - style: estilo da linha (default='--' tracejada).\n    - color: cor da linha (default='k' preto).\n    - lw: espessura da linha.\n    - label: legenda opcional para a linha.\n    \"\"\"\n    x_vals = [point1[0], point2[0]]\n    y_vals = [point1[1], point2[1]]\n    ax.plot(x_vals, y_vals, style, color=color, lw=lw)\n\n# Par\u00e2metros das distribui\u00e7\u00f5es gaussianas:\n# means = lista de tuplas (\u03bcx, \u03bcy) = m\u00e9dias em cada eixo\n# stds = lista de tuplas (\u03c3x, \u03c3y) = desvios em cada eixo\nmeans = [(2,3), (5,6), (8,1), (15,4)]\nstds = [(0.8,2.5), (1.2,1.9), (0.9,0.9), (0.5,2.0)]\n\n# Listas para acumular pontos (X) e r\u00f3tulos (y)\nX = []\ny = []\n\n# Para cada classe (0,1,2,3) gera 100 pontos 2D\n# np.random.normal aceita tuplas em loc/scale\n# loc=(\u03bcx, \u03bcy) =&gt; m\u00e9dia por eixo\n# scale=(\u03c3x, \u03c3y) =&gt; desvio por eixo\n# Isso equivale a gaussianas 2D com covari\u00e2ncia diagonal\nfor i, (mean, std) in enumerate(zip(means, stds)):\n    points = np.random.normal(loc=mean, scale=std, size=(100,2))\n    X.append(points)               # pontos da classe i\n    y.append(np.full(100, i))      # vetor [i, i, ..., i] (100 vezes)\n\n# Empilha todas as classes em um \u00fanico array\nX = np.vstack(X)  # shape (400, 2)\ny = np.hstack(y)  # shape (400,)\n\n# Cria o gr\u00e1fico de dispers\u00e3o\nfig, ax = plt.subplots(figsize=(8,6))\nfor i in range(4):\n    # Plota os pontos da classe i\n    ax.scatter(X[y==i,0], X[y==i,1], label=f'Class {i}', alpha=0.7)\n\n#Desenhando linhas arbitrarias de separa\u00e7\u00e3o\ndraw_line(ax, (5,-3), (2,15), style='-', color='purple')\ndraw_line(ax, (4.5,1.5), (12.5,5), style='-', color='purple')\ndraw_line(ax, (12.5,-3), (12.5,15), style='-', color='purple')\n\nax.legend()\nax.set_title(\"Synthetic 2D Gaussian Dataset\")\nax.set_xlabel(\"X1\")\nax.set_ylabel(\"X2\")\nplt.show()\n</code></pre>"},{"location":"exercicio1/main/#exercicio-2-nao-linearidade-em-5d-pca-5d-2d","title":"Exerc\u00edcio 2 \u2014 N\u00e3o-linearidade em 5D + PCA (5D \u2192 2D)","text":"<p>Objetivo. Criar dois grupos 5D com m\u00e9dias/covari\u00e2ncias especificadas e visualizar em 2D via PCA.</p>"},{"location":"exercicio1/main/#configuracao","title":"Configura\u00e7\u00e3o","text":"<ul> <li>Classe A: vetor de m\u00e9dia nulo; covari\u00e2ncias positivas entre algumas dimens\u00f5es.  </li> <li>Classe B: vetor de m\u00e9dia transladado (1,5 em todas as componentes); covari\u00e2ncias com sinais distintos, alterando forma e orienta\u00e7\u00e3o do grupo.</li> </ul>"},{"location":"exercicio1/main/#visualizacao_1","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio1/main/#analise-e-respostas_1","title":"An\u00e1lise e respostas","text":"<ul> <li>Rela\u00e7\u00e3o entre as classes (proje\u00e7\u00e3o 2D): Observa-se mistura parcial, embora possa se identificar certa separa\u00e7\u00e3o entre as nuvens.  </li> <li>Separabilidade linear: Embora possa ser observado uma distribui\u00e7\u00e3o com certa separa\u00e7\u00e3o na proje\u00e7\u00e3o 2d dos dados, uma separa\u00e7\u00e3o linear seria muito ineficiente para o caso proposto. Por haverem m\u00faltiplas dimens\u00f5es nos dados a separabilidade linear tenderia a perder muita informa\u00e7\u00e3o, por n\u00e3o haver um hiperplano perfeito capaz de separar as duas classes.</li> </ul>"},{"location":"exercicio1/main/#codigo_1","title":"C\u00f3digo","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Reprodutibilidade\nnp.random.seed(42)\n\n# -----------------------------\n# 1) Define par\u00e2metros 5D\n# -----------------------------\n# Classe A\nmu_A = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.3, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n], dtype=float)\n\n# Classe B\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigma_B = np.array([\n    [ 1.5, -0.7,  0.2,  0.0, 0.0],\n    [-0.7,  1.5,  0.4,  0.0, 0.0],\n    [ 0.2,  0.4,  1.5,  0.6, 0.0],\n    [ 0.0,  0.0,  0.6,  1.5, 0.3],\n    [ 0.0,  0.0,  0.0,  0.3, 1.5],\n], dtype=float)\n\n# -----------------------------\n# 2) Gera\u00e7\u00e3o dos dados (5D)\n# -----------------------------\nnA, nB = 500, 500\nXA = np.random.multivariate_normal(mean=mu_A, cov=Sigma_A, size=nA)\nXB = np.random.multivariate_normal(mean=mu_B, cov=Sigma_B, size=nB)\n\n# Empilha dados e r\u00f3tulos\nX_5d = np.vstack([XA, XB])         # (1000, 5)\ny    = np.array([0]*nA + [1]*nB)  # 0 = Classe A, 1 = Classe B\n\n# -----------------------------\n# 3) Redu\u00e7\u00e3o de dimensionalidade (PCA \u2192 2D)\n# -----------------------------\npca = PCA(n_components=2, random_state=42)\nX_2d = pca.fit_transform(X_5d)     # (1000, 2)\n\n# -----------------------------\n# 4) Visualiza\u00e7\u00e3o (apenas pontos)\n# -----------------------------\nplt.figure(figsize=(8, 6))\nplt.scatter(X_2d[y==0, 0], X_2d[y==0, 1], alpha=0.6, s=18, label=\"Class A\")\nplt.scatter(X_2d[y==1, 0], X_2d[y==1, 1], alpha=0.6, s=18, label=\"Class B\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Exercise 2 \u2014 PCA (5D \u2192 2D) scatter\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercicio1/main/#exercicio-3-spaceship-titanic-kaggle-pre-processamento-para-tanh","title":"Exerc\u00edcio 3 \u2014 Spaceship Titanic (Kaggle): Pr\u00e9-processamento para <code>tanh</code>","text":"<p>Objetivo. Preparar dados reais para uma MLP com <code>tanh</code>, assegurando entradas est\u00e1veis.</p>"},{"location":"exercicio1/main/#descricao-do-dataset","title":"Descri\u00e7\u00e3o do dataset","text":"<ul> <li>Objetivo do Dataset O Dataset simula um \"Titanic espacial\", que estaria lotado de passageiros e colidiu com uma anomalia espacial que trasnportou diversos passageiros para outra dimens\u00e3o. O objetivo do dataset \u00e9 descobrir quais passageiros teriam sido transportados para essa dimens\u00e3o alternativa baseado em seus dados.</li> <li>Alvo: <code>Transported</code> \u2014 indica se o passageiro foi transportado para outra dimens\u00e3o (bin\u00e1rio).  </li> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>, <code>CabinNum</code>, <code>Group</code>, <code>PaxInGroup</code>, <code>TotalSpend</code>.  </li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code>, <code>CabinDeck</code>, <code>CabinSide</code>.  </li> <li>Engenharia aplicada: A feature <code>Cabin</code> foi decomposta em <code>CabinDeck</code>, <code>CabinNum</code> e <code>CabinSide</code>;  <code>PassengerId</code> foi decomposto em <code>Group</code> e <code>PaxInGroup</code>; <code>Transported</code> foi convertido para 0/1.</li> </ul>"},{"location":"exercicio1/main/#faltantes","title":"Faltantes","text":"<ul> <li>Investiga\u00e7\u00e3o Todas as colunas, fora <code>PassengerId</code> e <code>Transported</code> possuiam dados faltantes.</li> <li>Num\u00e9ricas: imputa\u00e7\u00e3o pela mediana (robusta a outliers; preserva a posi\u00e7\u00e3o central).  </li> <li>Categ\u00f3ricas: imputa\u00e7\u00e3o pela moda (mant\u00e9m r\u00f3tulos conhecidos; evita categorias artificiais).  </li> </ul>"},{"location":"exercicio1/main/#tratamento-das-features","title":"Tratamento das Features","text":"<ul> <li>One-Hot Encoding Utilizado para tratamento de features categ\u00f3ricas.</li> <li>Normaliza\u00e7\u00e3o para <code>[-1, 1]</code>: Utilizado para tratamento de features num\u00e9ricas, de modo a acomodar os dados corretamente para a utiliza\u00e7\u00e3o da fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>. Se trata de uma boa pr\u00e1tica pois possibilita padroniza a escala das features e possibilita que a maior parte dos dados esteja na parte central da curva, otimizando o treinamento.</li> </ul>"},{"location":"exercicio1/main/#visualizacoes","title":"Visualiza\u00e7\u00f5es","text":"<p>Antes da transforma\u00e7\u00e3o </p> <p>Depois da transforma\u00e7\u00e3o </p>"},{"location":"exercicio1/main/#codigo_2","title":"C\u00f3digo","text":"<pre><code>#Imports e leitura\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n#Configs visuais\nplt.rcParams[\"figure.figsize\"] = (8, 5)\nplt.rcParams[\"axes.grid\"] = True\n\n#Reprodutibilidade\nnp.random.seed(42)\n\n#Caminho do arquivo\nCSV_PATH = \"spaceship.csv\"\n\ndf = pd.read_csv(CSV_PATH)\nprint(df.shape)\ndf.head()\n</code></pre> <pre><code>#Vis\u00e3o geral: tipos e faltantes\nprint(\"\\n=== info() ===\")\ndf.info()\n\nprint(\"\\n=== Missing values por coluna ===\")\nmissing_abs = df.isna().sum().sort_values(ascending=False)\nmissing_pct = (df.isna().mean()*100).sort_values(ascending=False)\ndisplay(pd.DataFrame({\"missing\": missing_abs, \"missing_%\": missing_pct.round(2)}))\n</code></pre> <pre><code>#Quebra Cabin em deck/num/side\ncabin = df[\"Cabin\"].astype(\"string\")\nparts = cabin.str.split(\"/\", expand=True)\ndf[\"CabinDeck\"] = parts[0]\ndf[\"CabinNum\"]  = pd.to_numeric(parts[1], errors=\"coerce\")\ndf[\"CabinSide\"] = parts[2]\n\n#Quebra PassengerId em grupo e \u00edndice no grupo\npid = df[\"PassengerId\"].astype(\"string\")\ngrp_pp = pid.str.split(\"_\", expand=True)\ndf[\"Group\"] = pd.to_numeric(grp_pp[0], errors=\"coerce\")  \ndf[\"PaxInGroup\"] = pd.to_numeric(grp_pp[1], errors=\"coerce\")\n\n#Transported -&gt; 0/1\ndf[\"Transported\"] = df[\"Transported\"].map({True:1, False:0, \"True\":1, \"False\":0}).astype(int)\n\n#Colunas que n\u00e3o vamos usar como features\ndrop_cols = [\"PassengerId\", \"Name\", \"Cabin\"] \n\n#Colunas num\u00e9ricas e categ\u00f3ricas\nnumeric_features = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Group\", \"CabinNum\", \"PaxInGroup\"]\ncategorical_features = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"CabinDeck\", \"CabinSide\"]\n\nprint(\"numeric_features:\", numeric_features)\nprint(\"categorical_features:\", categorical_features)\n\ndf_pre = df.drop(columns=drop_cols).copy()\ndf_pre.head()\n</code></pre> <pre><code>#Separa\u00e7\u00e3o X, y\ntarget = \"Transported\"\nX = df_pre.drop(columns=[target])\ny = df_pre[target].values\n</code></pre> <pre><code>#Sanitiza\u00e7\u00e3o (Corre\u00e7\u00e3o de erros no processamento)\nX = X.copy()\n\n#Booleans para strings\nfor col in [\"CryoSleep\", \"VIP\"]:\n    if col in X.columns:\n        X[col] = X[col].map({True: \"True\", False: \"False\"}).astype(\"object\")\n\n#CATEG\u00d3RICAS como 'object' e remover pd.NA\nfor c in categorical_features:\n    if c in X.columns:\n        X[c] = X[c].astype(\"object\")\n        mask = pd.isna(X[c])\n        if mask.any():\n            X.loc[mask, c] = np.nan\n\n#NUM\u00c9RICAS realmente num\u00e9ricas\nfor c in numeric_features:\n    if c in X.columns:\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n</code></pre> <pre><code># Num\u00e9ricas: imputar mediana + scaler (-1, 1)\nnum_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\",  MinMaxScaler(feature_range=(-1, 1))),\n])\n\n# Categ\u00f3ricas: imputar moda + OneHot\ncat_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\",  OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", num_pipe, numeric_features),\n        (\"cat\", cat_pipe, categorical_features),\n    ],\n    remainder=\"drop\",\n)\n\nX_proc = preprocessor.fit_transform(X)\n\nprint(\"X_proc shape:\", X_proc.shape)\n</code></pre> <pre><code>#Checagem de colunas ap\u00f3s OHE\nnum_names = numeric_features\ncat_names = preprocessor.named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out(categorical_features).tolist()\nfinal_feature_names = num_names + cat_names\n\nprint(\"Total de colunas ap\u00f3s OHE:\", len(final_feature_names))\n</code></pre> <pre><code>#Plotagem dos histogramas\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Antes\nX[\"Age\"].hist(ax=axes[0], bins=30, alpha=0.8)\naxes[0].set_title(\"Age \u2014 antes da escala\")\naxes[0].set_xlabel(\"Age\")\n\nX[\"FoodCourt\"].hist(ax=axes[1], bins=30, alpha=0.8)\naxes[1].set_title(\"FoodCourt \u2014 antes da escala\")\naxes[1].set_xlabel(\"FoodCourt\")\nplt.tight_layout()\nplt.show()\n\n# Depois\nage_idx = numeric_features.index(\"Age\")\nfood_idx = numeric_features.index(\"FoodCourt\")\n\nage_scaled   = X_proc[:, age_idx]\nfood_scaled  = X_proc[:, food_idx]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].hist(age_scaled, bins=30, alpha=0.8)\naxes[0].set_title(\"Age \u2014 depois (StandardScaler)\")\naxes[0].set_xlabel(\"z-score\")\n\naxes[1].hist(food_scaled, bins=30, alpha=0.8)\naxes[1].set_title(\"FoodCourt \u2014 depois (StandardScaler)\")\naxes[1].set_xlabel(\"z-score\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercicio2/main/","title":"Deep Learning \u2014 Perceptron","text":""},{"location":"exercicio2/main/#deep-learning-perceptron","title":"Deep Learning \u2014 Perceptron","text":"<p>Autor: Caio Ortega Boa Disciplina: Deep Learning Per\u00edodo: 2025.1 Link do Reposit\u00f3rio</p>"},{"location":"exercicio2/main/#sumario","title":"Sum\u00e1rio","text":"<ul> <li>Perceptron Implementa\u00e7\u00e3o de um Perceptron</li> <li>Exerc\u00edcio 1: Treinamento de Perceptron com Converg\u00eancia </li> <li>Exerc\u00edcio 2: Treinamento de Perceptron sem Converg\u00eancia</li> </ul>"},{"location":"exercicio2/main/#perceptron-implementacao-de-um-perceptron","title":"Perceptron \u2014 Implementa\u00e7\u00e3o de um Perceptron","text":"<p>Objetivo. Implementa\u00e7\u00e3o de um Perceptron sem utiliza\u00e7\u00e3o de bibliotecas auxiliares e modelos prontos, apenas Numpy para c\u00e1lculos matriciais.</p>"},{"location":"exercicio2/main/#codigo","title":"C\u00f3digo","text":"<pre><code>import numpy as np\n\nclass Perceptron:\n    \"\"\"\n    Single-layer Perceptron implemented from scratch.\n    \"\"\"\n    def __init__(\n        self,\n        lr: float = 0.1,\n        max_epochs: int = 100,\n        random_state: int | None = None,\n        track_history: bool = True,\n    ):\n        self.lr = lr\n        self.max_epochs = max_epochs\n        self.random_state = random_state\n        self.track_history = track_history\n\n        self.w = None\n        self.b = 0.0\n        self.accuracy_history_ = []\n\n    def decision_function(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute raw scores (w\u00b7x + b).\n        X: shape (n_samples, n_features)\n        \"\"\"\n        if self.w is None:\n            raise ValueError(\"Model is not fitted yet.\")\n        return X @ self.w + self.b\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict labels in {-1, +1} using sign(w\u00b7x + b).\n        \"\"\"\n        scores = self.decision_function(X)\n        return np.where(scores &gt;= 0, 1, -1)\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; dict:\n        \"\"\"\n        Fit the perceptron on labels in {-1, +1}.\n        Stops early if an epoch completes with zero updates.\n        Records accuracy after each epoch.\n        Returns a training log with 'epochs_run' and 'converged' keys.\n        \"\"\"\n        rng = np.random.default_rng(self.random_state)\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.w = np.zeros(n_features, dtype=float)\n        self.b = 0.0\n        self.accuracy_history_ = []\n\n        indices = np.arange(n_samples)\n\n        converged = False\n        epochs_run = 0\n\n        for epoch in range(self.max_epochs):\n            epochs_run += 1\n            # Shuffle each epoch for robustness\n            rng.shuffle(indices)\n            updates = 0\n\n            for idx in indices:\n                x_i = X[idx]\n                y_i = y[idx]  # must be -1 or +1\n                margin = y_i * (np.dot(self.w, x_i) + self.b)\n                if margin &lt;= 0:\n                    # Misclassified -&gt; update\n                    self.w = self.w + self.lr * y_i * x_i\n                    self.b = self.b + self.lr * y_i\n                    updates += 1\n\n            # Accuracy after epoch\n            y_pred = self.predict(X)\n            acc = np.mean(y_pred == y)\n            self.accuracy_history_.append(acc)\n\n            if updates == 0:\n                converged = True\n                break\n\n        return {\"epochs_run\": epochs_run, \"converged\": converged}\n</code></pre>"},{"location":"exercicio2/main/#exercicio-1-treinamento-de-perceptron-com-convergencia","title":"Exerc\u00edcio 1 \u2014 Treinamento de Perceptron com Converg\u00eancia","text":"<p>Objetivo. Gerar um dataset 2d com clara distin\u00e7\u00e3o entre classes para realiza\u00e7\u00e3o do treinamento de um perceptron.</p>"},{"location":"exercicio2/main/#parametros-utilizados","title":"Par\u00e2metros utilizados","text":"<ul> <li>M\u00e9dias (\u03bcx, \u03bcy): (1.5,1.5), (5,5)</li> <li>Desvios (\u03c3x, \u03c3y): (0.5; 0), (0; 0.5)</li> </ul>"},{"location":"exercicio2/main/#visualizacao","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio2/main/#resultados","title":"Resultados","text":"<ul> <li>Final Weights [0.304 , 0.199]</li> <li>Final Bias -1.6</li> <li>Epochs 2</li> <li>Fina Accuracy 1.0</li> </ul>"},{"location":"exercicio2/main/#analise-dos-resultados","title":"An\u00e1lise dos Resultados","text":"<ul> <li>Alta Separabilidade A alta separabilidade dos dados gerados, apresentando classes extremamente bem definidas por uma \u00fanica linha de decis\u00e3o, s\u00e3o respons\u00e1veis pelo baixo n\u00famero de \u00e9pocas necess\u00e1rio para a convers\u00e3o. Isso se deve pois, conforme o treinamento ocorre e a linha se enviesa para os dados, o modelo passa a classificar todos os dados corretamente, ocasionando em sua converg\u00eancia.</li> </ul>"},{"location":"exercicio2/main/#codigo_1","title":"C\u00f3digo","text":"<pre><code>mean0 = np.array([1.5, 1.5])\ncov0  = np.array([[0.5, 0.0],[0.0, 0.5]])\n\nmean1 = np.array([5.0, 5.0])\ncov1  = np.array([[0.5, 0.0],[0.0, 0.5]])\n\nn_per_class = 1000\n\nX0 = rng.multivariate_normal(mean0, cov0, size=n_per_class)\nX1 = rng.multivariate_normal(mean1, cov1, size=n_per_class)\n\nX = np.vstack([X0, X1])\ny01 = np.hstack([np.zeros(n_per_class, dtype=int), np.ones(n_per_class, dtype=int)])\ny = np.where(y01 == 1, 1, -1)\n\nplt.figure()\nplt.scatter(X0[:, 0], X0[:, 1], label=\"Class 0\")\nplt.scatter(X1[:, 0], X1[:, 1], label=\"Class 1\")\nplt.title(\"Exercise 1: Generated Data (Mostly Linearly Separable)\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Train perceptron\nclf = Perceptron(lr=0.1, max_epochs=100, random_state=42)\nlog = clf.fit(X, y)\n\n# Evaluate\ny_pred = clf.predict(X)\nacc = np.mean(y_pred == y)\nprint(\"Epochs run:\", log[\"epochs_run\"])\nprint(\"Converged:\", log[\"converged\"])\nprint(\"Final weights:\", clf.w)\nprint(\"Final bias:\", clf.b)\nprint(\"Final accuracy:\", acc)\n</code></pre>"},{"location":"exercicio2/main/#exercicio-2-treinamento-de-perceptron-sem-convergencia","title":"Exerc\u00edcio 2 \u2014 Treinamento de Perceptron sem Converg\u00eancia","text":"<p>Objetivo. Gerar um dataset 2d com baixa distin\u00e7\u00e3o entre classes para realiza\u00e7\u00e3o do treinamento de um perceptron.</p>"},{"location":"exercicio2/main/#parametros-utilizados_1","title":"Par\u00e2metros utilizados","text":"<ul> <li>M\u00e9dias (\u03bcx, \u03bcy): (3.0,3.0), (4.0,4.0)</li> <li>Desvios (\u03c3x, \u03c3y): (1.5; 0), (0; 1.5)</li> </ul>"},{"location":"exercicio2/main/#visualizacao_1","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio2/main/#resultados_1","title":"Resultados","text":"<ul> <li>Final Weights [-0.140 , 0.955]</li> <li>Final Bias -4.5</li> <li>Epochs 100</li> <li>Fina Accuracy 0.558</li> </ul>"},{"location":"exercicio2/main/#analise-dos-resultados_1","title":"An\u00e1lise dos Resultados","text":"<ul> <li>Baixa Separabilidade No seguinte dataset, havendo baixa separabilidade de dados, o modelo do perceptron n\u00e3o conseguiu convergir. Isso se deve a mistura e sobreposi\u00e7\u00e3o de dados que existe no dataset, impossibilitando que todas as classes sejam devidamente identificadas por uma \u00fanica linha de decis\u00e3o linear. Por conta disso o perceptron nunca ir\u00e1 encontrar os pesos ideais para o treinamento, n\u00e3o permitindo sua converg\u00eancia.</li> </ul>"},{"location":"exercicio2/main/#codigo_2","title":"C\u00f3digo","text":"<pre><code>mean0 = np.array([3.0, 3.0])\ncov0  = np.array([[1.5, 0.0],[0.0, 1.5]])\n\nmean1 = np.array([4.0, 4.0])\ncov1  = np.array([[1.5, 0.0],[0.0, 1.5]])\n\nn_per_class = 1000\n\nX0 = rng.multivariate_normal(mean0, cov0, size=n_per_class)\nX1 = rng.multivariate_normal(mean1, cov1, size=n_per_class)\n\nX = np.vstack([X0, X1])\ny01 = np.hstack([np.zeros(n_per_class, dtype=int), np.ones(n_per_class, dtype=int)])\ny = np.where(y01 == 1, 1, -1)\n\nplt.figure()\nplt.scatter(X0[:, 0], X0[:, 1], label=\"Class 0\")\nplt.scatter(X1[:, 0], X1[:, 1], label=\"Class 1\")\nplt.title(\"Exercise 1: Generated Data (Mostly Linearly Separable)\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>clf = Perceptron(lr=0.1, max_epochs=100, random_state=123)\nlog = clf.fit(X, y)\n\ny_pred = clf.predict(X)\nacc = np.mean(y_pred == y)\n\nprint(\"Epochs run:\", log[\"epochs_run\"])\nprint(\"Converged:\", log[\"converged\"])\nprint(\"Final weights:\", clf.w)\nprint(\"Final bias:\", clf.b)\nprint(\"Final accuracy:\", acc)\n</code></pre>"}]}