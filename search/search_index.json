{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#bem-vindoa-deep-learning-20251","title":"Bem-vindo(a) \u2014 Deep Learning (2025.1)","text":"<p>Autor: Caio Ortega Boa Turma: 2025.2 Reposit\u00f3rio: Este site re\u00fane todo o conte\u00fado produzido por Caio Ortega Boa na disciplina de Deep Learning \u2014 exerc\u00edcios, c\u00f3digos, an\u00e1lises, gr\u00e1ficos e anota\u00e7\u00f5es.</p>"},{"location":"#sobre-esta-pagina","title":"Sobre esta p\u00e1gina","text":"<p>Este \u00e9 o hub do projeto da mat\u00e9ria. Aqui voc\u00ea encontra: - vis\u00e3o geral do que foi feito; - links para notebooks/c\u00f3digos; - entregas da mat\u00e9ria;</p>"},{"location":"template/","title":"Template de Entrega","text":""},{"location":"template/#template-de-entrega","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"template/#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> <li>Maria Oliveira</li> <li>Grupo K<ul> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"template/#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"template/#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"template/#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"template/#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"template/#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"classification/model/","title":"Introdu\u00e7\u00e3o","text":"In\u00a0[35]: Copied! <pre>from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nsns.set_theme(style='whitegrid', context='notebook')\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 120)\n</pre> from __future__ import annotations  from pathlib import Path from typing import Dict, List, Optional, Tuple  import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from IPython.display import display  sns.set_theme(style='whitegrid', context='notebook') pd.set_option('display.max_columns', 100) pd.set_option('display.width', 120)  In\u00a0[36]: Copied! <pre># -----------------------------\n# Ativa\u00e7\u00f5es e derivadas\n# -----------------------------\ndef tanh(z: np.ndarray) -&gt; np.ndarray:\n    return np.tanh(z)\n\ndef dtanh_from_a(a: np.ndarray) -&gt; np.ndarray:\n    return 1.0 - a**2\n\ndef softmax(Z: np.ndarray, axis: int = 0) -&gt; np.ndarray:\n    \"\"\"\n    Z: (K, m) -&gt; aplica softmax por coluna (axis=0).\n    Retorna prob. por classe, colunas somam 1.\n    \"\"\"\n    Z_shift = Z - np.max(Z, axis=axis, keepdims=True)\n    e = np.exp(Z_shift)\n    return e / np.sum(e, axis=axis, keepdims=True)\n\ndef cross_entropy(y_true_oh: np.ndarray, y_prob: np.ndarray, eps: float = 1e-12) -&gt; float:\n    \"\"\"\n    y_true_oh: (m, K) one-hot\n    y_prob   : (m, K) probabilidades (softmax)\n    \"\"\"\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return float(-np.mean(np.sum(y_true_oh * np.log(y_prob), axis=1)))\n\n\n# -----------------------------\n# M\u00e9tricas de avalia\u00e7\u00e3o\n# -----------------------------\n\ndef accuracy_score(y_true: np.ndarray, y_pred_labels: np.ndarray) -&gt; float:\n    return float(np.mean(y_true == y_pred_labels))\n\ndef precision_recall_f1(y_true, y_pred, labels=None):\n    \"\"\"Calcula precis\u00e3o, recall e F1 por classe do zero\"\"\"\n    if labels is None:\n        labels = np.unique(np.concatenate([y_true, y_pred]))\n    metrics = {}\n    for label in labels:\n        tp = np.sum((y_pred == label) &amp; (y_true == label))\n        fp = np.sum((y_pred == label) &amp; (y_true != label))\n        fn = np.sum((y_pred != label) &amp; (y_true == label))\n        precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0.0\n        recall    = tp / (tp + fn) if (tp + fn) &gt; 0 else 0.0\n        f1        = 2*precision*recall/(precision+recall) if (precision+recall) &gt; 0 else 0.0\n        metrics[label] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n    return metrics\n\ndef confusion_matrix_true(y_true, y_pred, labels=None):\n    if labels is None:\n        labels = np.unique(np.concatenate([y_true, y_pred]))\n    cm = pd.DataFrame(0, index=labels, columns=labels)\n    for yt, yp in zip(y_true, y_pred):\n        cm.loc[yt, yp] += 1\n    return cm\n\n\n# -----------------------------\n# Divis\u00e3o treino/valida\u00e7\u00e3o/teste\n# -----------------------------\n\ndef train_test_split(\n    X: np.ndarray,\n    y: np.ndarray,\n    test_size: float = 0.15,\n    val_size: float = 0.15,\n    random_state: int = 42,\n):\n    \"\"\"Divide os dados em conjuntos de treino, validacao e teste (70/15/15 por padrao).\"\"\"\n    if not (0.0 &lt; test_size &lt; 1.0) or not (0.0 &lt; val_size &lt; 1.0):\n        raise ValueError('test_size e val_size devem estar entre 0 e 1.')\n    if test_size + val_size &gt;= 1.0:\n        raise ValueError('A soma de test_size e val_size deve ser inferior a 1.')\n\n    rng = np.random.default_rng(random_state)\n    m = X.shape[0]\n    idx = rng.permutation(m)\n\n    m_test = max(1, int(round(test_size * m)))\n    m_val = max(1, int(round(val_size * m)))\n\n    total_requested = m_test + m_val\n    if total_requested &gt;= m:\n        m_test = max(1, int(np.floor(test_size * m)))\n        m_val = max(1, int(np.floor(val_size * m)))\n        total_requested = m_test + m_val\n        if total_requested &gt;= m:\n            raise ValueError('Nao ha exemplos suficientes para o particionamento desejado.')\n\n    test_idx = idx[:m_test]\n    val_idx = idx[m_test:m_test + m_val]\n    train_idx = idx[m_test + m_val:]\n\n    if train_idx.size == 0:\n        raise ValueError('Conjunto de treino vazio; ajuste test_size/val_size.')\n\n    return (\n        X[train_idx],\n        X[val_idx],\n        X[test_idx],\n        y[train_idx],\n        y[val_idx],\n        y[test_idx],\n    )\n\n# -----------------------------\n# Inicializa\u00e7\u00e3o do pesos\n# -----------------------------\n\ndef xavier_init(fan_in: int, fan_out: int, rng: np.random.Generator) -&gt; np.ndarray:\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n    return rng.normal(0.0, std, size=(fan_out, fan_in))\n\n# -----------------------------\n# One Hot Encoding\n# -----------------------------\n\ndef one_hot(y: np.ndarray, K: int) -&gt; np.ndarray:\n    \"\"\"\n    y: (m,) com r\u00f3tulos inteiros [0..K-1]\n    retorna: (m, K) one-hot\n    \"\"\"\n    m = y.shape[0]\n    Y = np.zeros((m, K), dtype=float)\n    Y[np.arange(m), y.astype(int)] = 1.0\n    return Y\n</pre> # ----------------------------- # Ativa\u00e7\u00f5es e derivadas # ----------------------------- def tanh(z: np.ndarray) -&gt; np.ndarray:     return np.tanh(z)  def dtanh_from_a(a: np.ndarray) -&gt; np.ndarray:     return 1.0 - a**2  def softmax(Z: np.ndarray, axis: int = 0) -&gt; np.ndarray:     \"\"\"     Z: (K, m) -&gt; aplica softmax por coluna (axis=0).     Retorna prob. por classe, colunas somam 1.     \"\"\"     Z_shift = Z - np.max(Z, axis=axis, keepdims=True)     e = np.exp(Z_shift)     return e / np.sum(e, axis=axis, keepdims=True)  def cross_entropy(y_true_oh: np.ndarray, y_prob: np.ndarray, eps: float = 1e-12) -&gt; float:     \"\"\"     y_true_oh: (m, K) one-hot     y_prob   : (m, K) probabilidades (softmax)     \"\"\"     y_prob = np.clip(y_prob, eps, 1.0 - eps)     return float(-np.mean(np.sum(y_true_oh * np.log(y_prob), axis=1)))   # ----------------------------- # M\u00e9tricas de avalia\u00e7\u00e3o # -----------------------------  def accuracy_score(y_true: np.ndarray, y_pred_labels: np.ndarray) -&gt; float:     return float(np.mean(y_true == y_pred_labels))  def precision_recall_f1(y_true, y_pred, labels=None):     \"\"\"Calcula precis\u00e3o, recall e F1 por classe do zero\"\"\"     if labels is None:         labels = np.unique(np.concatenate([y_true, y_pred]))     metrics = {}     for label in labels:         tp = np.sum((y_pred == label) &amp; (y_true == label))         fp = np.sum((y_pred == label) &amp; (y_true != label))         fn = np.sum((y_pred != label) &amp; (y_true == label))         precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0.0         recall    = tp / (tp + fn) if (tp + fn) &gt; 0 else 0.0         f1        = 2*precision*recall/(precision+recall) if (precision+recall) &gt; 0 else 0.0         metrics[label] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}     return metrics  def confusion_matrix_true(y_true, y_pred, labels=None):     if labels is None:         labels = np.unique(np.concatenate([y_true, y_pred]))     cm = pd.DataFrame(0, index=labels, columns=labels)     for yt, yp in zip(y_true, y_pred):         cm.loc[yt, yp] += 1     return cm   # ----------------------------- # Divis\u00e3o treino/valida\u00e7\u00e3o/teste # -----------------------------  def train_test_split(     X: np.ndarray,     y: np.ndarray,     test_size: float = 0.15,     val_size: float = 0.15,     random_state: int = 42, ):     \"\"\"Divide os dados em conjuntos de treino, validacao e teste (70/15/15 por padrao).\"\"\"     if not (0.0 &lt; test_size &lt; 1.0) or not (0.0 &lt; val_size &lt; 1.0):         raise ValueError('test_size e val_size devem estar entre 0 e 1.')     if test_size + val_size &gt;= 1.0:         raise ValueError('A soma de test_size e val_size deve ser inferior a 1.')      rng = np.random.default_rng(random_state)     m = X.shape[0]     idx = rng.permutation(m)      m_test = max(1, int(round(test_size * m)))     m_val = max(1, int(round(val_size * m)))      total_requested = m_test + m_val     if total_requested &gt;= m:         m_test = max(1, int(np.floor(test_size * m)))         m_val = max(1, int(np.floor(val_size * m)))         total_requested = m_test + m_val         if total_requested &gt;= m:             raise ValueError('Nao ha exemplos suficientes para o particionamento desejado.')      test_idx = idx[:m_test]     val_idx = idx[m_test:m_test + m_val]     train_idx = idx[m_test + m_val:]      if train_idx.size == 0:         raise ValueError('Conjunto de treino vazio; ajuste test_size/val_size.')      return (         X[train_idx],         X[val_idx],         X[test_idx],         y[train_idx],         y[val_idx],         y[test_idx],     )  # ----------------------------- # Inicializa\u00e7\u00e3o do pesos # -----------------------------  def xavier_init(fan_in: int, fan_out: int, rng: np.random.Generator) -&gt; np.ndarray:     std = np.sqrt(2.0 / (fan_in + fan_out))     return rng.normal(0.0, std, size=(fan_out, fan_in))  # ----------------------------- # One Hot Encoding # -----------------------------  def one_hot(y: np.ndarray, K: int) -&gt; np.ndarray:     \"\"\"     y: (m,) com r\u00f3tulos inteiros [0..K-1]     retorna: (m, K) one-hot     \"\"\"     m = y.shape[0]     Y = np.zeros((m, K), dtype=float)     Y[np.arange(m), y.astype(int)] = 1.0     return Y In\u00a0[37]: Copied! <pre>class MLP:\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_layers: List[int] = [16, 16],\n        output_dim: int = 2,        \n        lr: float = 0.05,\n        max_epochs: int = 500,\n        batch_size: Optional[int] = None,\n        random_state: Optional[int] = 42,\n        l2: float = 0.0,\n        track_history: bool = True,\n    ):\n        self.input_dim = input_dim\n        self.hidden_layers = hidden_layers\n        self.output_dim = output_dim\n        self.lr = lr\n        self.max_epochs = max_epochs\n        self.batch_size = batch_size\n        self.random_state = random_state\n        self.l2 = l2\n        self.track_history = track_history\n\n        self.params_ = None\n        self.loss_history_: List[float] = []\n        self.acc_history_: List[float] = []\n\n    # ---------- initialization ----------\n    def _init_params(self, rng: np.random.Generator) -&gt; None:\n        layer_sizes = [self.input_dim] + self.hidden_layers + [self.output_dim]\n        W, b = [], []\n        for l in range(1, len(layer_sizes)):\n            fan_in = layer_sizes[l-1]\n            fan_out = layer_sizes[l]\n            W_l = xavier_init(fan_in, fan_out, rng)\n            b_l = np.zeros((fan_out, 1))\n            W.append(W_l)\n            b.append(b_l)\n        self.params_ = {\"W\": W, \"b\": b}\n\n    # ---------- forward ----------\n    def _forward(self, X: np.ndarray):\n        W, B = self.params_[\"W\"], self.params_[\"b\"]\n        A = X.T  \n        caches = [{\"A\": A}]  \n\n        # hidden layers\n        for l in range(len(self.hidden_layers)):\n            Z = W[l] @ A + B[l]\n            A = tanh(Z)\n            caches.append({\"Z\": Z, \"A\": A})\n\n        # output layer (softmax)\n        ZL = W[-1] @ A + B[-1]\n        P = softmax(ZL, axis=0)\n        caches.append({\"Z\": ZL, \"A\": P})\n        return caches, P.T\n\n    # ---------- backward ----------\n    def _backward(self, caches, y: np.ndarray):\n        W = self.params_[\"W\"]\n        L = len(W)\n        m = y.shape[0]\n\n        A0 = caches[0][\"A\"]\n        A_list = [A0] + [c[\"A\"] for c in caches[1:]]\n\n        Y = one_hot(y.reshape(-1), self.output_dim).T\n        P = A_list[-1]\n\n        dZ = (P - Y) / m\n        dW = [None] * L\n        dB = [None] * L\n\n        # \u00faltima camada\n        A_prev = A_list[-2]\n        dW[L-1] = dZ @ A_prev.T\n        dB[L-1] = np.sum(dZ, axis=1, keepdims=True)\n\n        # ocultas\n        for l in reversed(range(L-1)):\n            dA = W[l+1].T @ dZ\n            A_l = A_list[l+1]\n            dZ = dA * dtanh_from_a(A_l)\n\n            A_prev = A_list[l]\n            dW[l] = dZ @ A_prev.T\n            dB[l] = np.sum(dZ, axis=1, keepdims=True)\n\n        if self.l2:\n            reg_scale = self.l2 / m\n            for l in range(L):\n                dW[l] += reg_scale * W[l]\n\n        return dW, dB\n\n    # ---------- update ----------\n    def _update(self, dW, dB, lr: float) -&gt; None:\n        for l in range(len(self.params_[\"W\"])):\n            self.params_[\"W\"][l] -= lr * dW[l]\n            self.params_[\"b\"][l] -= lr * dB[l]\n\n    # ---------- fit ----------\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        rng = np.random.default_rng(self.random_state)\n        self._init_params(rng)\n\n        m = X.shape[0]\n        batch_size = self.batch_size or m\n\n        for epoch in range(1, self.max_epochs + 1):\n            idx = rng.permutation(m)\n            X_shuf = X[idx]\n            y_shuf = y[idx]\n\n            for start in range(0, m, batch_size):\n                end = min(start + batch_size, m)\n                Xb = X_shuf[start:end]\n                yb = y_shuf[start:end]\n\n                caches, _ = self._forward(Xb)\n                dW, dB = self._backward(caches, yb)\n                self._update(dW, dB, self.lr)\n\n            if self.track_history:\n                P_full = self.predict_proba(X)\n                Y_full = one_hot(y, self.output_dim)\n                loss = cross_entropy(Y_full, P_full)\n                if self.l2:\n                    reg = (self.l2 / (2 * m)) * sum(np.sum(W ** 2) for W in self.params_[\"W\"])\n                    loss += reg\n                y_pred = np.argmax(P_full, axis=1)\n                acc = accuracy_score(y, y_pred)\n                self.loss_history_.append(loss)\n                self.acc_history_.append(acc)\n\n        return {\"epochs_run\": self.max_epochs}\n\n    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n        _, P = self._forward(X)\n        return P\n\n    def decision_function(self, X: np.ndarray) -&gt; np.ndarray:\n        W, B = self.params_[\"W\"], self.params_[\"b\"]\n        A = X.T\n        for l in range(len(self.hidden_layers)):\n            A = tanh(W[l] @ A + B[l])\n        ZL = W[-1] @ A + B[-1]\n        return ZL.T\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        P = self.predict_proba(X)\n        return np.argmax(P, axis=1)\n</pre> class MLP:     def __init__(         self,         input_dim: int,         hidden_layers: List[int] = [16, 16],         output_dim: int = 2,                 lr: float = 0.05,         max_epochs: int = 500,         batch_size: Optional[int] = None,         random_state: Optional[int] = 42,         l2: float = 0.0,         track_history: bool = True,     ):         self.input_dim = input_dim         self.hidden_layers = hidden_layers         self.output_dim = output_dim         self.lr = lr         self.max_epochs = max_epochs         self.batch_size = batch_size         self.random_state = random_state         self.l2 = l2         self.track_history = track_history          self.params_ = None         self.loss_history_: List[float] = []         self.acc_history_: List[float] = []      # ---------- initialization ----------     def _init_params(self, rng: np.random.Generator) -&gt; None:         layer_sizes = [self.input_dim] + self.hidden_layers + [self.output_dim]         W, b = [], []         for l in range(1, len(layer_sizes)):             fan_in = layer_sizes[l-1]             fan_out = layer_sizes[l]             W_l = xavier_init(fan_in, fan_out, rng)             b_l = np.zeros((fan_out, 1))             W.append(W_l)             b.append(b_l)         self.params_ = {\"W\": W, \"b\": b}      # ---------- forward ----------     def _forward(self, X: np.ndarray):         W, B = self.params_[\"W\"], self.params_[\"b\"]         A = X.T           caches = [{\"A\": A}]            # hidden layers         for l in range(len(self.hidden_layers)):             Z = W[l] @ A + B[l]             A = tanh(Z)             caches.append({\"Z\": Z, \"A\": A})          # output layer (softmax)         ZL = W[-1] @ A + B[-1]         P = softmax(ZL, axis=0)         caches.append({\"Z\": ZL, \"A\": P})         return caches, P.T      # ---------- backward ----------     def _backward(self, caches, y: np.ndarray):         W = self.params_[\"W\"]         L = len(W)         m = y.shape[0]          A0 = caches[0][\"A\"]         A_list = [A0] + [c[\"A\"] for c in caches[1:]]          Y = one_hot(y.reshape(-1), self.output_dim).T         P = A_list[-1]          dZ = (P - Y) / m         dW = [None] * L         dB = [None] * L          # \u00faltima camada         A_prev = A_list[-2]         dW[L-1] = dZ @ A_prev.T         dB[L-1] = np.sum(dZ, axis=1, keepdims=True)          # ocultas         for l in reversed(range(L-1)):             dA = W[l+1].T @ dZ             A_l = A_list[l+1]             dZ = dA * dtanh_from_a(A_l)              A_prev = A_list[l]             dW[l] = dZ @ A_prev.T             dB[l] = np.sum(dZ, axis=1, keepdims=True)          if self.l2:             reg_scale = self.l2 / m             for l in range(L):                 dW[l] += reg_scale * W[l]          return dW, dB      # ---------- update ----------     def _update(self, dW, dB, lr: float) -&gt; None:         for l in range(len(self.params_[\"W\"])):             self.params_[\"W\"][l] -= lr * dW[l]             self.params_[\"b\"][l] -= lr * dB[l]      # ---------- fit ----------     def fit(self, X: np.ndarray, y: np.ndarray):         rng = np.random.default_rng(self.random_state)         self._init_params(rng)          m = X.shape[0]         batch_size = self.batch_size or m          for epoch in range(1, self.max_epochs + 1):             idx = rng.permutation(m)             X_shuf = X[idx]             y_shuf = y[idx]              for start in range(0, m, batch_size):                 end = min(start + batch_size, m)                 Xb = X_shuf[start:end]                 yb = y_shuf[start:end]                  caches, _ = self._forward(Xb)                 dW, dB = self._backward(caches, yb)                 self._update(dW, dB, self.lr)              if self.track_history:                 P_full = self.predict_proba(X)                 Y_full = one_hot(y, self.output_dim)                 loss = cross_entropy(Y_full, P_full)                 if self.l2:                     reg = (self.l2 / (2 * m)) * sum(np.sum(W ** 2) for W in self.params_[\"W\"])                     loss += reg                 y_pred = np.argmax(P_full, axis=1)                 acc = accuracy_score(y, y_pred)                 self.loss_history_.append(loss)                 self.acc_history_.append(acc)          return {\"epochs_run\": self.max_epochs}      def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:         _, P = self._forward(X)         return P      def decision_function(self, X: np.ndarray) -&gt; np.ndarray:         W, B = self.params_[\"W\"], self.params_[\"b\"]         A = X.T         for l in range(len(self.hidden_layers)):             A = tanh(W[l] @ A + B[l])         ZL = W[-1] @ A + B[-1]         return ZL.T      def predict(self, X: np.ndarray) -&gt; np.ndarray:         P = self.predict_proba(X)         return np.argmax(P, axis=1)   In\u00a0[38]: Copied! <pre>from __future__ import annotations\nfrom typing import Dict, List\nimport re\nimport numpy as np\nimport pandas as pd\n\nclass TrackingFeaturePreprocessor:\n    \"\"\"Custom feature processor used to align training and inference data.\"\"\"\n    key_columns = ['lab_id', 'video_id']\n    keep_columns = ['mouse1_id', 'mouse2_id', 'mouse3_id', 'mouse4_id', 'body_parts_tracked', 'behaviors_labeled']\n    drop_columns = ['mouse1_condition', 'mouse2_condition', 'mouse3_condition']\n\n    def __init__(self):\n        self.numeric_stats: Dict[str, Dict[str, float]] = {}\n        self.categorical_levels: Dict[str, List[str]] = {}\n        self.categorical_mode: Dict[str, str] = {}        \n        self.feature_columns: List[str] = []\n        self.age_columns = {'mouse1_age', 'mouse2_age', 'mouse3_age', 'mouse4_age'}\n\n    @staticmethod\n    def _sanitize_token(value) -&gt; str:\n        token = str(value).strip().lower().replace(' ', '_')\n        cleaned = ''.join(ch if ch.isalnum() or ch == '_' else '_' for ch in token)\n        cleaned = cleaned.strip('_')\n        return cleaned or 'unknown'\n\n    @staticmethod\n    def _parse_age_to_float(series: pd.Series) -&gt; pd.Series:\n        def _parse_one(x) -&gt; float:\n            if pd.isna(x):\n                return np.nan\n            s = str(x)\n            nums = re.findall(r'(\\d+(?:\\.\\d+)?)', s)\n            if not nums:\n                return np.nan\n            vals = [float(n) for n in nums]\n            if len(vals) &gt;= 2:\n                return float((vals[0] + vals[1]) / 2.0)  \n            return float(vals[0])                       \n        return series.apply(_parse_one).astype(float)\n\n    def _fit_numeric(self, series: pd.Series, column: str) -&gt; None:\n        values = pd.to_numeric(series, errors='coerce')\n        valid = values.dropna()\n        if valid.empty:\n            self.numeric_stats[column] = {'fill': 0.0, 'min': 0.0, 'max': 0.0}\n            return\n        fill = float(valid.mean())\n        min_val = float(valid.min())\n        max_val = float(valid.max())\n        self.numeric_stats[column] = {'fill': fill, 'min': min_val, 'max': max_val}\n\n    def fit(self, df: pd.DataFrame) -&gt; None:\n        df_copy = df.copy()\n        for column in df_copy.columns:\n            if column in self.key_columns:\n                continue\n            if column in self.keep_columns:\n                continue\n            if column in self.drop_columns:\n                df_copy = df_copy.drop(columns=[column])\n                continue\n\n            series = df_copy[column]\n\n            if column in self.age_columns:\n                parsed_age = self._parse_age_to_float(series)\n                self._fit_numeric(parsed_age, column)\n                continue\n\n            if pd.api.types.is_numeric_dtype(series):\n                self._fit_numeric(series, column)\n            else:\n                numeric_candidate = pd.to_numeric(series, errors='coerce')\n                if numeric_candidate.notna().sum() &gt;= len(series) * 0.5:\n                    self._fit_numeric(numeric_candidate, column)\n                else:\n                    cats = series.dropna().astype(str)\n                    if cats.empty:\n                        levels = ['__missing__']\n                        mode_val = '__missing__'\n                    else:\n                        levels = sorted(cats.unique().tolist())\n                        mode_val = str(cats.mode().iloc[0])\n                    self.categorical_levels[column] = levels\n                    self.categorical_mode[column] = mode_val\n\n        feature_matrix = self._build_matrix(df_copy)\n        self.feature_columns = [c for c in feature_matrix.columns if c not in self.key_columns]\n\n    def _scale_numeric(self, series: pd.Series, stats: Dict[str, float]) -&gt; pd.Series:\n        values = pd.to_numeric(series, errors='coerce')\n        values = values.fillna(stats['fill'])\n        min_val = stats['min']\n        max_val = stats['max']\n        if np.isclose(max_val, min_val):\n            return pd.Series(0.0, index=series.index)\n        scaled = (values - min_val) / (max_val - min_val)\n        return 2.0 * scaled - 1.0\n\n    def _encode_categorical(self, series: pd.Series, column: str) -&gt; pd.DataFrame:\n        levels = self.categorical_levels[column]\n        mode_val = self.categorical_mode[column]\n\n        processed = series.astype(str).fillna(mode_val)\n        processed = processed.where(processed.isin(levels), mode_val)\n\n        encoded = {}\n        for level in levels:\n            col_name = f\"{column}_{self._sanitize_token(level)}\"\n            encoded[col_name] = (processed == level).astype(float)\n        return pd.DataFrame(encoded, index=series.index)\n\n    def _build_matrix(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        base_cols: Dict[str, pd.Series] = {}\n\n        for key in self.key_columns:\n            if key in df.columns:\n                base_cols[key] = df[key]\n            else:\n                base_cols[key] = pd.Series(np.nan, index=df.index, name=key)\n\n        for column, stats in self.numeric_stats.items():\n            if column in df.columns:\n                series = df[column]\n                if column in self.age_columns:\n                    series = self._parse_age_to_float(series)\n            else:\n                series = pd.Series(np.nan, index=df.index, name=column)\n            base_cols[column] = self._scale_numeric(series, stats)\n\n        matrix = pd.concat(base_cols, axis=1)\n\n        cat_frames = []\n        for column in self.categorical_levels:\n            if column in df.columns:\n                series = df[column]\n            else:\n                series = pd.Series(self.categorical_mode[column], index=df.index, name=column)\n            encoded = self._encode_categorical(series, column)\n            cat_frames.append(encoded)\n\n        if cat_frames:\n            matrix = pd.concat([matrix] + cat_frames, axis=1)\n\n        return matrix.copy()\n\n\n    def transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        if not self.numeric_stats and not self.categorical_levels:\n            raise RuntimeError('Preprocessor is not fitted.')\n\n        matrix = self._build_matrix(df.copy())\n\n        final_cols = self.key_columns + self.feature_columns\n        matrix = matrix.reindex(columns=final_cols, fill_value=0.0)\n\n        return matrix.copy()\n\n    def fit_transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        self.fit(df)\n        return self.transform(df)\n\n    def __repr__(self) -&gt; str:\n        return (\n            f'TrackingFeaturePreprocessor(numeric={len(self.numeric_stats)}, '\n            f'categorical={len(self.categorical_levels)}, '\n            f'features={len(self.feature_columns)})'\n        )\n</pre> from __future__ import annotations from typing import Dict, List import re import numpy as np import pandas as pd  class TrackingFeaturePreprocessor:     \"\"\"Custom feature processor used to align training and inference data.\"\"\"     key_columns = ['lab_id', 'video_id']     keep_columns = ['mouse1_id', 'mouse2_id', 'mouse3_id', 'mouse4_id', 'body_parts_tracked', 'behaviors_labeled']     drop_columns = ['mouse1_condition', 'mouse2_condition', 'mouse3_condition']      def __init__(self):         self.numeric_stats: Dict[str, Dict[str, float]] = {}         self.categorical_levels: Dict[str, List[str]] = {}         self.categorical_mode: Dict[str, str] = {}                 self.feature_columns: List[str] = []         self.age_columns = {'mouse1_age', 'mouse2_age', 'mouse3_age', 'mouse4_age'}      @staticmethod     def _sanitize_token(value) -&gt; str:         token = str(value).strip().lower().replace(' ', '_')         cleaned = ''.join(ch if ch.isalnum() or ch == '_' else '_' for ch in token)         cleaned = cleaned.strip('_')         return cleaned or 'unknown'      @staticmethod     def _parse_age_to_float(series: pd.Series) -&gt; pd.Series:         def _parse_one(x) -&gt; float:             if pd.isna(x):                 return np.nan             s = str(x)             nums = re.findall(r'(\\d+(?:\\.\\d+)?)', s)             if not nums:                 return np.nan             vals = [float(n) for n in nums]             if len(vals) &gt;= 2:                 return float((vals[0] + vals[1]) / 2.0)               return float(vals[0])                                return series.apply(_parse_one).astype(float)      def _fit_numeric(self, series: pd.Series, column: str) -&gt; None:         values = pd.to_numeric(series, errors='coerce')         valid = values.dropna()         if valid.empty:             self.numeric_stats[column] = {'fill': 0.0, 'min': 0.0, 'max': 0.0}             return         fill = float(valid.mean())         min_val = float(valid.min())         max_val = float(valid.max())         self.numeric_stats[column] = {'fill': fill, 'min': min_val, 'max': max_val}      def fit(self, df: pd.DataFrame) -&gt; None:         df_copy = df.copy()         for column in df_copy.columns:             if column in self.key_columns:                 continue             if column in self.keep_columns:                 continue             if column in self.drop_columns:                 df_copy = df_copy.drop(columns=[column])                 continue              series = df_copy[column]              if column in self.age_columns:                 parsed_age = self._parse_age_to_float(series)                 self._fit_numeric(parsed_age, column)                 continue              if pd.api.types.is_numeric_dtype(series):                 self._fit_numeric(series, column)             else:                 numeric_candidate = pd.to_numeric(series, errors='coerce')                 if numeric_candidate.notna().sum() &gt;= len(series) * 0.5:                     self._fit_numeric(numeric_candidate, column)                 else:                     cats = series.dropna().astype(str)                     if cats.empty:                         levels = ['__missing__']                         mode_val = '__missing__'                     else:                         levels = sorted(cats.unique().tolist())                         mode_val = str(cats.mode().iloc[0])                     self.categorical_levels[column] = levels                     self.categorical_mode[column] = mode_val          feature_matrix = self._build_matrix(df_copy)         self.feature_columns = [c for c in feature_matrix.columns if c not in self.key_columns]      def _scale_numeric(self, series: pd.Series, stats: Dict[str, float]) -&gt; pd.Series:         values = pd.to_numeric(series, errors='coerce')         values = values.fillna(stats['fill'])         min_val = stats['min']         max_val = stats['max']         if np.isclose(max_val, min_val):             return pd.Series(0.0, index=series.index)         scaled = (values - min_val) / (max_val - min_val)         return 2.0 * scaled - 1.0      def _encode_categorical(self, series: pd.Series, column: str) -&gt; pd.DataFrame:         levels = self.categorical_levels[column]         mode_val = self.categorical_mode[column]          processed = series.astype(str).fillna(mode_val)         processed = processed.where(processed.isin(levels), mode_val)          encoded = {}         for level in levels:             col_name = f\"{column}_{self._sanitize_token(level)}\"             encoded[col_name] = (processed == level).astype(float)         return pd.DataFrame(encoded, index=series.index)      def _build_matrix(self, df: pd.DataFrame) -&gt; pd.DataFrame:         base_cols: Dict[str, pd.Series] = {}          for key in self.key_columns:             if key in df.columns:                 base_cols[key] = df[key]             else:                 base_cols[key] = pd.Series(np.nan, index=df.index, name=key)          for column, stats in self.numeric_stats.items():             if column in df.columns:                 series = df[column]                 if column in self.age_columns:                     series = self._parse_age_to_float(series)             else:                 series = pd.Series(np.nan, index=df.index, name=column)             base_cols[column] = self._scale_numeric(series, stats)          matrix = pd.concat(base_cols, axis=1)          cat_frames = []         for column in self.categorical_levels:             if column in df.columns:                 series = df[column]             else:                 series = pd.Series(self.categorical_mode[column], index=df.index, name=column)             encoded = self._encode_categorical(series, column)             cat_frames.append(encoded)          if cat_frames:             matrix = pd.concat([matrix] + cat_frames, axis=1)          return matrix.copy()       def transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:         if not self.numeric_stats and not self.categorical_levels:             raise RuntimeError('Preprocessor is not fitted.')          matrix = self._build_matrix(df.copy())          final_cols = self.key_columns + self.feature_columns         matrix = matrix.reindex(columns=final_cols, fill_value=0.0)          return matrix.copy()      def fit_transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:         self.fit(df)         return self.transform(df)      def __repr__(self) -&gt; str:         return (             f'TrackingFeaturePreprocessor(numeric={len(self.numeric_stats)}, '             f'categorical={len(self.categorical_levels)}, '             f'features={len(self.feature_columns)})'         )  In\u00a0[39]: Copied! <pre>def build_tracking_features(metadata_df: pd.DataFrame, tracking_root: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Constr\u00f3i um DataFrame por (lab_id, video_id) com colunas base e estat\u00edsticas de movimento\n    a partir de arquivos .parquet de tracking.\n\n    Regras de movimento:\n    - Ordena por (mouse_id, bodypart, video_frame) e calcula deslocamentos dx/dy por diff.\n    - Dist\u00e2ncia por frame = sqrt(dx^2 + dy^2); acumula por (mouse_id, bodypart).\n    - Cria colunas 'mean_movement_*' normalizadas por n\u00famero de frames do v\u00eddeo.\n    \"\"\"\n    records = []\n\n    for row in metadata_df[['lab_id', 'video_id']].drop_duplicates().itertuples(index=False):\n        base_rows = metadata_df[(metadata_df['lab_id'] == row.lab_id) &amp; (metadata_df['video_id'] == row.video_id)]\n        base = base_rows.iloc[0]\n        record = {'lab_id': row.lab_id, 'video_id': row.video_id}\n\n        for col in base.index:\n            record[col] = base[col]\n\n        path_file = tracking_root / row.lab_id / f\"{row.video_id}.parquet\"\n        if path_file.exists():\n            df_track = pd.read_parquet(path_file)\n            if not df_track.empty and df_track['video_frame'].nunique() &gt; 0:\n                df_track = df_track.sort_values(['mouse_id', 'bodypart', 'video_frame'])\n                df_track[['dx', 'dy']] = df_track.groupby(['mouse_id', 'bodypart'])[['x', 'y']].diff()\n                df_track[['dx', 'dy']] = df_track[['dx', 'dy']].fillna(0.0)\n                df_track['distance'] = np.sqrt(df_track['dx']**2 + df_track['dy']**2)\n\n                frame_count = int(df_track['video_frame'].nunique())\n                record['total_frames'] = frame_count\n\n                # Dist\u00e2ncia por (mouse, bodypart)\n                dist_mouse_body = df_track.groupby(['mouse_id', 'bodypart'])['distance'].sum()\n                for (mouse_id, bodypart), dist in dist_mouse_body.items():\n                    key = f\"mean_movement_mouse{int(mouse_id)}_{TrackingFeaturePreprocessor._sanitize_token(bodypart)}\"\n                    record[key] = float(dist) / frame_count\n\n                # Dist\u00e2ncia por mouse\n                dist_mouse = df_track.groupby('mouse_id')['distance'].sum()\n                for mouse_id, dist in dist_mouse.items():\n                    record[f\"mean_movement_mouse{int(mouse_id)}\"] = float(dist) / frame_count\n\n                # Dist\u00e2ncia total normalizada\n                overall = float(dist_mouse.sum()) if not dist_mouse.empty else float(df_track['distance'].sum())\n                record['mean_movement_overall'] = overall / frame_count\n            else:\n                # Fallback: total_frames a partir de dura\u00e7\u00e3o * fps, se existir\n                record.setdefault(\n                    'total_frames',\n                    base.get('video_duration_sec', 0) * base.get('frames_per_second', 0)\n                )\n        else:\n            # Fallback se arquivo n\u00e3o existe\n            record.setdefault(\n                'total_frames',\n                base.get('video_duration_sec', 0) * base.get('frames_per_second', 0)\n            )\n\n        records.append(record)\n\n    return pd.DataFrame(records)\n</pre> def build_tracking_features(metadata_df: pd.DataFrame, tracking_root: Path) -&gt; pd.DataFrame:     \"\"\"     Constr\u00f3i um DataFrame por (lab_id, video_id) com colunas base e estat\u00edsticas de movimento     a partir de arquivos .parquet de tracking.      Regras de movimento:     - Ordena por (mouse_id, bodypart, video_frame) e calcula deslocamentos dx/dy por diff.     - Dist\u00e2ncia por frame = sqrt(dx^2 + dy^2); acumula por (mouse_id, bodypart).     - Cria colunas 'mean_movement_*' normalizadas por n\u00famero de frames do v\u00eddeo.     \"\"\"     records = []      for row in metadata_df[['lab_id', 'video_id']].drop_duplicates().itertuples(index=False):         base_rows = metadata_df[(metadata_df['lab_id'] == row.lab_id) &amp; (metadata_df['video_id'] == row.video_id)]         base = base_rows.iloc[0]         record = {'lab_id': row.lab_id, 'video_id': row.video_id}          for col in base.index:             record[col] = base[col]          path_file = tracking_root / row.lab_id / f\"{row.video_id}.parquet\"         if path_file.exists():             df_track = pd.read_parquet(path_file)             if not df_track.empty and df_track['video_frame'].nunique() &gt; 0:                 df_track = df_track.sort_values(['mouse_id', 'bodypart', 'video_frame'])                 df_track[['dx', 'dy']] = df_track.groupby(['mouse_id', 'bodypart'])[['x', 'y']].diff()                 df_track[['dx', 'dy']] = df_track[['dx', 'dy']].fillna(0.0)                 df_track['distance'] = np.sqrt(df_track['dx']**2 + df_track['dy']**2)                  frame_count = int(df_track['video_frame'].nunique())                 record['total_frames'] = frame_count                  # Dist\u00e2ncia por (mouse, bodypart)                 dist_mouse_body = df_track.groupby(['mouse_id', 'bodypart'])['distance'].sum()                 for (mouse_id, bodypart), dist in dist_mouse_body.items():                     key = f\"mean_movement_mouse{int(mouse_id)}_{TrackingFeaturePreprocessor._sanitize_token(bodypart)}\"                     record[key] = float(dist) / frame_count                  # Dist\u00e2ncia por mouse                 dist_mouse = df_track.groupby('mouse_id')['distance'].sum()                 for mouse_id, dist in dist_mouse.items():                     record[f\"mean_movement_mouse{int(mouse_id)}\"] = float(dist) / frame_count                  # Dist\u00e2ncia total normalizada                 overall = float(dist_mouse.sum()) if not dist_mouse.empty else float(df_track['distance'].sum())                 record['mean_movement_overall'] = overall / frame_count             else:                 # Fallback: total_frames a partir de dura\u00e7\u00e3o * fps, se existir                 record.setdefault(                     'total_frames',                     base.get('video_duration_sec', 0) * base.get('frames_per_second', 0)                 )         else:             # Fallback se arquivo n\u00e3o existe             record.setdefault(                 'total_frames',                 base.get('video_duration_sec', 0) * base.get('frames_per_second', 0)             )          records.append(record)      return pd.DataFrame(records) In\u00a0[40]: Copied! <pre>def to_numpy_array(obj, dtype=None):\n    if isinstance(obj, np.ndarray):\n        arr = obj\n    elif hasattr(obj, 'to_numpy'):\n        arr = obj.to_numpy()\n    else:\n        arr = np.array(obj)\n    if dtype is not None:\n        arr = arr.astype(dtype, copy=False)\n    return arr\n\ndef train_mlp_classifier(X, y, hidden_layers, output_dim, *, lr=0.05, max_epochs=2000, batch_size=64, random_state=42, l2=0.0, track_history=True):\n    X_np = to_numpy_array(X, dtype=np.float64)\n    y_np = to_numpy_array(y, dtype=int)\n    model = MLP(\n        input_dim=X_np.shape[1],\n        hidden_layers=hidden_layers,\n        output_dim=output_dim,\n        lr=lr,\n        max_epochs=max_epochs,\n        batch_size=batch_size,\n        random_state=random_state,\n        l2=l2,\n        track_history=track_history,\n    )\n    model.fit(X_np, y_np)\n    return model\n</pre> def to_numpy_array(obj, dtype=None):     if isinstance(obj, np.ndarray):         arr = obj     elif hasattr(obj, 'to_numpy'):         arr = obj.to_numpy()     else:         arr = np.array(obj)     if dtype is not None:         arr = arr.astype(dtype, copy=False)     return arr  def train_mlp_classifier(X, y, hidden_layers, output_dim, *, lr=0.05, max_epochs=2000, batch_size=64, random_state=42, l2=0.0, track_history=True):     X_np = to_numpy_array(X, dtype=np.float64)     y_np = to_numpy_array(y, dtype=int)     model = MLP(         input_dim=X_np.shape[1],         hidden_layers=hidden_layers,         output_dim=output_dim,         lr=lr,         max_epochs=max_epochs,         batch_size=batch_size,         random_state=random_state,         l2=l2,         track_history=track_history,     )     model.fit(X_np, y_np)     return model  In\u00a0[41]: Copied! <pre># Caminhos e carregamento dos metadados\nNOTEBOOK_ROOT = Path.cwd().resolve()\nDATASET_CANDIDATES = [\n    NOTEBOOK_ROOT / 'data' / 'raw' / 'MABe-mouse-behavior-detection',\n    NOTEBOOK_ROOT / 'Data' / 'raw' / 'MABe-mouse-behavior-detection',\n    NOTEBOOK_ROOT / 'ClassificationProject' / 'data' / 'raw' / 'MABe-mouse-behavior-detection',\n    NOTEBOOK_ROOT / 'ClassificationProject' / 'Data' / 'raw' / 'MABe-mouse-behavior-detection',\n    NOTEBOOK_ROOT.parent / 'data' / 'raw' / 'MABe-mouse-behavior-detection',\n    NOTEBOOK_ROOT.parent / 'Data' / 'raw' / 'MABe-mouse-behavior-detection',\n]\nfor cand in DATASET_CANDIDATES:\n    if (cand / 'train.csv').exists():\n        DATASET_DIR = cand\n        break\nelse:\n    raise FileNotFoundError('N\u00e3o foi possivel localizar o diret\u00f3rio com train.csv')\n\nANNOTATION_DIR = DATASET_DIR / 'train_annotation'\nTRACKING_DIR = DATASET_DIR / 'train_tracking'\nTRAIN_PATH = DATASET_DIR / 'train.csv'\nTEST_PATH = DATASET_DIR / 'test.csv'\n\ntrain = pd.read_csv(TRAIN_PATH)\ntest_meta = pd.read_csv(TEST_PATH)\n\nprint('Diret\u00f3rio dos dados:', DATASET_DIR)\nprint('train.csv -&gt;', train.shape)\ndisplay(train.head())\n</pre> # Caminhos e carregamento dos metadados NOTEBOOK_ROOT = Path.cwd().resolve() DATASET_CANDIDATES = [     NOTEBOOK_ROOT / 'data' / 'raw' / 'MABe-mouse-behavior-detection',     NOTEBOOK_ROOT / 'Data' / 'raw' / 'MABe-mouse-behavior-detection',     NOTEBOOK_ROOT / 'ClassificationProject' / 'data' / 'raw' / 'MABe-mouse-behavior-detection',     NOTEBOOK_ROOT / 'ClassificationProject' / 'Data' / 'raw' / 'MABe-mouse-behavior-detection',     NOTEBOOK_ROOT.parent / 'data' / 'raw' / 'MABe-mouse-behavior-detection',     NOTEBOOK_ROOT.parent / 'Data' / 'raw' / 'MABe-mouse-behavior-detection', ] for cand in DATASET_CANDIDATES:     if (cand / 'train.csv').exists():         DATASET_DIR = cand         break else:     raise FileNotFoundError('N\u00e3o foi possivel localizar o diret\u00f3rio com train.csv')  ANNOTATION_DIR = DATASET_DIR / 'train_annotation' TRACKING_DIR = DATASET_DIR / 'train_tracking' TRAIN_PATH = DATASET_DIR / 'train.csv' TEST_PATH = DATASET_DIR / 'test.csv'  train = pd.read_csv(TRAIN_PATH) test_meta = pd.read_csv(TEST_PATH)  print('Diret\u00f3rio dos dados:', DATASET_DIR) print('train.csv -&gt;', train.shape) display(train.head())  <pre>Diret\u00f3rio dos dados: C:\\Users\\cailu\\OneDrive\\Documentos\\Work\\Deep\\ClassificationProject\\data\\raw\\MABe-mouse-behavior-detection\ntrain.csv -&gt; (8790, 38)\n</pre> lab_id video_id mouse1_strain mouse1_color mouse1_sex mouse1_id mouse1_age mouse1_condition mouse2_strain mouse2_color mouse2_sex mouse2_id mouse2_age mouse2_condition mouse3_strain mouse3_color mouse3_sex mouse3_id mouse3_age mouse3_condition mouse4_strain mouse4_color mouse4_sex mouse4_id mouse4_age mouse4_condition frames_per_second video_duration_sec pix_per_cm_approx video_width_pix video_height_pix arena_width_cm arena_height_cm arena_shape arena_type body_parts_tracked behaviors_labeled tracking_method 0 AdaptableSnail 44566106 CD-1 (ICR) white male 10.0 8-12 weeks wireless device CD-1 (ICR) white male 24.0 8-12 weeks wireless device CD-1 (ICR) white male 38.0 8-12 weeks wireless device CD-1 (ICR) white male 51.0 8-12 weeks wireless device 30.0 615.6 16.0 1228 1068 60.0 60.0 square familiar [\"body_center\", \"ear_left\", \"ear_right\", \"head... [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta... DeepLabCut 1 AdaptableSnail 143861384 CD-1 (ICR) white male 3.0 8-12 weeks NaN CD-1 (ICR) white male 17.0 8-12 weeks NaN CD-1 (ICR) white male 31.0 8-12 weeks NaN CD-1 (ICR) white male 44.0 8-12 weeks NaN 25.0 3599.0 9.7 968 608 60.0 60.0 square familiar [\"body_center\", \"ear_left\", \"ear_right\", \"late... [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta... DeepLabCut 2 AdaptableSnail 209576908 CD-1 (ICR) white male 7.0 8-12 weeks NaN CD-1 (ICR) white male 21.0 8-12 weeks NaN CD-1 (ICR) white male 35.0 8-12 weeks NaN CD-1 (ICR) white male 48.0 8-12 weeks NaN 30.0 615.2 16.0 1266 1100 60.0 60.0 square familiar [\"body_center\", \"ear_left\", \"ear_right\", \"late... [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta... DeepLabCut 3 AdaptableSnail 278643799 CD-1 (ICR) white male 11.0 8-12 weeks wireless device CD-1 (ICR) white male 25.0 8-12 weeks wireless device CD-1 (ICR) white male 39.0 8-12 weeks wireless device NaN NaN NaN NaN NaN NaN 30.0 619.7 16.0 1224 1100 60.0 60.0 square familiar [\"body_center\", \"ear_left\", \"ear_right\", \"head... [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta... DeepLabCut 4 AdaptableSnail 351967631 CD-1 (ICR) white male 14.0 8-12 weeks NaN CD-1 (ICR) white male 28.0 8-12 weeks NaN CD-1 (ICR) white male 42.0 8-12 weeks NaN NaN NaN NaN NaN 8-12 weeks NaN 30.0 602.6 16.0 1204 1068 60.0 60.0 square familiar [\"body_center\", \"ear_left\", \"ear_right\", \"late... [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta... DeepLabCut In\u00a0[42]: Copied! <pre>summary = (\n    pd.DataFrame({\n        'coluna': train.columns,\n        'dtype': train.dtypes.astype(str),\n        'n_nulos': train.isna().sum(),\n        'perc_nulos': (train.isna().mean() * 100).round(2),\n    })\n    .sort_values('perc_nulos', ascending=False)\n)\ndisplay(summary)\n</pre> summary = (     pd.DataFrame({         'coluna': train.columns,         'dtype': train.dtypes.astype(str),         'n_nulos': train.isna().sum(),         'perc_nulos': (train.isna().mean() * 100).round(2),     })     .sort_values('perc_nulos', ascending=False) ) display(summary)  coluna dtype n_nulos perc_nulos mouse4_condition mouse4_condition object 8787 99.97 mouse4_color mouse4_color object 8780 99.89 mouse4_strain mouse4_strain object 8780 99.89 mouse4_sex mouse4_sex object 8780 99.89 mouse4_id mouse4_id float64 8780 99.89 mouse4_age mouse4_age object 8779 99.87 mouse3_id mouse3_id float64 8773 99.81 mouse2_id mouse2_id float64 8682 98.77 mouse1_id mouse1_id float64 7974 90.72 behaviors_labeled behaviors_labeled object 7926 90.17 mouse3_condition mouse3_condition object 857 9.75 mouse3_age mouse3_age object 847 9.64 mouse3_color mouse3_color object 847 9.64 mouse3_strain mouse3_strain object 847 9.64 mouse3_sex mouse3_sex object 847 9.64 mouse2_condition mouse2_condition object 799 9.09 mouse1_age mouse1_age object 650 7.39 mouse2_age mouse2_age object 650 7.39 mouse1_condition mouse1_condition object 561 6.38 mouse2_sex mouse2_sex object 48 0.55 mouse1_sex mouse1_sex object 11 0.13 arena_type arena_type object 11 0.13 mouse1_strain mouse1_strain object 0 0.00 mouse1_color mouse1_color object 0 0.00 lab_id lab_id object 0 0.00 video_id video_id int64 0 0.00 mouse2_color mouse2_color object 0 0.00 mouse2_strain mouse2_strain object 0 0.00 frames_per_second frames_per_second float64 0 0.00 video_duration_sec video_duration_sec float64 0 0.00 video_width_pix video_width_pix int64 0 0.00 pix_per_cm_approx pix_per_cm_approx float64 0 0.00 video_height_pix video_height_pix int64 0 0.00 arena_width_cm arena_width_cm float64 0 0.00 arena_shape arena_shape object 0 0.00 arena_height_cm arena_height_cm float64 0 0.00 body_parts_tracked body_parts_tracked object 0 0.00 tracking_method tracking_method object 0 0.00 In\u00a0[43]: Copied! <pre>missing_ratio = train.isna().mean().sort_values(ascending=False)\ntop_missing = missing_ratio.head(20)\nif top_missing.gt(0).any():\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=top_missing.values, y=top_missing.index, orient='h', color='firebrick')\n    plt.xlabel('Propor\u00e7\u00e3o de valores ausentes')\n    plt.ylabel('Coluna')\n    plt.title('Top 20 colunas com mais valores ausentes')\n    plt.tight_layout()\nelse:\n    print('Nenhuma coluna com valores ausentes.')\n</pre> missing_ratio = train.isna().mean().sort_values(ascending=False) top_missing = missing_ratio.head(20) if top_missing.gt(0).any():     plt.figure(figsize=(10, 6))     sns.barplot(x=top_missing.values, y=top_missing.index, orient='h', color='firebrick')     plt.xlabel('Propor\u00e7\u00e3o de valores ausentes')     plt.ylabel('Coluna')     plt.title('Top 20 colunas com mais valores ausentes')     plt.tight_layout() else:     print('Nenhuma coluna com valores ausentes.')  In\u00a0[44]: Copied! <pre>annotation_index = (\n    train[['lab_id', 'video_id']]\n    .drop_duplicates()\n    .assign(parquet_path=lambda df: [ANNOTATION_DIR / lab / (str(video) + '.parquet') for lab, video in df[['lab_id', 'video_id']].itertuples(index=False)])\n)\nannotation_index['exists'] = annotation_index['parquet_path'].apply(lambda p: p.exists())\n\nannotation_records = []\nfor row in annotation_index.itertuples(index=False):\n    record = {'lab_id': row.lab_id, 'video_id': row.video_id, 'top_action': None, 'unique_actions': 0}\n    if row.exists:\n        df_ann = pd.read_parquet(row.parquet_path)\n        if 'action' in df_ann.columns:\n            actions = df_ann['action'].dropna().astype(str)\n            if not actions.empty:\n                record['unique_actions'] = int(actions.nunique())\n                record['top_action'] = actions.mode().iloc[0]\n    annotation_records.append(record)\n\nannotation_schema = pd.DataFrame(annotation_records)\ndisplay(annotation_schema.head())\n</pre> annotation_index = (     train[['lab_id', 'video_id']]     .drop_duplicates()     .assign(parquet_path=lambda df: [ANNOTATION_DIR / lab / (str(video) + '.parquet') for lab, video in df[['lab_id', 'video_id']].itertuples(index=False)]) ) annotation_index['exists'] = annotation_index['parquet_path'].apply(lambda p: p.exists())  annotation_records = [] for row in annotation_index.itertuples(index=False):     record = {'lab_id': row.lab_id, 'video_id': row.video_id, 'top_action': None, 'unique_actions': 0}     if row.exists:         df_ann = pd.read_parquet(row.parquet_path)         if 'action' in df_ann.columns:             actions = df_ann['action'].dropna().astype(str)             if not actions.empty:                 record['unique_actions'] = int(actions.nunique())                 record['top_action'] = actions.mode().iloc[0]     annotation_records.append(record)  annotation_schema = pd.DataFrame(annotation_records) display(annotation_schema.head())  lab_id video_id top_action unique_actions 0 AdaptableSnail 44566106 rear 7 1 AdaptableSnail 143861384 attack 4 2 AdaptableSnail 209576908 rear 7 3 AdaptableSnail 278643799 rear 7 4 AdaptableSnail 351967631 rear 7 In\u00a0[45]: Copied! <pre>class_distribution = (\n    annotation_schema\n    .dropna(subset=['top_action'])\n    .groupby('top_action', as_index=False)\n    .size()\n    .rename(columns={'size': 'contagem'})\n    .assign(percentual=lambda df: (df['contagem'] / df['contagem'].sum() * 100).round(2))\n    .sort_values('contagem', ascending=False)\n)\ndisplay(class_distribution)\n\nplt.figure(figsize=(10, 4))\nsns.barplot(\n    data=class_distribution,\n    x='contagem',\n    y='top_action',\n    hue='top_action',          \n    dodge=False,              \n    legend=False,              \n    orient='h',\n    palette='Blues_d'\n)\nplt.xlabel('N\u00famero de v\u00eddeos')\nplt.ylabel('A\u00e7\u00e3o predominante')\nplt.title('Distribui\u00e7\u00e3o das classes (top_action)')\nplt.tight_layout()\nplt.show()\n</pre> class_distribution = (     annotation_schema     .dropna(subset=['top_action'])     .groupby('top_action', as_index=False)     .size()     .rename(columns={'size': 'contagem'})     .assign(percentual=lambda df: (df['contagem'] / df['contagem'].sum() * 100).round(2))     .sort_values('contagem', ascending=False) ) display(class_distribution)  plt.figure(figsize=(10, 4)) sns.barplot(     data=class_distribution,     x='contagem',     y='top_action',     hue='top_action',               dodge=False,                   legend=False,                   orient='h',     palette='Blues_d' ) plt.xlabel('N\u00famero de v\u00eddeos') plt.ylabel('A\u00e7\u00e3o predominante') plt.title('Distribui\u00e7\u00e3o das classes (top_action)') plt.tight_layout() plt.show() top_action contagem percentual 15 sniff 510 59.10 1 attack 94 10.89 16 sniffbody 92 10.66 18 sniffgenital 27 3.13 11 rear 23 2.67 7 escape 22 2.55 12 reciprocalsniff 20 2.32 10 mount 13 1.51 5 defend 13 1.51 8 follow 12 1.39 3 chase 10 1.16 17 sniffface 9 1.04 14 shepherd 8 0.93 6 dominance 3 0.35 0 approach 2 0.23 13 selfgroom 2 0.23 2 avoid 1 0.12 9 huddle 1 0.12 4 climb 1 0.12 In\u00a0[46]: Copied! <pre>preprocessor = TrackingFeaturePreprocessor()\n\ntrain_tracking_features_raw = build_tracking_features(train, TRACKING_DIR)\ntrain_tracking_features = preprocessor.fit_transform(train_tracking_features_raw)\n\nprint('Shape das features de treino:', train_tracking_features.shape)\ndisplay(train_tracking_features.head())\n</pre> preprocessor = TrackingFeaturePreprocessor()  train_tracking_features_raw = build_tracking_features(train, TRACKING_DIR) train_tracking_features = preprocessor.fit_transform(train_tracking_features_raw)  print('Shape das features de treino:', train_tracking_features.shape) display(train_tracking_features.head()) <pre>Shape das features de treino: (8790, 169)\n</pre> lab_id video_id mouse1_age mouse2_age mouse3_age mouse4_age frames_per_second video_duration_sec pix_per_cm_approx video_width_pix video_height_pix arena_width_cm arena_height_cm total_frames mean_movement_mouse1_body_center mean_movement_mouse1_ear_left mean_movement_mouse1_ear_right mean_movement_mouse1_headpiece_bottombackleft mean_movement_mouse1_headpiece_bottombackright mean_movement_mouse1_headpiece_bottomfrontleft mean_movement_mouse1_headpiece_bottomfrontright mean_movement_mouse1_headpiece_topbackleft mean_movement_mouse1_headpiece_topbackright mean_movement_mouse1_headpiece_topfrontleft mean_movement_mouse1_headpiece_topfrontright mean_movement_mouse1_lateral_left mean_movement_mouse1_lateral_right mean_movement_mouse1_neck mean_movement_mouse1_nose mean_movement_mouse1_tail_base mean_movement_mouse1_tail_midpoint mean_movement_mouse1_tail_tip mean_movement_mouse2_body_center mean_movement_mouse2_ear_left mean_movement_mouse2_ear_right mean_movement_mouse2_headpiece_bottombackleft mean_movement_mouse2_headpiece_bottombackright mean_movement_mouse2_headpiece_bottomfrontleft mean_movement_mouse2_headpiece_bottomfrontright mean_movement_mouse2_headpiece_topbackleft mean_movement_mouse2_headpiece_topbackright mean_movement_mouse2_headpiece_topfrontleft mean_movement_mouse2_headpiece_topfrontright mean_movement_mouse2_lateral_left mean_movement_mouse2_lateral_right mean_movement_mouse2_neck mean_movement_mouse2_nose mean_movement_mouse2_tail_base mean_movement_mouse2_tail_midpoint mean_movement_mouse2_tail_tip ... mouse1_strain_c57bl_6j mouse1_strain_c57bl_6j_x_ai148 mouse1_strain_c57bl_6n mouse1_strain_cd_1__icr mouse1_strain_cd1 mouse1_strain_cfw mouse1_color_black mouse1_color_black_and_tan mouse1_color_brown mouse1_color_white mouse1_sex_female mouse1_sex_male mouse2_strain_129_svevtac mouse2_strain_balb_c mouse2_strain_btbr mouse2_strain_c57bl_6j mouse2_strain_c57bl_6n mouse2_strain_cd_1__icr mouse2_strain_cd1 mouse2_strain_cfw mouse2_color_black mouse2_color_black_and_tan mouse2_color_brown mouse2_color_white mouse2_sex_female mouse2_sex_male mouse3_strain_btbr mouse3_strain_c57bl_6j mouse3_strain_cd_1__icr mouse3_color_black mouse3_color_black_and_tan mouse3_color_white mouse3_sex_male mouse4_strain_cd_1__icr mouse4_color_white mouse4_sex_male mouse4_condition_wireless_device arena_shape_circular arena_shape_rectangular arena_shape_split_rectangluar arena_shape_square arena_type_csds arena_type_divided_territories arena_type_familiar arena_type_neutral arena_type_resident_intruder tracking_method_deeplabcut tracking_method_mars tracking_method_sleap tracking_method_custom_hrnet 0 AdaptableSnail 44566106 -0.875 -0.636364 -1.0 0.0 -0.636430 -0.939683 -0.647059 0.061785 -0.061224 -0.11836 -0.111111 -0.939759 -0.960432 -0.948686 -0.951348 0.459698 0.625304 1.000000 0.849507 0.818705 0.656221 0.959277 0.814653 -0.783642 -0.776518 -0.997528 -0.938540 -0.956437 -0.941935 -0.880600 -0.964879 -0.954465 -0.956896 0.346775 -0.217390 1.000000 0.087506 0.168588 -0.007170 0.024077 0.145082 -0.851819 -0.851773 -0.976947 -0.945099 -0.964225 -0.953431 -0.906248 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 1 AdaptableSnail 143861384 -0.875 -0.636364 -1.0 0.0 -0.727322 -0.638055 -0.858824 -0.235698 -0.555317 -0.11836 -0.111111 -0.699761 -0.975767 -0.969271 -0.968166 -0.181315 -0.082663 -0.121918 -0.028470 -0.071601 -0.111757 -0.074174 -0.023978 -0.902197 -0.896337 -0.985371 -0.963143 -0.970407 -0.962360 -0.951715 -0.982981 -0.979380 -0.979619 -0.170346 -0.210071 -0.008683 -0.172426 -0.108358 -0.168676 -0.194576 -0.123404 -0.948036 -0.944909 -0.994420 -0.972623 -0.982051 -0.979887 -0.974322 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 2 AdaptableSnail 209576908 -0.875 -0.636364 -1.0 0.0 -0.636430 -0.939723 -0.647059 0.105263 -0.026853 -0.11836 -0.111111 -0.939807 -0.992425 -0.989159 -0.985311 -0.181315 -0.082663 -0.121918 -0.028470 -0.071601 -0.111757 -0.074174 -0.023978 -0.999587 -0.974984 -0.877168 -0.975087 -0.990738 -0.974640 -0.871448 -0.979055 -0.969977 -0.966727 -0.170346 -0.210071 -0.008683 -0.172426 -0.108358 -0.168676 -0.194576 -0.123404 -0.902014 -0.888578 -0.873141 -0.914698 -0.968340 -0.718290 -0.570016 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 3 AdaptableSnail 278643799 -0.875 -0.636364 -1.0 0.0 -0.636430 -0.939268 -0.647059 0.057208 -0.026853 -0.11836 -0.111111 -0.939348 -0.966929 -0.962044 -0.956604 0.249285 0.543860 0.459263 0.595970 0.446340 0.394311 0.551154 0.563325 -0.848530 -0.836349 -0.999736 -0.943965 -0.968168 -0.964868 -0.877953 -0.972472 -0.970371 -0.962565 -0.656161 0.038446 -0.285044 -0.260822 -0.376917 -0.198067 -0.358580 -0.246975 -0.882749 -0.890343 -1.000000 -0.947649 -0.969692 -0.959646 -0.900788 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 4 AdaptableSnail 351967631 -0.875 -0.636364 -1.0 0.0 -0.636430 -0.940997 -0.647059 0.034325 -0.061224 -0.11836 -0.111111 -0.941077 -0.984569 -0.979558 -0.979380 -0.181315 -0.082663 -0.121918 -0.028470 -0.071601 -0.111757 -0.074174 -0.023978 -0.949500 -0.938457 -0.877168 -0.941749 -0.984812 -0.982970 -0.921578 -0.965989 -0.959242 -0.959233 -0.170346 -0.210071 -0.008683 -0.172426 -0.108358 -0.168676 -0.194576 -0.123404 -0.859333 -0.859062 -0.993936 -0.924538 -0.964023 -0.948110 -0.910008 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 <p>5 rows \u00d7 169 columns</p> In\u00a0[47]: Copied! <pre>feature_cols = [c for c in train_tracking_features.columns if c not in ('lab_id', 'video_id', 'mouse1_id', 'mouse2_id', 'mouse_3_id', 'mouse4_id', 'body_parts_tracked', 'behaviors_labeled')]\ntraining_dataset = train_tracking_features.merge(annotation_schema[['lab_id', 'video_id', 'unique_actions']], on=['lab_id', 'video_id'], how='inner')\ntraining_dataset = training_dataset.dropna(subset=['unique_actions']).copy()\ntraining_dataset['unique_actions'] = training_dataset['unique_actions'].astype(int)\n\ncounts = sorted(training_dataset['unique_actions'].unique())\ncount_to_idx = {count: idx for idx, count in enumerate(counts)}\nidx_to_count = {idx: count for count, idx in count_to_idx.items()}\n\nX = training_dataset[feature_cols].to_numpy(dtype=np.float64)\ny = training_dataset['unique_actions'].map(count_to_idx).to_numpy(dtype=int)\n</pre> feature_cols = [c for c in train_tracking_features.columns if c not in ('lab_id', 'video_id', 'mouse1_id', 'mouse2_id', 'mouse_3_id', 'mouse4_id', 'body_parts_tracked', 'behaviors_labeled')] training_dataset = train_tracking_features.merge(annotation_schema[['lab_id', 'video_id', 'unique_actions']], on=['lab_id', 'video_id'], how='inner') training_dataset = training_dataset.dropna(subset=['unique_actions']).copy() training_dataset['unique_actions'] = training_dataset['unique_actions'].astype(int)  counts = sorted(training_dataset['unique_actions'].unique()) count_to_idx = {count: idx for idx, count in enumerate(counts)} idx_to_count = {idx: count for count, idx in count_to_idx.items()}  X = training_dataset[feature_cols].to_numpy(dtype=np.float64) y = training_dataset['unique_actions'].map(count_to_idx).to_numpy(dtype=int) In\u00a0[48]: Copied! <pre>X_train, X_valid, X_test, y_train, y_valid, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.15,\n    val_size=0.15,\n    random_state=42,\n)\n\nX_train_np = to_numpy_array(X_train, dtype=np.float64)\nX_valid_np = to_numpy_array(X_valid, dtype=np.float64)\nX_test_np = to_numpy_array(X_test, dtype=np.float64)\ny_train_np = to_numpy_array(y_train, dtype=int)\ny_valid_np = to_numpy_array(y_valid, dtype=int)\ny_test_np = to_numpy_array(y_test, dtype=int)\n\nbaseline_model = train_mlp_classifier(\n    X_train_np,\n    y_train_np,\n    hidden_layers=[128, 64, 32],\n    output_dim=len(counts),\n    lr=0.05,\n    max_epochs=3000,\n    batch_size=64,\n    random_state=42,\n    l2=0.01,\n    track_history=True,\n)\n\nproba_train = baseline_model.predict_proba(X_train_np)\nproba_test = baseline_model.predict_proba(X_test_np)\n\npred_train = proba_train.argmax(axis=1)\npred_test = proba_test.argmax(axis=1)\n\ntrain_acc = accuracy_score(y_train_np, pred_train)\ntest_acc = accuracy_score(y_test_np, pred_test)\n\nprint(f'Acuracia (treino): {train_acc:.3f}')\nprint(f'Acuracia (teste): {test_acc:.3f}')\n</pre> X_train, X_valid, X_test, y_train, y_valid, y_test = train_test_split(     X,     y,     test_size=0.15,     val_size=0.15,     random_state=42, )  X_train_np = to_numpy_array(X_train, dtype=np.float64) X_valid_np = to_numpy_array(X_valid, dtype=np.float64) X_test_np = to_numpy_array(X_test, dtype=np.float64) y_train_np = to_numpy_array(y_train, dtype=int) y_valid_np = to_numpy_array(y_valid, dtype=int) y_test_np = to_numpy_array(y_test, dtype=int)  baseline_model = train_mlp_classifier(     X_train_np,     y_train_np,     hidden_layers=[128, 64, 32],     output_dim=len(counts),     lr=0.05,     max_epochs=3000,     batch_size=64,     random_state=42,     l2=0.01,     track_history=True, )  proba_train = baseline_model.predict_proba(X_train_np) proba_test = baseline_model.predict_proba(X_test_np)  pred_train = proba_train.argmax(axis=1) pred_test = proba_test.argmax(axis=1)  train_acc = accuracy_score(y_train_np, pred_train) test_acc = accuracy_score(y_test_np, pred_test)  print(f'Acuracia (treino): {train_acc:.3f}') print(f'Acuracia (teste): {test_acc:.3f}')  <pre>Acuracia (treino): 0.955\nAcuracia (teste): 0.946\n</pre> In\u00a0[49]: Copied! <pre>if baseline_model.track_history and baseline_model.loss_history_ and baseline_model.acc_history_:\n    plt.figure(figsize=(6, 3))\n    plt.plot(baseline_model.loss_history_, label='loss')\n    plt.xlabel('Epoca')\n    plt.ylabel('Loss')\n    plt.title('Historico de treino da MLP')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n    plt.plot(baseline_model.acc_history_, label='metric')\n    plt.xlabel('Epoca')\n    plt.ylabel('Acuracia')\n    plt.title('Historico de treino da MLP')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Historico de loss indisponivel.')\n</pre> if baseline_model.track_history and baseline_model.loss_history_ and baseline_model.acc_history_:     plt.figure(figsize=(6, 3))     plt.plot(baseline_model.loss_history_, label='loss')     plt.xlabel('Epoca')     plt.ylabel('Loss')     plt.title('Historico de treino da MLP')     plt.legend()     plt.tight_layout()     plt.show()     plt.plot(baseline_model.acc_history_, label='metric')     plt.xlabel('Epoca')     plt.ylabel('Acuracia')     plt.title('Historico de treino da MLP')     plt.legend()     plt.tight_layout()     plt.show() else:     print('Historico de loss indisponivel.')  In\u00a0[50]: Copied! <pre>X_train_full = np.vstack([X_train_np, X_test_np])\ny_train_full = np.concatenate([y_train_np, y_test_np])\n\nfull_model = train_mlp_classifier(\n    X_train_full,\n    y_train_full,\n    hidden_layers=[128, 64, 32],\n    output_dim=len(counts),\n    lr=0.05,\n    max_epochs=3000,\n    batch_size=64,\n    random_state=42,\n    l2=0.01,\n    track_history=True,\n)\n\npred_full_train = full_model.predict(X_train_full)\npred_full_valid = full_model.predict(X_valid_np)\n</pre> X_train_full = np.vstack([X_train_np, X_test_np]) y_train_full = np.concatenate([y_train_np, y_test_np])  full_model = train_mlp_classifier(     X_train_full,     y_train_full,     hidden_layers=[128, 64, 32],     output_dim=len(counts),     lr=0.05,     max_epochs=3000,     batch_size=64,     random_state=42,     l2=0.01,     track_history=True, )  pred_full_train = full_model.predict(X_train_full) pred_full_valid = full_model.predict(X_valid_np)  In\u00a0[51]: Copied! <pre>train_full_acc = accuracy_score(y_train_full, pred_full_train)\nvalid_full_acc = accuracy_score(y_valid_np, pred_full_valid)\nprint(f'Acuracia (full model - validacao): {valid_full_acc:.3f}')\nmetrics = precision_recall_f1(y_train_full, pred_full_train, labels=np.unique(y_train_full))\nfor label, vals in metrics.items():\n    print(f\"Classe {label}: \"\n          f\"precision={vals['precision']:.3f}, \"\n          f\"recall={vals['recall']:.3f}, \"\n          f\"f1={vals['f1']:.3f}\")\n\ncm = confusion_matrix_true(y_train_full, pred_full_train)\nprint(\"\\nMatriz de confus\u00e3o\")\nprint(cm)\n</pre> train_full_acc = accuracy_score(y_train_full, pred_full_train) valid_full_acc = accuracy_score(y_valid_np, pred_full_valid) print(f'Acuracia (full model - validacao): {valid_full_acc:.3f}') metrics = precision_recall_f1(y_train_full, pred_full_train, labels=np.unique(y_train_full)) for label, vals in metrics.items():     print(f\"Classe {label}: \"           f\"precision={vals['precision']:.3f}, \"           f\"recall={vals['recall']:.3f}, \"           f\"f1={vals['f1']:.3f}\")  cm = confusion_matrix_true(y_train_full, pred_full_train) print(\"\\nMatriz de confus\u00e3o\") print(cm) <pre>Acuracia (full model - validacao): 0.946\nClasse 0: precision=1.000, recall=1.000, f1=1.000\nClasse 1: precision=0.695, recall=0.512, f1=0.590\nClasse 2: precision=0.519, recall=0.911, f1=0.661\nClasse 3: precision=0.553, recall=0.302, f1=0.391\nClasse 4: precision=0.567, recall=0.376, f1=0.452\nClasse 5: precision=0.553, recall=0.441, f1=0.491\nClasse 6: precision=0.750, recall=0.559, f1=0.641\nClasse 7: precision=0.900, recall=0.500, f1=0.643\nClasse 8: precision=1.000, recall=1.000, f1=1.000\nClasse 9: precision=0.000, recall=0.000, f1=0.000\nClasse 10: precision=0.875, recall=1.000, f1=0.933\nClasse 11: precision=1.000, recall=1.000, f1=1.000\nClasse 12: precision=1.000, recall=0.500, f1=0.667\nClasse 13: precision=0.000, recall=0.000, f1=0.000\nClasse 14: precision=0.000, recall=0.000, f1=0.000\nClasse 15: precision=0.000, recall=0.000, f1=0.000\nClasse 16: precision=0.300, recall=1.000, f1=0.462\n\nMatriz de confus\u00e3o\n      0   1    2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n0   6733   0    0   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n1      0  41   34   3   1   1   0   0   0   0   0   0   0   0   0   0   0\n2      0   8  205   8   4   0   0   0   0   0   0   0   0   0   0   0   0\n3      0   7  100  52  11   1   1   0   0   0   0   0   0   0   0   0   0\n4      0   3   37  11  38  12   0   0   0   0   0   0   0   0   0   0   0\n5      0   0   12   4   7  26  10   0   0   0   0   0   0   0   0   0   0\n6      0   0    5   9   5   7  33   0   0   0   0   0   0   0   0   0   0\n7      0   0    2   6   1   0   0   9   0   0   0   0   0   0   0   0   0\n8      0   0    0   0   0   0   0   0   2   0   0   0   0   0   0   0   0\n9      0   0    0   0   0   0   0   1   0   0   1   0   0   0   0   0   0\n10     0   0    0   0   0   0   0   0   0   0   7   0   0   0   0   0   0\n11     0   0    0   0   0   0   0   0   0   0   0   2   0   0   0   0   0\n12     0   0    0   0   0   0   0   0   0   0   0   0   1   0   0   0   1\n13     0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n14     0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   2\n15     0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   3\n16     0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   3\n</pre> In\u00a0[52]: Copied! <pre>full_feature_df = train_tracking_features[['lab_id', 'video_id'] + feature_cols].copy()\nlabels_df = annotation_schema[['lab_id', 'video_id', 'unique_actions']].copy()\nfull_unique_df = full_feature_df.merge(labels_df, on=['lab_id', 'video_id'], how='left')\n\nmissing_mask = full_unique_df['unique_actions'].isna()\nif missing_mask.any():\n    X_missing = to_numpy_array(full_unique_df.loc[missing_mask, feature_cols], dtype=np.float64)\n    preds_missing = full_model.predict(X_missing)\n    full_unique_df.loc[missing_mask, 'unique_actions'] = [idx_to_count[int(idx)] for idx in preds_missing]\n\nfull_unique_df['unique_actions'] = full_unique_df['unique_actions'].astype(int)\nprint('Dataset final (videos x colunas):', full_unique_df.shape)\ndisplay(full_unique_df.head())\n</pre> full_feature_df = train_tracking_features[['lab_id', 'video_id'] + feature_cols].copy() labels_df = annotation_schema[['lab_id', 'video_id', 'unique_actions']].copy() full_unique_df = full_feature_df.merge(labels_df, on=['lab_id', 'video_id'], how='left')  missing_mask = full_unique_df['unique_actions'].isna() if missing_mask.any():     X_missing = to_numpy_array(full_unique_df.loc[missing_mask, feature_cols], dtype=np.float64)     preds_missing = full_model.predict(X_missing)     full_unique_df.loc[missing_mask, 'unique_actions'] = [idx_to_count[int(idx)] for idx in preds_missing]  full_unique_df['unique_actions'] = full_unique_df['unique_actions'].astype(int) print('Dataset final (videos x colunas):', full_unique_df.shape) display(full_unique_df.head())  <pre>Dataset final (videos x colunas): (8790, 170)\n</pre> lab_id video_id mouse1_age mouse2_age mouse3_age mouse4_age frames_per_second video_duration_sec pix_per_cm_approx video_width_pix video_height_pix arena_width_cm arena_height_cm total_frames mean_movement_mouse1_body_center mean_movement_mouse1_ear_left mean_movement_mouse1_ear_right mean_movement_mouse1_headpiece_bottombackleft mean_movement_mouse1_headpiece_bottombackright mean_movement_mouse1_headpiece_bottomfrontleft mean_movement_mouse1_headpiece_bottomfrontright mean_movement_mouse1_headpiece_topbackleft mean_movement_mouse1_headpiece_topbackright mean_movement_mouse1_headpiece_topfrontleft mean_movement_mouse1_headpiece_topfrontright mean_movement_mouse1_lateral_left mean_movement_mouse1_lateral_right mean_movement_mouse1_neck mean_movement_mouse1_nose mean_movement_mouse1_tail_base mean_movement_mouse1_tail_midpoint mean_movement_mouse1_tail_tip mean_movement_mouse2_body_center mean_movement_mouse2_ear_left mean_movement_mouse2_ear_right mean_movement_mouse2_headpiece_bottombackleft mean_movement_mouse2_headpiece_bottombackright mean_movement_mouse2_headpiece_bottomfrontleft mean_movement_mouse2_headpiece_bottomfrontright mean_movement_mouse2_headpiece_topbackleft mean_movement_mouse2_headpiece_topbackright mean_movement_mouse2_headpiece_topfrontleft mean_movement_mouse2_headpiece_topfrontright mean_movement_mouse2_lateral_left mean_movement_mouse2_lateral_right mean_movement_mouse2_neck mean_movement_mouse2_nose mean_movement_mouse2_tail_base mean_movement_mouse2_tail_midpoint mean_movement_mouse2_tail_tip ... mouse1_strain_c57bl_6j_x_ai148 mouse1_strain_c57bl_6n mouse1_strain_cd_1__icr mouse1_strain_cd1 mouse1_strain_cfw mouse1_color_black mouse1_color_black_and_tan mouse1_color_brown mouse1_color_white mouse1_sex_female mouse1_sex_male mouse2_strain_129_svevtac mouse2_strain_balb_c mouse2_strain_btbr mouse2_strain_c57bl_6j mouse2_strain_c57bl_6n mouse2_strain_cd_1__icr mouse2_strain_cd1 mouse2_strain_cfw mouse2_color_black mouse2_color_black_and_tan mouse2_color_brown mouse2_color_white mouse2_sex_female mouse2_sex_male mouse3_strain_btbr mouse3_strain_c57bl_6j mouse3_strain_cd_1__icr mouse3_color_black mouse3_color_black_and_tan mouse3_color_white mouse3_sex_male mouse4_strain_cd_1__icr mouse4_color_white mouse4_sex_male mouse4_condition_wireless_device arena_shape_circular arena_shape_rectangular arena_shape_split_rectangluar arena_shape_square arena_type_csds arena_type_divided_territories arena_type_familiar arena_type_neutral arena_type_resident_intruder tracking_method_deeplabcut tracking_method_mars tracking_method_sleap tracking_method_custom_hrnet unique_actions 0 AdaptableSnail 44566106 -0.875 -0.636364 -1.0 0.0 -0.636430 -0.939683 -0.647059 0.061785 -0.061224 -0.11836 -0.111111 -0.939759 -0.960432 -0.948686 -0.951348 0.459698 0.625304 1.000000 0.849507 0.818705 0.656221 0.959277 0.814653 -0.783642 -0.776518 -0.997528 -0.938540 -0.956437 -0.941935 -0.880600 -0.964879 -0.954465 -0.956896 0.346775 -0.217390 1.000000 0.087506 0.168588 -0.007170 0.024077 0.145082 -0.851819 -0.851773 -0.976947 -0.945099 -0.964225 -0.953431 -0.906248 ... 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 7 1 AdaptableSnail 143861384 -0.875 -0.636364 -1.0 0.0 -0.727322 -0.638055 -0.858824 -0.235698 -0.555317 -0.11836 -0.111111 -0.699761 -0.975767 -0.969271 -0.968166 -0.181315 -0.082663 -0.121918 -0.028470 -0.071601 -0.111757 -0.074174 -0.023978 -0.902197 -0.896337 -0.985371 -0.963143 -0.970407 -0.962360 -0.951715 -0.982981 -0.979380 -0.979619 -0.170346 -0.210071 -0.008683 -0.172426 -0.108358 -0.168676 -0.194576 -0.123404 -0.948036 -0.944909 -0.994420 -0.972623 -0.982051 -0.979887 -0.974322 ... 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 4 2 AdaptableSnail 209576908 -0.875 -0.636364 -1.0 0.0 -0.636430 -0.939723 -0.647059 0.105263 -0.026853 -0.11836 -0.111111 -0.939807 -0.992425 -0.989159 -0.985311 -0.181315 -0.082663 -0.121918 -0.028470 -0.071601 -0.111757 -0.074174 -0.023978 -0.999587 -0.974984 -0.877168 -0.975087 -0.990738 -0.974640 -0.871448 -0.979055 -0.969977 -0.966727 -0.170346 -0.210071 -0.008683 -0.172426 -0.108358 -0.168676 -0.194576 -0.123404 -0.902014 -0.888578 -0.873141 -0.914698 -0.968340 -0.718290 -0.570016 ... 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 7 3 AdaptableSnail 278643799 -0.875 -0.636364 -1.0 0.0 -0.636430 -0.939268 -0.647059 0.057208 -0.026853 -0.11836 -0.111111 -0.939348 -0.966929 -0.962044 -0.956604 0.249285 0.543860 0.459263 0.595970 0.446340 0.394311 0.551154 0.563325 -0.848530 -0.836349 -0.999736 -0.943965 -0.968168 -0.964868 -0.877953 -0.972472 -0.970371 -0.962565 -0.656161 0.038446 -0.285044 -0.260822 -0.376917 -0.198067 -0.358580 -0.246975 -0.882749 -0.890343 -1.000000 -0.947649 -0.969692 -0.959646 -0.900788 ... 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 7 4 AdaptableSnail 351967631 -0.875 -0.636364 -1.0 0.0 -0.636430 -0.940997 -0.647059 0.034325 -0.061224 -0.11836 -0.111111 -0.941077 -0.984569 -0.979558 -0.979380 -0.181315 -0.082663 -0.121918 -0.028470 -0.071601 -0.111757 -0.074174 -0.023978 -0.949500 -0.938457 -0.877168 -0.941749 -0.984812 -0.982970 -0.921578 -0.965989 -0.959242 -0.959233 -0.170346 -0.210071 -0.008683 -0.172426 -0.108358 -0.168676 -0.194576 -0.123404 -0.859333 -0.859062 -0.993936 -0.924538 -0.964023 -0.948110 -0.910008 ... 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 7 <p>5 rows \u00d7 170 columns</p> In\u00a0[53]: Copied! <pre>min_unique = full_unique_df['unique_actions'].min()\nmax_unique = full_unique_df['unique_actions'].max()\nif max_unique &gt; min_unique:\n    full_unique_df['unique_actions_scaled'] = 2 * (full_unique_df['unique_actions'] - min_unique) / (max_unique - min_unique) - 1\nelse:\n    full_unique_df['unique_actions_scaled'] = 0.0\n\nprint('Intervalo original:', min_unique, '-&gt;', max_unique)\nprint('Intervalo escalonado:', full_unique_df['unique_actions_scaled'].min(), '-&gt;', full_unique_df['unique_actions_scaled'].max())\n</pre> min_unique = full_unique_df['unique_actions'].min() max_unique = full_unique_df['unique_actions'].max() if max_unique &gt; min_unique:     full_unique_df['unique_actions_scaled'] = 2 * (full_unique_df['unique_actions'] - min_unique) / (max_unique - min_unique) - 1 else:     full_unique_df['unique_actions_scaled'] = 0.0  print('Intervalo original:', min_unique, '-&gt;', max_unique) print('Intervalo escalonado:', full_unique_df['unique_actions_scaled'].min(), '-&gt;', full_unique_df['unique_actions_scaled'].max())  <pre>Intervalo original: 0 -&gt; 17\nIntervalo escalonado: -1.0 -&gt; 1.0\n</pre> In\u00a0[54]: Copied! <pre>action_labels = annotation_schema[['lab_id', 'video_id', 'top_action']].dropna(subset=['top_action']).copy()\nfull_actions_df = full_unique_df.merge(action_labels, on=['lab_id', 'video_id'], how='left')\nobserved_mask = full_actions_df['top_action'].notna()\n\nif observed_mask.sum() == 0:\n    raise RuntimeError('Nenhuma acao predominante disponivel para treinamento.')\n\ny_action_raw = full_actions_df.loc[observed_mask, 'top_action']\ny_action_codes, action_levels = pd.factorize(y_action_raw, sort=True)\naction_levels = list(action_levels)\n\nX_action = to_numpy_array(full_actions_df.loc[observed_mask, feature_cols], dtype=np.float64)\ny_action = to_numpy_array(y_action_codes, dtype=int)\nX_tr_act, X_va_act, X_te_act, y_tr_act, y_va_act, y_te_act = train_test_split(\n    X_action,\n    y_action,\n    test_size=0.15,\n    val_size=0.15,\n    random_state=42,\n)\n\naction_model = train_mlp_classifier(\n    X_tr_act,\n    y_tr_act,\n    hidden_layers=[128, 64, 32],\n    output_dim=len(action_levels),\n    lr=0.05,\n    max_epochs=3000,\n    batch_size=64,\n    random_state=42,\n    l2=0.01,\n    track_history=True,\n)\n\npred_tr = action_model.predict(X_tr_act)\npred_te = action_model.predict(X_te_act)\nprint('Acuracia (treino):', accuracy_score(y_tr_act, pred_tr))\nprint('Acuracia (teste):', accuracy_score(y_te_act, pred_te))\n</pre>  action_labels = annotation_schema[['lab_id', 'video_id', 'top_action']].dropna(subset=['top_action']).copy() full_actions_df = full_unique_df.merge(action_labels, on=['lab_id', 'video_id'], how='left') observed_mask = full_actions_df['top_action'].notna()  if observed_mask.sum() == 0:     raise RuntimeError('Nenhuma acao predominante disponivel para treinamento.')  y_action_raw = full_actions_df.loc[observed_mask, 'top_action'] y_action_codes, action_levels = pd.factorize(y_action_raw, sort=True) action_levels = list(action_levels)  X_action = to_numpy_array(full_actions_df.loc[observed_mask, feature_cols], dtype=np.float64) y_action = to_numpy_array(y_action_codes, dtype=int) X_tr_act, X_va_act, X_te_act, y_tr_act, y_va_act, y_te_act = train_test_split(     X_action,     y_action,     test_size=0.15,     val_size=0.15,     random_state=42, )  action_model = train_mlp_classifier(     X_tr_act,     y_tr_act,     hidden_layers=[128, 64, 32],     output_dim=len(action_levels),     lr=0.05,     max_epochs=3000,     batch_size=64,     random_state=42,     l2=0.01,     track_history=True, )  pred_tr = action_model.predict(X_tr_act) pred_te = action_model.predict(X_te_act) print('Acuracia (treino):', accuracy_score(y_tr_act, pred_tr)) print('Acuracia (teste):', accuracy_score(y_te_act, pred_te)) <pre>Acuracia (treino): 0.8743801652892562\nAcuracia (teste): 0.7906976744186046\n</pre> In\u00a0[55]: Copied! <pre>X_train_full_act = np.vstack([X_tr_act, X_te_act])\ny_train_full_act = np.concatenate([y_tr_act, y_te_act])\naction_full_model = train_mlp_classifier(\n    X_train_full_act,\n    y_train_full_act,\n    hidden_layers=[128, 64, 32],\n    output_dim=len(action_levels),\n    lr=0.05,\n    max_epochs=3000,\n    batch_size=64,\n    random_state=42,\n    l2=0.01,\n    track_history=True,\n)\n\npred_va_full = action_full_model.predict(X_va_act)\n</pre> X_train_full_act = np.vstack([X_tr_act, X_te_act]) y_train_full_act = np.concatenate([y_tr_act, y_te_act]) action_full_model = train_mlp_classifier(     X_train_full_act,     y_train_full_act,     hidden_layers=[128, 64, 32],     output_dim=len(action_levels),     lr=0.05,     max_epochs=3000,     batch_size=64,     random_state=42,     l2=0.01,     track_history=True, )  pred_va_full = action_full_model.predict(X_va_act) In\u00a0[56]: Copied! <pre>print('Acuracia (full model - validacao):', accuracy_score(y_va_act, pred_va_full))\nmetrics = precision_recall_f1(y_va_act, pred_va_full, labels=np.unique(y_va_act))\nfor label, vals in metrics.items():\n    print(f\"Classe {label}: \"\n          f\"precision={vals['precision']:.3f}, \"\n          f\"recall={vals['recall']:.3f}, \"\n          f\"f1={vals['f1']:.3f}\")\n\ncm = confusion_matrix_true(y_va_act, pred_va_full)\nprint(\"\\nMatriz de confus\u00e3o\")\nprint(cm)\n</pre> print('Acuracia (full model - validacao):', accuracy_score(y_va_act, pred_va_full)) metrics = precision_recall_f1(y_va_act, pred_va_full, labels=np.unique(y_va_act)) for label, vals in metrics.items():     print(f\"Classe {label}: \"           f\"precision={vals['precision']:.3f}, \"           f\"recall={vals['recall']:.3f}, \"           f\"f1={vals['f1']:.3f}\")  cm = confusion_matrix_true(y_va_act, pred_va_full) print(\"\\nMatriz de confus\u00e3o\") print(cm) <pre>Acuracia (full model - validacao): 0.6434108527131783\nClasse 0: precision=0.000, recall=0.000, f1=0.000\nClasse 1: precision=0.722, recall=0.619, f1=0.667\nClasse 3: precision=0.000, recall=0.000, f1=0.000\nClasse 4: precision=0.000, recall=0.000, f1=0.000\nClasse 5: precision=0.333, recall=1.000, f1=0.500\nClasse 7: precision=0.333, recall=0.600, f1=0.429\nClasse 8: precision=0.000, recall=0.000, f1=0.000\nClasse 9: precision=0.000, recall=0.000, f1=0.000\nClasse 10: precision=0.000, recall=0.000, f1=0.000\nClasse 11: precision=0.333, recall=0.250, f1=0.286\nClasse 12: precision=0.667, recall=0.500, f1=0.571\nClasse 15: precision=0.778, recall=0.889, f1=0.830\nClasse 16: precision=0.556, recall=0.312, f1=0.400\nClasse 18: precision=0.286, recall=0.400, f1=0.333\n\nMatriz de confus\u00e3o\n    0   1   3   4   5   7   8   9   10  11  12  13  15  16  17  18\n0    0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n1    0  13   1   0   0   1   1   0   0   0   0   0   3   2   0   0\n3    0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   2\n4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n5    0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n7    0   0   0   0   2   3   0   0   0   0   0   0   0   0   0   0\n8    0   0   0   0   0   3   0   0   0   0   0   0   0   0   0   0\n9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n10   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n11   0   2   0   0   0   1   0   0   0   1   0   0   0   0   0   0\n12   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   2\n13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n15   0   1   0   0   0   0   0   0   0   2   0   1  56   2   1   0\n16   0   0   0   0   0   0   0   0   0   0   0   0  11   5   0   0\n17   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n18   0   1   0   0   0   0   0   0   0   0   1   0   1   0   0   2\n</pre> In\u00a0[57]: Copied! <pre>if action_model.track_history and action_model.loss_history_ and action_model.acc_history_:\n    plt.figure(figsize=(6, 3))\n    plt.plot(action_model.loss_history_, label='loss')\n    plt.xlabel('Epoca')\n    plt.ylabel('Loss')\n    plt.title('Historico de treino da MLP')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n    plt.plot(action_model.acc_history_, label='metric')\n    plt.xlabel('Epoca')\n    plt.ylabel('Acuracia')\n    plt.title('Historico de treino da MLP')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Historico de loss indisponivel.')\n</pre> if action_model.track_history and action_model.loss_history_ and action_model.acc_history_:     plt.figure(figsize=(6, 3))     plt.plot(action_model.loss_history_, label='loss')     plt.xlabel('Epoca')     plt.ylabel('Loss')     plt.title('Historico de treino da MLP')     plt.legend()     plt.tight_layout()     plt.show()     plt.plot(action_model.acc_history_, label='metric')     plt.xlabel('Epoca')     plt.ylabel('Acuracia')     plt.title('Historico de treino da MLP')     plt.legend()     plt.tight_layout()     plt.show() else:     print('Historico de loss indisponivel.')"},{"location":"classification/model/#introducao","title":"Introdu\u00e7\u00e3o\u00b6","text":"<p>O presente projeto tem como objetivo aplicar t\u00e9cnicas de Deep Learning para resolver um problema de classifica\u00e7\u00e3o multiclasse utilizando redes neurais artificiais. O dataset escolhido, oriundo do MABe Challenge \u2013 Social Action Recognition in Mice, cont\u00e9m informa\u00e7\u00f5es de v\u00eddeos de camundongos e respectivas anota\u00e7\u00f5es de comportamento. O foco \u00e9 prever a a\u00e7\u00e3o realizada a cada Frame dos v\u00eddeos a partir dos dados fornecidos. Por quest\u00f5es de processamento necess\u00e1rio e tempo para realiza\u00e7\u00e3o do projeto, este foi simplificado para predi\u00e7\u00e3o da a\u00e7\u00e3o predominante em cada v\u00eddeo analisado do dataset.</p>"},{"location":"classification/model/#grupo","title":"Grupo\u00b6","text":"<p>Caio Ortega Boa , Pedro Toledo Piza Civita, Gabriel Mendon\u00e7a de Mello Hermida</p>"},{"location":"classification/model/#roadmap-do-projeto","title":"Roadmap do Projeto\u00b6","text":"<p>O desenvolvimento foi conduzido nas seguintes etapas:</p> <ol> <li><p>Defini\u00e7\u00e3o do Modelo MLP e Fun\u00e7\u00f5es \u00dateis</p> <ul> <li>Scaler utilizado para normaliza\u00e7\u00e3o das features.</li> <li>Defini\u00e7\u00e3o da arquitetura de MLP utilizada.</li> <li>Fun\u00e7\u00f5es \u00fateis utilizadas no projeto.</li> </ul> </li> <li><p>An\u00e1lise Explorat\u00f3ria dos Dados (EDA)</p> <ul> <li>Inspe\u00e7\u00e3o das vari\u00e1veis dispon\u00edveis no dataset.</li> <li>Verifica\u00e7\u00e3o da distribui\u00e7\u00e3o das classes-alvo e balanceamento do conjunto.</li> <li>Identifica\u00e7\u00e3o de valores faltantes, inconsist\u00eancias e outliers.</li> </ul> </li> <li><p>Tratamento Inicial dos Dados</p> <ul> <li>Pipeline de tratamento para dados do projeto.</li> <li>Normaliza\u00e7\u00e3o ou padroniza\u00e7\u00e3o de atributos num\u00e9ricos.</li> <li>Codifica\u00e7\u00e3o das vari\u00e1veis categ\u00f3ricas para uso no modelo.</li> <li>Gera\u00e7\u00e3o de features.</li> <li>Tratamento de valores faltantes.</li> </ul> </li> <li><p>**Treinamento do Modelo para Predi\u00e7\u00e3o de *Unique Actions***</p> <ul> <li>Primeira fase de treinamento direcionada a prever o n\u00famero de a\u00e7\u00f5es distintas (unique actions) presentes no v\u00eddeo.</li> <li>Registro do desempenho com m\u00e9tricas quantitativas.</li> <li>Implementa\u00e7\u00e3o do output do modelo como features para predi\u00e7\u00e3o futura.</li> </ul> </li> <li><p>Treinamento do Modelo Final</p> <ul> <li>Constru\u00e7\u00e3o do modelo definitivo para prever a a\u00e7\u00e3o predominante.</li> <li>An\u00e1lise de curvas de erro (loss e accuracy) para verificar converg\u00eancia.</li> </ul> </li> </ol>"},{"location":"classification/model/#descricao-do-dataset-mabe-calms21-mouse-behavior-detection","title":"Descri\u00e7\u00e3o do Dataset: MABe / CalMS21 \u2014 Mouse Behavior Detection\u00b6","text":"<p>A competi\u00e7\u00e3o utilizada como fonte deste projeto \u00e9 MABe \u2014 Mouse Behavior Detection publicada no Kaggle: https://www.kaggle.com/competitions/MABe-mouse-behavior-detection/data</p> <p>A base de dados \u00e9 derivada do dataset CalMS21 (Caltech Mouse Social Interactions 2021), utilizado no desafio MABe de reconhecimento de comportamentos sociais entre camundongos.</p>"},{"location":"classification/model/#principais-caracteristicas","title":"Principais caracter\u00edsticas\u00b6","text":"Item Descri\u00e7\u00e3o Tipo de problema Classifica\u00e7\u00e3o multiclasse / frame-level behavior detection Dom\u00ednio Comportamento social de camundongos a partir de dados de pose estimada Formato dos dados Sequ\u00eancias (v\u00eddeos) compostas por frames, onde cada frame cont\u00e9m:  - Coordenadas 2D de 7 keypoints para cada um dos dois animais (residente e intruso)  - Anota\u00e7\u00f5es de comportamento (quando dispon\u00edveis) Escala / amplitude Mais de 6 milh\u00f5es de frames n\u00e3o anotados (dados de pose apenas), e mais de 1 milh\u00e3o de frames anotados para comportamentos sociais. - Considerando apenas o dataset de train, esse possui um shape de 8790, 38. Licen\u00e7a / distribui\u00e7\u00e3o O CalMS21 \u00e9 distribu\u00eddo sob Creative Commons Attribution-NonCommercial-ShareAlike (CC-BY-NC-SA)."},{"location":"classification/model/#justificativa-para-escolha-deste-dataset","title":"Justificativa para escolha deste dataset\u00b6","text":"<ul> <li>Complexidade suficiente: o dataset oferece um desafio realista, com muitas amostras e m\u00faltiplas classes, superando o m\u00ednimo exigido (\u2265 1.000 amostras e \u2265 5 features).</li> <li>Originalidade e impacto: n\u00e3o \u00e9 uma base trivial ou j\u00e1 trivialmente explorada em cursos.</li> </ul>"},{"location":"classification/model/#arquitetura-inicial-cogitada","title":"Arquitetura Inicial Cogitada\u00b6","text":"<p>No in\u00edcio do projeto foi delineada uma arquitetura mais ambiciosa, com o objetivo de explorar ao m\u00e1ximo o potencial do dataset. A proposta envolvia m\u00faltiplas etapas integradas:</p> <ol> <li><p>Predi\u00e7\u00e3o de Unique Actions</p> <ul> <li>Modelo voltado para estimar o n\u00famero de a\u00e7\u00f5es distintas registradas em cada v\u00eddeo.</li> </ul> </li> <li><p>Montagem do Dataset Frame a Frame</p> <ul> <li>Estrutura\u00e7\u00e3o detalhada dos dados em n\u00edvel de quadro (frame), preservando informa\u00e7\u00f5es de cada instante temporal.</li> </ul> </li> <li><p>Predi\u00e7\u00e3o Frame a Frame da A\u00e7\u00e3o Realizada</p> <ul> <li>Treinamento de um modelo espec\u00edfico para classificar a a\u00e7\u00e3o em cada frame individualmente, capturando a din\u00e2mica temporal.</li> </ul> </li> <li><p>Treinamento de Modelo para Prever quais Ratos Participaram da A\u00e7\u00e3o</p> <ul> <li>Identifica\u00e7\u00e3o de quais indiv\u00edduos estavam envolvidos em cada a\u00e7\u00e3o observada, enriquecendo a granularidade das anota\u00e7\u00f5es.</li> </ul> </li> <li><p>Montagem do Dataset Final</p> <ul> <li>Integra\u00e7\u00e3o dos resultados anteriores em uma estrutura completa: para cada frame, indicar qual rato realizou qual a\u00e7\u00e3o sobre qual outro rato.</li> </ul> </li> </ol>"},{"location":"classification/model/#simplificacao-do-projeto","title":"Simplifica\u00e7\u00e3o do Projeto\u00b6","text":"<p>Devido a limita\u00e7\u00f5es de tempo e capacidade computacional, a arquitetura inicial precisou ser simplificada. O escopo foi ajustado para duas tarefas principais:</p> <ol> <li><p>Predi\u00e7\u00e3o de Unique Actions</p> <ul> <li>Modelo voltado apenas para prever o n\u00famero de a\u00e7\u00f5es distintas em cada v\u00eddeo, sem granularidade quadro a quadro.</li> </ul> </li> <li><p>Predi\u00e7\u00e3o da A\u00e7\u00e3o Mais Realizada no V\u00eddeo</p> <ul> <li>Treinamento de um modelo final para identificar a a\u00e7\u00e3o predominante em cada v\u00eddeo, reduzindo o problema a uma classifica\u00e7\u00e3o multiclasse direta.</li> </ul> </li> </ol>"},{"location":"classification/model/#definicao-do-modelo-mlp-e-funcoes-uteis","title":"Defini\u00e7\u00e3o do Modelo MLP e Fun\u00e7\u00f5es \u00dateis\u00b6","text":"<p>Nesta etapa do projeto foi constru\u00edda a base para o modelo de classifica\u00e7\u00e3o: um Multi-Layer Perceptron (MLP) implementado do zero em Python com <code>NumPy</code>. O objetivo \u00e9 compreender todos os passos envolvidos no treinamento de uma rede neural, desde a prepara\u00e7\u00e3o dos dados at\u00e9 o c\u00e1lculo das m\u00e9tricas finais.</p>"},{"location":"classification/model/#preparacao-do-ambiente","title":"Prepara\u00e7\u00e3o do Ambiente\u00b6","text":"<p>Foram importadas bibliotecas fundamentais para an\u00e1lise e manipula\u00e7\u00e3o de dados (<code>numpy</code>, <code>pandas</code>), visualiza\u00e7\u00e3o (<code>matplotlib</code>, <code>seaborn</code>), al\u00e9m de configura\u00e7\u00f5es para melhorar a exibi\u00e7\u00e3o de tabelas no notebook.</p>"},{"location":"classification/model/#funcoes-auxiliares","title":"Fun\u00e7\u00f5es Auxiliares\u00b6","text":"<p>Para viabilizar o treinamento e avalia\u00e7\u00e3o do modelo, foram definidas fun\u00e7\u00f5es essenciais:</p> <ul> <li>Normaliza\u00e7\u00e3o dos dados: implementa\u00e7\u00e3o do escalamento de vari\u00e1veis para o intervalo [-1, 1], garantindo melhor estabilidade durante o treinamento da rede para <code>tanh</code>.</li> <li>Fun\u00e7\u00f5es de ativa\u00e7\u00e3o: <code>tanh</code> e <code>softmax</code>, al\u00e9m de suas derivadas, necess\u00e1rias para a propaga\u00e7\u00e3o direta e retropropaga\u00e7\u00e3o.</li> <li>Fun\u00e7\u00e3o de perda: Cross-Entropy Loss, apropriada para classifica\u00e7\u00e3o multiclasse, calculando o erro entre as probabilidades previstas e os r\u00f3tulos verdadeiros.</li> <li>M\u00e9tricas de avalia\u00e7\u00e3o: <code>accuracy_score</code>, <code>f1_score</code>, <code>precision</code>, <code>recall</code> e <code>confusion_matrix</code> para medir a taxa de acertos do modelo.</li> <li>Divis\u00e3o do dataset: fun\u00e7\u00e3o <code>train_test_split</code>, que embaralha os dados e separa em treino e teste, garantindo reprodutibilidade via <code>random_state</code>.</li> <li>Inicializa\u00e7\u00e3o de pesos: m\u00e9todo <code>xavier_init</code>, que favorece uma inicializa\u00e7\u00e3o de pesos para propaga\u00e7\u00e3o de gradiente mais est\u00e1vel em redes profundas.</li> <li>Codifica\u00e7\u00e3o de r\u00f3tulos: fun\u00e7\u00e3o <code>one_hot</code>, que converte classes inteiras em vetores one-hot para uso na fun\u00e7\u00e3o de perda.</li> </ul>"},{"location":"classification/model/#estrutura-do-mlp","title":"Estrutura do MLP\u00b6","text":"<p>A classe <code>MLP</code> re\u00fane todos os componentes do modelo:</p> <ul> <li>Hiperpar\u00e2metros configur\u00e1veis: dimens\u00f5es de entrada e sa\u00edda, n\u00famero e tamanho das camadas escondidas, taxa de aprendizado (<code>lr</code>), n\u00famero de \u00e9pocas, tamanho de lote (<code>batch_size</code>), regulariza\u00e7\u00e3o L2 e semente aleat\u00f3ria para reprodutibilidade.</li> <li>Inicializa\u00e7\u00e3o de par\u00e2metros: pesos e vieses s\u00e3o inicializados com o m\u00e9todo de Xavier, garantindo maior efici\u00eancia no in\u00edcio do treino.</li> <li>Forward pass: realiza a propaga\u00e7\u00e3o dos dados pelas camadas, aplicando <code>tanh</code> nas camadas escondidas e <code>softmax</code> na sa\u00edda para gerar probabilidades.</li> <li>Backward pass: implementa a retropropaga\u00e7\u00e3o do erro, atualizando gradientes para pesos e vieses, com suporte a regulariza\u00e7\u00e3o L2.</li> <li>Fun\u00e7\u00e3o de atualiza\u00e7\u00e3o: aplica descida do gradiente para refinar os par\u00e2metros a cada itera\u00e7\u00e3o.</li> <li>Treinamento (<code>fit</code>): organiza o loop de \u00e9pocas, realiza o embaralhamento dos dados, treino em mini-batches, c\u00e1lculo da fun\u00e7\u00e3o de perda e registro hist\u00f3rico de loss e accuracy.</li> <li>Predi\u00e7\u00e3o: m\u00e9todos para retornar probabilidades (<code>predict_proba</code>), scores lineares (<code>decision_function</code>) e r\u00f3tulos finais (<code>predict</code>).</li> </ul>"},{"location":"classification/model/#pre-processamento-de-features","title":"Pr\u00e9-processamento de Features\u00b6","text":"<p>Para alinhar corretamente os dados de treino e infer\u00eancia, foi criada a classe <code>TrackingFeaturePreprocessor</code>. Ela garante consist\u00eancia no tratamento dos dados brutos de tracking e metadados, normalizando vari\u00e1veis num\u00e9ricas e codificando categorias.</p>"},{"location":"classification/model/#principais-funcionalidades","title":"Principais funcionalidades:\u00b6","text":"<ul> <li><p>Normaliza\u00e7\u00e3o num\u00e9rica</p> <ul> <li>C\u00e1lculo de estat\u00edsticas b\u00e1sicas (m\u00e9dia, m\u00ednimo, m\u00e1ximo) para cada coluna num\u00e9rica.</li> <li>Escalonamento para o intervalo [-1, 1], garantindo maior estabilidade ao treinamento da rede.</li> <li>Preenchimento de valores ausentes com a m\u00e9dia da coluna.</li> <li>Essa escolha foi feita porque a fun\u00e7\u00e3o de ativa\u00e7\u00e3o utilizada no MLP (<code>tanh</code>) tamb\u00e9m opera nesse mesmo intervalo, reduzindo riscos de satura\u00e7\u00e3o e acelerando a converg\u00eancia.</li> </ul> </li> <li><p>Codifica\u00e7\u00e3o categ\u00f3rica</p> <ul> <li>Identifica\u00e7\u00e3o dos n\u00edveis presentes em cada vari\u00e1vel categ\u00f3rica.</li> <li>Transforma\u00e7\u00e3o em vari\u00e1veis dummy (one-hot encoding) com nomes sanitizados, evitando problemas com espa\u00e7os ou caracteres especiais.</li> <li>Inclus\u00e3o expl\u00edcita de uma categoria <code>__missing__</code> para valores ausentes, assegurando que a aus\u00eancia seja tratada como informa\u00e7\u00e3o v\u00e1lida e n\u00e3o como erro.</li> <li>Essa decis\u00e3o evita perda de amostras e mant\u00e9m a consist\u00eancia em casos de infer\u00eancia com categorias ausentes no treino.</li> </ul> </li> <li><p>Constru\u00e7\u00e3o da matriz de features</p> <ul> <li>Montagem de um <code>DataFrame</code> consolidado com colunas-chave (<code>lab_id</code>, <code>video_id</code>), atributos num\u00e9ricos normalizados e categ\u00f3ricos codificados.</li> <li>Garantia de consist\u00eancia entre diferentes fases (treino, valida\u00e7\u00e3o e teste), mesmo quando h\u00e1 aus\u00eancia de determinadas colunas.</li> </ul> </li> <li><p>Tratamento de Colunas</p> <ul> <li>Colunas de <code>age</code> s\u00e3o tratadas separadamente atrav\u00e9s da m\u00e9dia do intervalo para serem tratadas como num\u00e9ricas.</li> <li>Colunas de <code>condition</code> removidas, por gerarem colunas excessivas em one hot e agregar pouca informa\u00e7\u00e3o.</li> <li>Colunas de <code>mouse_id</code>, <code>behaviours_labeled</code> e <code>body_parts_tracked</code> n\u00e3o utilizadas como features, pois n\u00e3o agregam informa\u00e7\u00e3o relevante, por\u00e9m s\u00e3o utilizadas na constru\u00e7\u00e3o de novas features e organiza\u00e7\u00e3o.</li> </ul> </li> <li><p>M\u00e9todos principais</p> <ul> <li><code>fit</code>: aprende estat\u00edsticas num\u00e9ricas e n\u00edveis categ\u00f3ricos a partir de um <code>DataFrame</code>.</li> <li><code>transform</code>: aplica a transforma\u00e7\u00e3o aprendida a novos dados, preservando as colunas de features.</li> <li><code>fit_transform</code>: combina\u00e7\u00e3o dos dois passos anteriores para conveni\u00eancia.</li> </ul> </li> </ul>"},{"location":"classification/model/#construcao-de-features-de-tracking","title":"Constru\u00e7\u00e3o de Features de Tracking\u00b6","text":"<p>Para complementar os metadados b\u00e1sicos, foi desenvolvida a fun\u00e7\u00e3o <code>build_tracking_features</code>. O objetivo \u00e9 transformar os arquivos de tracking (<code>.parquet</code>) em atributos quantitativos que representem o movimento dos camundongos ao longo dos v\u00eddeos, possibilitando sua utiliza\u00e7\u00e3o como entrada no MLP.</p>"},{"location":"classification/model/#estrutura-da-funcao","title":"Estrutura da Fun\u00e7\u00e3o\u00b6","text":"<ul> <li><p>Entrada</p> <ul> <li><code>metadata_df</code>: DataFrame com metadados dos v\u00eddeos (contendo <code>lab_id</code> e <code>video_id</code>).</li> <li><code>tracking_root</code>: diret\u00f3rio raiz onde est\u00e3o localizados os arquivos de tracking (<code>.parquet</code>).</li> </ul> </li> <li><p>Processo de constru\u00e7\u00e3o</p> <ol> <li>Para cada v\u00eddeo \u00fanico (<code>lab_id</code>, <code>video_id</code>), \u00e9 carregado o arquivo de tracking correspondente.</li> <li>Os registros s\u00e3o ordenados por <code>(mouse_id, bodypart, video_frame)</code> para manter a sequ\u00eancia temporal.</li> <li>S\u00e3o calculados os deslocamentos <code>dx</code> e <code>dy</code> entre frames consecutivos de cada parte do corpo.</li> <li>A dist\u00e2ncia percorrida em cada frame \u00e9 dada por <code>sqrt(dx\u00b2 + dy\u00b2)</code>.</li> <li>Essas dist\u00e2ncias s\u00e3o agregadas de tr\u00eas formas:<ul> <li>Por mouse e parte do corpo: gera colunas como <code>mean_movement_mouse1_paw</code> (dist\u00e2ncia normalizada por frame).</li> <li>Por mouse: dist\u00e2ncia total percorrida pelo animal em todos os bodyparts.</li> <li>Global: dist\u00e2ncia total acumulada no v\u00eddeo (<code>mean_movement_overall</code>).</li> </ul> </li> </ol> </li> <li><p>Coluna <code>total_frames</code></p> <ul> <li>Obtida a partir do n\u00famero de frames \u00fanicos no arquivo de tracking.</li> <li>Caso o arquivo esteja vazio ou ausente, \u00e9 estimada a partir de <code>video_duration_sec \u00d7 frames_per_second</code>.</li> </ul> </li> <li><p>Sa\u00edda</p> <ul> <li>Um <code>DataFrame</code> consolidado, em que cada linha corresponde a um v\u00eddeo, contendo:<ul> <li>Metadados b\u00e1sicos (<code>lab_id</code>, <code>video_id</code>, etc.).</li> <li>Colunas com estat\u00edsticas de movimento normalizadas.</li> <li>Contagem total de frames dispon\u00edveis para cada v\u00eddeo.</li> </ul> </li> </ul> </li> </ul>"},{"location":"classification/model/#funcoes-auxiliares-para-treinamento","title":"Fun\u00e7\u00f5es Auxiliares para Treinamento\u00b6","text":"<ul> <li><p><code>to_numpy_array</code> Converte diferentes formatos de dados (<code>np.ndarray</code>, <code>pandas.DataFrame</code>, listas) para <code>numpy.array</code>, assegurando compatibilidade com o modelo.</p> </li> <li><p><code>train_mlp_classifier</code> Facilita o treinamento de um MLP, encapsulando a prepara\u00e7\u00e3o dos dados e a inicializa\u00e7\u00e3o do modelo. Permite especificar hiperpar\u00e2metros como n\u00famero de camadas escondidas, taxa de aprendizado, regulariza\u00e7\u00e3o L2 e tamanho de lote.</p> </li> </ul>"},{"location":"classification/model/#estrutura-de-diretorios-e-carregamento-dos-dados","title":"Estrutura de Diret\u00f3rios e Carregamento dos Dados\u00b6","text":"<p>Ao final, o c\u00f3digo define caminhos candidatos para o dataset e carrega os arquivos principais:</p> <ul> <li><code>train.csv</code>: metadados com r\u00f3tulos.</li> <li><code>test.csv</code>: metadados para submiss\u00e3o.</li> <li><code>train_tracking</code>: informa\u00e7\u00f5es frame a frame de movimentos.</li> <li><code>train_annotation</code>: anota\u00e7\u00f5es detalhadas dos v\u00eddeos.</li> </ul>"},{"location":"classification/model/#explicacao-do-dataset","title":"Explica\u00e7\u00e3o do Dataset\u00b6","text":"<ul> <li><p>Identifica\u00e7\u00e3o</p> <ul> <li><code>lab_id</code> (categ\u00f3rica) \u2014 identificador do laborat\u00f3rio; chave para localiza\u00e7\u00e3o de anota\u00e7\u00f5es externas.</li> <li><code>video_id</code> (num\u00e9rica) \u2014 identificador \u00fanico do v\u00eddeo dentro do laborat\u00f3rio.</li> </ul> </li> <li><p>Atributos dos camundongos (at\u00e9 <code>mouse1</code>\u2013<code>mouse4</code>)</p> <ul> <li><code>mouse{n}_strain</code> (categ\u00f3rica) \u2014 linhagem declarada.</li> <li><code>mouse{n}_color</code> (categ\u00f3rica) \u2014 varia\u00e7\u00e3o de cor.</li> <li><code>mouse{n}_sex</code> (categ\u00f3rica) \u2014 sexo.</li> <li><code>mouse{n}_id</code> (num\u00e9rica) \u2014 identificador sequencial no estudo.</li> <li><code>mouse{n}_age</code> (bum\u00e9rica) \u2014 idade m\u00e9rdia dos ratos.</li> <li><code>mouse{n}_condition</code> (categ\u00f3rica) \u2014 condi\u00e7\u00e3o experimental aplicada. (Removida)</li> </ul> </li> <li><p>Atributos do v\u00eddeo/arena</p> <ul> <li><code>frames_per_second</code> (num\u00e9rica) \u2014 taxa de quadros.</li> <li><code>video_duration_sec</code> (num\u00e9rica) \u2014 dura\u00e7\u00e3o em segundos.</li> <li><code>pix_per_cm_approx</code> (num\u00e9rica) \u2014 aproxima\u00e7\u00e3o de pixel\u2192cent\u00edmetro.</li> <li><code>video_width_pix</code>, <code>video_height_pix</code> (num\u00e9ricas) \u2014 dimens\u00f5es em pixels.</li> <li><code>arena_width_cm</code>, <code>arena_height_cm</code> (num\u00e9ricas) \u2014 dimens\u00f5es da arena.</li> <li><code>arena_shape</code>, <code>arena_type</code> (categ\u00f3ricas) \u2014 morfologia e tipo de arena.</li> <li><code>body_parts_tracked</code>, <code>behaviors_labeled</code> (lista) \u2014 keypoints e comportamentos dispon\u00edveis.</li> <li><code>tracking_method</code> (categ\u00f3rica) \u2014 m\u00e9todo de captura de poses.</li> </ul> </li> <li><p>Alvos/vari\u00e1veis derivadas</p> <ul> <li><code>unique_actions</code> (num\u00e9rica) \u2014 contagem de comportamentos distintos observados por v\u00eddeo (feature auxiliar).</li> <li><code>top_action</code> (categ\u00f3rica) \u2014 comportamento predominante por v\u00eddeo (alvo da etapa de classifica\u00e7\u00e3o).</li> </ul> </li> </ul>"},{"location":"classification/model/#construcao-do-dataset-para-unique-actions","title":"Constru\u00e7\u00e3o do Dataset para Unique Actions\u00b6","text":"<p>O primeiro passo do treinamento consiste na prepara\u00e7\u00e3o das anota\u00e7\u00f5es frame a frame dos v\u00eddeos, transformando-as em r\u00f3tulos mais compactos que possam ser utilizados em n\u00edvel de v\u00eddeo.</p>"},{"location":"classification/model/#estruturacao-das-anotacoes","title":"Estrutura\u00e7\u00e3o das Anota\u00e7\u00f5es\u00b6","text":"<ol> <li><p>Constru\u00e7\u00e3o do \u00edndice de anota\u00e7\u00f5es (<code>annotation_index</code>)</p> <ul> <li>Cada v\u00eddeo (<code>lab_id</code>, <code>video_id</code>) foi associado ao caminho esperado para seu arquivo <code>.parquet</code> de anota\u00e7\u00f5es.</li> <li>Foi criada a coluna <code>exists</code> indicando se o arquivo de anota\u00e7\u00f5es est\u00e1 dispon\u00edvel no diret\u00f3rio correspondente.</li> <li>Esse passo garante rastreabilidade entre v\u00eddeos e suas anota\u00e7\u00f5es detalhadas.</li> </ul> </li> <li><p>Extra\u00e7\u00e3o de informa\u00e7\u00f5es principais por v\u00eddeo</p> <ul> <li>Para cada v\u00eddeo com anota\u00e7\u00f5es dispon\u00edveis:<ul> <li><code>unique_actions</code>: n\u00famero total de a\u00e7\u00f5es distintas identificadas.</li> <li><code>top_action</code>: a\u00e7\u00e3o mais frequente (modo estat\u00edstico) observada no v\u00eddeo.</li> </ul> </li> <li>Para v\u00eddeos sem anota\u00e7\u00f5es ou sem coluna de a\u00e7\u00e3o, foram atribu\u00eddos valores padr\u00e3o (<code>unique_actions = 0</code>, <code>top_action = None</code>).</li> </ul> </li> <li><p>Schema de Anota\u00e7\u00f5es (<code>annotation_schema</code>)</p> <ul> <li>Resultado consolidado em um <code>DataFrame</code> com as seguintes colunas:<ul> <li><code>lab_id</code>, <code>video_id</code></li> <li><code>top_action</code> \u2192 vari\u00e1vel categ\u00f3rica alvo para predi\u00e7\u00e3o da a\u00e7\u00e3o predominante.</li> <li><code>unique_actions</code> \u2192 vari\u00e1vel num\u00e9rica alvo para predi\u00e7\u00e3o do n\u00famero de a\u00e7\u00f5es distintas.</li> </ul> </li> </ul> </li> </ol>"},{"location":"classification/model/#distribuicao-das-classes-top_action","title":"Distribui\u00e7\u00e3o das Classes (<code>top_action</code>)\u00b6","text":"<p>Para avaliar balanceamento de classes, foi computada a distribui\u00e7\u00e3o de <code>top_action</code>:</p> <ul> <li>Tabela com contagem absoluta por classe e percentual relativo.</li> <li>Visualiza\u00e7\u00e3o com gr\u00e1fico de barras horizontal, destacando as classes mais frequentes.</li> </ul>"},{"location":"classification/model/#consideracoes-da-analise","title":"Considera\u00e7\u00f5es da An\u00e1lise\u00b6","text":"<ul> <li>Colunas faltantes predominantemente sendo informa\u00e7\u00f5es espec\u00edficas de alguns ratos, principalmente dos mouse_3 e mouse_4, menos presentes nos v\u00eddeos e a\u00e7\u00f5es, portanto valores faltantes foram tratados pela m\u00e9dia e moda, gerando mais coes\u00e3o aos dados.</li> <li>Coluna Target consideravelmente desbalanceada.</li> </ul>"},{"location":"classification/model/#montagem-do-dataset-para-predicao-de-uniqueactions","title":"Montagem do Dataset para Predi\u00e7\u00e3o de UniqueActions\u00b6","text":"<p>Ap\u00f3s a extra\u00e7\u00e3o das anota\u00e7\u00f5es, foi necess\u00e1rio alinhar as informa\u00e7\u00f5es de movimento dos camundongos (features de tracking) com os r\u00f3tulos definidos anteriormente (<code>unique_actions</code>).</p>"},{"location":"classification/model/#adicao-das-features-de-tracking","title":"Adi\u00e7\u00e3o das Features de Tracking\u00b6","text":"<ul> <li>Foi utilizada a fun\u00e7\u00e3o de <code>BuildTrackingFeatures</code> para gerar Features considerando os dados de tracking.</li> </ul>"},{"location":"classification/model/#pre-processamento-das-features","title":"Pr\u00e9-processamento das Features\u00b6","text":"<ul> <li>Foi utilizada a classe <code>TrackingFeaturePreprocessor</code> para:<ul> <li>Normalizar vari\u00e1veis num\u00e9ricas para o intervalo [-1, 1].</li> <li>Codificar vari\u00e1veis categ\u00f3ricas em formato one-hot.</li> <li>Construir uma matriz de atributos consistente entre treino e infer\u00eancia.</li> </ul> </li> </ul> <p>O resultado \u00e9 o <code>train_tracking_features</code>, que cont\u00e9m para cada v\u00eddeo:</p> <ul> <li>Colunas-chave (<code>lab_id</code>, <code>video_id</code>).</li> <li>Estat\u00edsticas de movimento normalizadas.</li> <li>Atributos categ\u00f3ricos transformados em vetores bin\u00e1rios.</li> </ul>"},{"location":"classification/model/#construcao-do-dataset-de-treinamento","title":"Constru\u00e7\u00e3o do Dataset de Treinamento\u00b6","text":"<ol> <li>As features processadas foram integradas ao <code>annotation_schema</code>, adicionando a coluna de r\u00f3tulo <code>unique_actions</code>.</li> <li>Foram descartados v\u00eddeos sem informa\u00e7\u00e3o v\u00e1lida de a\u00e7\u00f5es (valores nulos).</li> <li>A coluna <code>unique_actions</code> foi convertida para inteiro, representando o n\u00famero de a\u00e7\u00f5es distintas identificadas em cada v\u00eddeo.</li> </ol>"},{"location":"classification/model/#codificacao-das-classes","title":"Codifica\u00e7\u00e3o das Classes\u00b6","text":"<ul> <li>Foi criado um mapeamento valor \u2192 \u00edndice (<code>count_to_idx</code>) e \u00edndice \u2192 valor (<code>idx_to_count</code>), garantindo compatibilidade entre r\u00f3tulos originais e representa\u00e7\u00e3o num\u00e9rica exigida pelo modelo.</li> <li>Essa codifica\u00e7\u00e3o permite tratar a vari\u00e1vel-alvo como problema de classifica\u00e7\u00e3o multiclasse, em vez de regress\u00e3o pura.</li> </ul>"},{"location":"classification/model/#matrizes-finais","title":"Matrizes Finais\u00b6","text":"<ul> <li><code>X</code>: matriz de features num\u00e9ricas (<code>float64</code>).</li> <li><code>y</code>: vetor de r\u00f3tulos discretizados (<code>int</code>).</li> </ul> <p>Essas estruturas formam a base para o treinamento do primeiro modelo MLP voltado \u00e0 predi\u00e7\u00e3o de <code>unique_actions</code>.</p>"},{"location":"classification/model/#treinamento-do-modelo-para-predicao-de-unique-actions","title":"Treinamento do Modelo para Predi\u00e7\u00e3o de Unique Actions\u00b6","text":"<p>Com o dataset de features (<code>X</code>) e r\u00f3tulos (<code>y</code>) montado, foi poss\u00edvel iniciar o treinamento do primeiro modelo MLP.</p>"},{"location":"classification/model/#divisao-em-conjuntos-de-treino-e-validacao","title":"Divis\u00e3o em Conjuntos de Treino e Valida\u00e7\u00e3o\u00b6","text":"<ul> <li>Foi realizada uma separa\u00e7\u00e3o em 80% para treino, 15% para teste e 15% para valida\u00e7\u00e3o.</li> <li>A divis\u00e3o foi embaralhada e controlada por uma semente fixa (<code>random_state=42</code>), assegurando reprodutibilidade.</li> </ul>"},{"location":"classification/model/#preparacao-dos-dados","title":"Prepara\u00e7\u00e3o dos Dados\u00b6","text":"<ul> <li>As matrizes de treino e valida\u00e7\u00e3o foram convertidas explicitamente para <code>numpy.ndarray</code> no tipo adequado.</li> <li>Essa padroniza\u00e7\u00e3o evita inconsist\u00eancias durante o treinamento e a infer\u00eancia.</li> </ul>"},{"location":"classification/model/#configuracao-do-modelo-mlp","title":"Configura\u00e7\u00e3o do Modelo MLP\u00b6","text":"<p>O modelo inicial (baseline) foi definido com os seguintes par\u00e2metros:</p> <ul> <li>Camadas escondidas: [128, 64, 32] neur\u00f4nios.</li> <li>Taxa de aprendizado (<code>lr</code>): 0.05.</li> <li>N\u00famero m\u00e1ximo de \u00e9pocas: 3000.</li> <li>Tamanho do lote (<code>batch_size</code>): 64.</li> <li>Regulariza\u00e7\u00e3o L2: 0.01.</li> </ul>"},{"location":"classification/model/#avaliacao-inicial","title":"Avalia\u00e7\u00e3o Inicial\u00b6","text":"<p>Ap\u00f3s o treinamento, foram gerados:</p> <ul> <li>Predi\u00e7\u00f5es de probabilidade (<code>predict_proba</code>) para treino e valida\u00e7\u00e3o.</li> <li>Classes previstas (<code>argmax</code>) para cada exemplo.</li> <li>C\u00e1lculo da acur\u00e1cia em ambos os conjuntos.</li> </ul>"},{"location":"classification/model/#curvas-de-aprendizado-historico-de-loss","title":"Curvas de Aprendizado: Hist\u00f3rico de Loss\u00b6","text":"<p>Para compreender o comportamento do treinamento do MLP, foi gerada a curva de evolu\u00e7\u00e3o da fun\u00e7\u00e3o de perda (loss) ao longo das \u00e9pocas.</p>"},{"location":"classification/model/#objetivo-da-visualizacao","title":"Objetivo da Visualiza\u00e7\u00e3o\u00b6","text":"<ul> <li>Avaliar se o modelo converge adequadamente durante o processo de otimiza\u00e7\u00e3o.</li> <li>Identificar padr\u00f5es de underfitting ou overfitting.</li> </ul>"},{"location":"classification/model/#resultados","title":"Resultados\u00b6","text":"<ul> <li>O gr\u00e1fico exibe a perda m\u00e9dia por \u00e9poca durante o treinamento.</li> <li>A an\u00e1lise da curva fornece ind\u00edcios de:<ul> <li>Converg\u00eancia est\u00e1vel \u2192 redu\u00e7\u00e3o consistente da perda ao longo das \u00e9pocas.</li> <li>Estagna\u00e7\u00e3o \u2192 aus\u00eancia de melhora ap\u00f3s certo n\u00famero de \u00e9pocas, sugerindo que o modelo atingiu seu limite com a configura\u00e7\u00e3o atual.</li> </ul> </li> </ul>"},{"location":"classification/model/#treinamento-final-e-consolidacao-do-dataset","title":"Treinamento Final e Consolida\u00e7\u00e3o do Dataset\u00b6","text":"<p>Ap\u00f3s a fase de valida\u00e7\u00e3o, foi realizado o treinamento definitivo do modelo de predi\u00e7\u00e3o de <code>unique_actions</code>, utilizando toda a base dispon\u00edvel. Esse modelo mais profundo permite capturar com mais dados o padr\u00e3o de <code>unique_actions</code> por v\u00eddeo, possibilitando melhor utiliza\u00e7\u00e3o da resposta como feature futura.</p>"},{"location":"classification/model/#montagem-do-dataset-final","title":"Montagem do Dataset Final\u00b6","text":"<p>Com o modelo treinado, foi montado um dataset consolidado para as pr\u00f3ximas etapas:</p>"},{"location":"classification/model/#resultado-final","title":"Resultado Final\u00b6","text":"<p>O dataset consolidado (<code>full_unique_df</code>) cont\u00e9m:</p> <ul> <li>Identifica\u00e7\u00e3o dos v\u00eddeos (<code>lab_id</code>, <code>video_id</code>).</li> <li>Features num\u00e9ricas e categ\u00f3ricas derivadas do tracking.</li> <li>Coluna <code>unique_actions</code> (observada ou imputada).</li> <li>Coluna <code>unique_actions_scaled</code>, padronizada para an\u00e1lises futuras.</li> </ul> <p>Esse dataset representa o ponto de transi\u00e7\u00e3o entre a etapa de predi\u00e7\u00e3o de quantidade de a\u00e7\u00f5es (<code>unique_actions</code>) e a pr\u00f3xima fase de modelagem, focada na predi\u00e7\u00e3o da a\u00e7\u00e3o mais realizada (<code>top_action</code>).</p>"},{"location":"classification/model/#treinamento-do-modelo-para-predicao-da-acao-predominante-top-action","title":"Treinamento do Modelo para Predi\u00e7\u00e3o da A\u00e7\u00e3o Predominante (Top Action)\u00b6","text":"<p>Ap\u00f3s consolidar o dataset de <code>unique_actions</code>, o pr\u00f3ximo objetivo foi treinar um modelo MLP para prever a a\u00e7\u00e3o mais realizada em cada v\u00eddeo. Esse passo amplia a complexidade da tarefa, passando de uma contagem de a\u00e7\u00f5es para uma classifica\u00e7\u00e3o multiclasse das categorias de comportamento observadas.</p>"},{"location":"classification/model/#construcao-do-conjunto-de-treinamento","title":"Constru\u00e7\u00e3o do Conjunto de Treinamento\u00b6","text":"<ol> <li>Foram extra\u00eddas as anota\u00e7\u00f5es dispon\u00edveis de <code>top_action</code> no <code>annotation_schema</code>.</li> <li>O dataset consolidado (<code>full_unique_df</code>) foi unido \u00e0s labels, resultando em <code>full_actions_df</code>.</li> <li>Casos com <code>top_action</code> ausente foram descartados da etapa de treino, mantendo apenas v\u00eddeos com anota\u00e7\u00f5es v\u00e1lidas.</li> </ol>"},{"location":"classification/model/#codificacao-das-classes","title":"Codifica\u00e7\u00e3o das Classes\u00b6","text":"<ul> <li>A vari\u00e1vel categ\u00f3rica <code>top_action</code> foi convertida em r\u00f3tulos num\u00e9ricos usando <code>pd.factorize</code>.</li> <li>Foi gerada a lista <code>action_levels</code> com os nomes originais das classes, garantindo a rastreabilidade entre \u00edndices num\u00e9ricos e labels.</li> </ul>"},{"location":"classification/model/#configuracao-do-modelo-mlp","title":"Configura\u00e7\u00e3o do Modelo MLP\u00b6","text":"<p>O modelo inicial (baseline) foi definido com os seguintes par\u00e2metros:</p> <ul> <li>Camadas escondidas: [128, 64, 32] neur\u00f4nios.</li> <li>Taxa de aprendizado (<code>lr</code>): 0.05.</li> <li>N\u00famero m\u00e1ximo de \u00e9pocas: 3000.</li> <li>Tamanho do lote (<code>batch_size</code>): 64.</li> <li>Regulariza\u00e7\u00e3o L2: 0.01.</li> </ul>"},{"location":"classification/model/#predicoes-e-consolidacao","title":"Predi\u00e7\u00f5es e Consolida\u00e7\u00e3o\u00b6","text":"<ol> <li>O modelo treinado foi aplicado em todos os v\u00eddeos (<code>X_all_actions</code>).</li> <li>Foram geradas as predi\u00e7\u00f5es (<code>pred_top_action</code>) e atribu\u00eddas labels correspondentes.</li> <li>Foi criada a coluna <code>most_frequent_action</code>, que preserva o valor anotado quando dispon\u00edvel e, em caso de aus\u00eancia, utiliza a predi\u00e7\u00e3o do modelo.</li> <li>As colunas adicionadas ao dataset final (<code>full_unique_df</code>) foram:<ul> <li><code>top_action</code>: anota\u00e7\u00e3o original (quando presente).</li> <li><code>pred_top_action</code>: resultado do modelo.</li> </ul> </li> </ol>"},{"location":"classification/model/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais\u00b6","text":""},{"location":"classification/model/#parametros-utilizados","title":"Par\u00e2metros Utilizados\u00b6","text":"<ul> <li>Regulariza\u00e7\u00e3o l2 - utilizada para corrigir overfitting (accuracy da valida\u00e7\u00e3o consideravelmente inferior).</li> <li>\u00c9pocas - valor decidido analisando estabilidade das curvas de erro e accuracy.</li> <li>Batches - valor de 64 para mini-batches providenciando balanceamento entre velocidade e precis\u00e3o.</li> <li>Camadas Ocultas - valores escolhidos para otimizar a precis\u00e3o e velocidade de treinamento.</li> <li>Softmax - camada de sa\u00edda com softmax para otimiza\u00e7\u00e3o e redu\u00e7\u00e3o de ambiguidade em problemas multiclasses.</li> </ul>"},{"location":"classification/model/#limitacoes-e-melhorias-futuras","title":"Limita\u00e7\u00f5es e Melhorias Futuras\u00b6","text":"<ul> <li>Previs\u00e3o por v\u00eddeo - a simplifica\u00e7\u00e3o realizada n\u00e3o considera as diferentes a\u00e7\u00f5es que podem ocorrer em distintos espa\u00e7os temporais do v\u00eddeo, n\u00e3o cumprindo a proposta da competi\u00e7\u00e3o corretamente.</li> <li>Previs\u00e3o por Frame - utiliza\u00e7\u00e3o de previs\u00e3o por frame ou batches de frames para cumprir a proposta da competi\u00e7\u00e3o de maneira correta.</li> <li>Estruturas de Dados Inteligentes - adaptar a MLP e utiliza\u00e7\u00e3o dos dados para implementa\u00e7\u00e3o de previs\u00e3o por frame, possibilitando otimiza\u00e7\u00e3o do uso da RAM ao comportar datasets exponencialmente maiores, como \u00e9 o caso considerando os frames de cada v\u00eddeo.</li> <li>Previs\u00e3o dos agentes - para cumprir a proposta final da competi\u00e7\u00e3o, realizar tamb\u00e9m a previs\u00e3o dos atores (rato agente e rato passivo) em cada uma das a\u00e7\u00f5es previstas.</li> </ul>"},{"location":"exercicio1/main/","title":"Exercicio 1","text":""},{"location":"exercicio1/main/#deep-learning-data","title":"Deep Learning \u2014 Data","text":"<p>Autor: Caio Ortega Boa Disciplina: Deep Learning Per\u00edodo: 2025.1 Link do Reposit\u00f3rio</p>"},{"location":"exercicio1/main/#sumario","title":"Sum\u00e1rio","text":"<ul> <li>Exerc\u00edcio 1: Separabilidade em 2D (dados sint\u00e9ticos gaussianos)  </li> <li>Exerc\u00edcio 2: N\u00e3o-linearidade em 5D e proje\u00e7\u00e3o PCA \u2192 2D  </li> <li>Exerc\u00edcio 3: Pr\u00e9-processamento do Spaceship Titanic (Kaggle) para MLP com <code>tanh</code></li> </ul>"},{"location":"exercicio1/main/#exercicio-1-class-separability-em-2d","title":"Exerc\u00edcio 1 \u2014 Class Separability em 2D","text":"<p>Objetivo. Explorar como a distribui\u00e7\u00e3o de quatro classes em 2D influencia a complexidade das fronteiras de decis\u00e3o que uma rede neural precisaria aprender.</p>"},{"location":"exercicio1/main/#parametros-utilizados","title":"Par\u00e2metros utilizados","text":"<ul> <li>M\u00e9dias (\u03bcx, \u03bcy): (2,3), (5,6), (8,1), (15,4)</li> <li>Desvios (\u03c3x, \u03c3y): (0,8; 2,5), (1,2; 1,9), (0,9; 0,9), (0,5; 2,0) </li> </ul>"},{"location":"exercicio1/main/#visualizacao","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio1/main/#analise-e-respostas","title":"An\u00e1lise e respostas","text":"<ul> <li>Distribui\u00e7\u00e3o e overlap: Classes 0 e 1 apresentam sobreposi\u00e7\u00e3o consider\u00e1vel enquanto as classes 1 e 2 apresentam leve sobreposi\u00e7\u00e3o; a Classe 3 est\u00e1 deslocada \u00e0 direita sem nenhuma sobreposi\u00e7\u00e3o.  </li> <li>Uma fronteira linear simples separa tudo? N\u00e3o. Com uma \u00fanica fronteira linear n\u00e3o \u00e9 poss\u00edvel separar todas as classes corretamente.  </li> <li>\u201cSketch\u201d das fronteiras que a rede aprenderia: Para separar de maneira eficiente todas as classes seriam necess\u00e1rias pelo menos 3 fronteiras lineares.</li> </ul>"},{"location":"exercicio1/main/#codigo","title":"C\u00f3digo","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Garante reprodutibilidade dos n\u00fameros aleat\u00f3rios\nnp.random.seed(42)\n\ndef draw_line(ax, point1, point2, style='--', color='k', lw=2):\n    \"\"\"\n    Desenha uma linha entre dois pontos no gr\u00e1fico.\n\n    Par\u00e2metros:\n    - ax: objeto matplotlib.axes onde a linha ser\u00e1 desenhada.\n    - point1: tupla (x1, y1) do primeiro ponto.\n    - point2: tupla (x2, y2) do segundo ponto.\n    - style: estilo da linha (default='--' tracejada).\n    - color: cor da linha (default='k' preto).\n    - lw: espessura da linha.\n    - label: legenda opcional para a linha.\n    \"\"\"\n    x_vals = [point1[0], point2[0]]\n    y_vals = [point1[1], point2[1]]\n    ax.plot(x_vals, y_vals, style, color=color, lw=lw)\n\n# Par\u00e2metros das distribui\u00e7\u00f5es gaussianas:\n# means = lista de tuplas (\u03bcx, \u03bcy) = m\u00e9dias em cada eixo\n# stds = lista de tuplas (\u03c3x, \u03c3y) = desvios em cada eixo\nmeans = [(2,3), (5,6), (8,1), (15,4)]\nstds = [(0.8,2.5), (1.2,1.9), (0.9,0.9), (0.5,2.0)]\n\n# Listas para acumular pontos (X) e r\u00f3tulos (y)\nX = []\ny = []\n\n# Para cada classe (0,1,2,3) gera 100 pontos 2D\n# np.random.normal aceita tuplas em loc/scale\n# loc=(\u03bcx, \u03bcy) =&gt; m\u00e9dia por eixo\n# scale=(\u03c3x, \u03c3y) =&gt; desvio por eixo\n# Isso equivale a gaussianas 2D com covari\u00e2ncia diagonal\nfor i, (mean, std) in enumerate(zip(means, stds)):\n    points = np.random.normal(loc=mean, scale=std, size=(100,2))\n    X.append(points)               # pontos da classe i\n    y.append(np.full(100, i))      # vetor [i, i, ..., i] (100 vezes)\n\n# Empilha todas as classes em um \u00fanico array\nX = np.vstack(X)  # shape (400, 2)\ny = np.hstack(y)  # shape (400,)\n\n# Cria o gr\u00e1fico de dispers\u00e3o\nfig, ax = plt.subplots(figsize=(8,6))\nfor i in range(4):\n    # Plota os pontos da classe i\n    ax.scatter(X[y==i,0], X[y==i,1], label=f'Class {i}', alpha=0.7)\n\n#Desenhando linhas arbitrarias de separa\u00e7\u00e3o\ndraw_line(ax, (5,-3), (2,15), style='-', color='purple')\ndraw_line(ax, (4.5,1.5), (12.5,5), style='-', color='purple')\ndraw_line(ax, (12.5,-3), (12.5,15), style='-', color='purple')\n\nax.legend()\nax.set_title(\"Synthetic 2D Gaussian Dataset\")\nax.set_xlabel(\"X1\")\nax.set_ylabel(\"X2\")\nplt.show()\n</code></pre>"},{"location":"exercicio1/main/#exercicio-2-nao-linearidade-em-5d-pca-5d-2d","title":"Exerc\u00edcio 2 \u2014 N\u00e3o-linearidade em 5D + PCA (5D \u2192 2D)","text":"<p>Objetivo. Criar dois grupos 5D com m\u00e9dias/covari\u00e2ncias especificadas e visualizar em 2D via PCA.</p>"},{"location":"exercicio1/main/#configuracao","title":"Configura\u00e7\u00e3o","text":"<ul> <li>Classe A: vetor de m\u00e9dia nulo; covari\u00e2ncias positivas entre algumas dimens\u00f5es.  </li> <li>Classe B: vetor de m\u00e9dia transladado (1,5 em todas as componentes); covari\u00e2ncias com sinais distintos, alterando forma e orienta\u00e7\u00e3o do grupo.</li> </ul>"},{"location":"exercicio1/main/#visualizacao_1","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio1/main/#analise-e-respostas_1","title":"An\u00e1lise e respostas","text":"<ul> <li>Rela\u00e7\u00e3o entre as classes (proje\u00e7\u00e3o 2D): Observa-se mistura parcial, embora possa se identificar certa separa\u00e7\u00e3o entre as nuvens.  </li> <li>Separabilidade linear: Embora possa ser observado uma distribui\u00e7\u00e3o com certa separa\u00e7\u00e3o na proje\u00e7\u00e3o 2d dos dados, uma separa\u00e7\u00e3o linear seria muito ineficiente para o caso proposto. Por haverem m\u00faltiplas dimens\u00f5es nos dados a separabilidade linear tenderia a perder muita informa\u00e7\u00e3o, por n\u00e3o haver um hiperplano perfeito capaz de separar as duas classes.</li> </ul>"},{"location":"exercicio1/main/#codigo_1","title":"C\u00f3digo","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Reprodutibilidade\nnp.random.seed(42)\n\n# -----------------------------\n# 1) Define par\u00e2metros 5D\n# -----------------------------\n# Classe A\nmu_A = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.3, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n], dtype=float)\n\n# Classe B\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigma_B = np.array([\n    [ 1.5, -0.7,  0.2,  0.0, 0.0],\n    [-0.7,  1.5,  0.4,  0.0, 0.0],\n    [ 0.2,  0.4,  1.5,  0.6, 0.0],\n    [ 0.0,  0.0,  0.6,  1.5, 0.3],\n    [ 0.0,  0.0,  0.0,  0.3, 1.5],\n], dtype=float)\n\n# -----------------------------\n# 2) Gera\u00e7\u00e3o dos dados (5D)\n# -----------------------------\nnA, nB = 500, 500\nXA = np.random.multivariate_normal(mean=mu_A, cov=Sigma_A, size=nA)\nXB = np.random.multivariate_normal(mean=mu_B, cov=Sigma_B, size=nB)\n\n# Empilha dados e r\u00f3tulos\nX_5d = np.vstack([XA, XB])         # (1000, 5)\ny    = np.array([0]*nA + [1]*nB)  # 0 = Classe A, 1 = Classe B\n\n# -----------------------------\n# 3) Redu\u00e7\u00e3o de dimensionalidade (PCA \u2192 2D)\n# -----------------------------\npca = PCA(n_components=2, random_state=42)\nX_2d = pca.fit_transform(X_5d)     # (1000, 2)\n\n# -----------------------------\n# 4) Visualiza\u00e7\u00e3o (apenas pontos)\n# -----------------------------\nplt.figure(figsize=(8, 6))\nplt.scatter(X_2d[y==0, 0], X_2d[y==0, 1], alpha=0.6, s=18, label=\"Class A\")\nplt.scatter(X_2d[y==1, 0], X_2d[y==1, 1], alpha=0.6, s=18, label=\"Class B\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Exercise 2 \u2014 PCA (5D \u2192 2D) scatter\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercicio1/main/#exercicio-3-spaceship-titanic-kaggle-pre-processamento-para-tanh","title":"Exerc\u00edcio 3 \u2014 Spaceship Titanic (Kaggle): Pr\u00e9-processamento para <code>tanh</code>","text":"<p>Objetivo. Preparar dados reais para uma MLP com <code>tanh</code>, assegurando entradas est\u00e1veis.</p>"},{"location":"exercicio1/main/#descricao-do-dataset","title":"Descri\u00e7\u00e3o do dataset","text":"<ul> <li>Objetivo do Dataset O Dataset simula um \"Titanic espacial\", que estaria lotado de passageiros e colidiu com uma anomalia espacial que trasnportou diversos passageiros para outra dimens\u00e3o. O objetivo do dataset \u00e9 descobrir quais passageiros teriam sido transportados para essa dimens\u00e3o alternativa baseado em seus dados.</li> <li>Alvo: <code>Transported</code> \u2014 indica se o passageiro foi transportado para outra dimens\u00e3o (bin\u00e1rio).  </li> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>, <code>CabinNum</code>, <code>Group</code>, <code>PaxInGroup</code>, <code>TotalSpend</code>.  </li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code>, <code>CabinDeck</code>, <code>CabinSide</code>.  </li> <li>Engenharia aplicada: A feature <code>Cabin</code> foi decomposta em <code>CabinDeck</code>, <code>CabinNum</code> e <code>CabinSide</code>;  <code>PassengerId</code> foi decomposto em <code>Group</code> e <code>PaxInGroup</code>; <code>Transported</code> foi convertido para 0/1.</li> </ul>"},{"location":"exercicio1/main/#faltantes","title":"Faltantes","text":"<ul> <li>Investiga\u00e7\u00e3o Todas as colunas, fora <code>PassengerId</code> e <code>Transported</code> possuiam dados faltantes.</li> <li>Num\u00e9ricas: imputa\u00e7\u00e3o pela mediana (robusta a outliers; preserva a posi\u00e7\u00e3o central).  </li> <li>Categ\u00f3ricas: imputa\u00e7\u00e3o pela moda (mant\u00e9m r\u00f3tulos conhecidos; evita categorias artificiais).  </li> </ul>"},{"location":"exercicio1/main/#tratamento-das-features","title":"Tratamento das Features","text":"<ul> <li>One-Hot Encoding Utilizado para tratamento de features categ\u00f3ricas.</li> <li>Normaliza\u00e7\u00e3o para <code>[-1, 1]</code>: Utilizado para tratamento de features num\u00e9ricas, de modo a acomodar os dados corretamente para a utiliza\u00e7\u00e3o da fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>. Se trata de uma boa pr\u00e1tica pois possibilita padroniza a escala das features e possibilita que a maior parte dos dados esteja na parte central da curva, otimizando o treinamento.</li> </ul>"},{"location":"exercicio1/main/#visualizacoes","title":"Visualiza\u00e7\u00f5es","text":"<p>Antes da transforma\u00e7\u00e3o </p> <p>Depois da transforma\u00e7\u00e3o </p>"},{"location":"exercicio1/main/#codigo_2","title":"C\u00f3digo","text":"<pre><code>#Imports e leitura\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n#Configs visuais\nplt.rcParams[\"figure.figsize\"] = (8, 5)\nplt.rcParams[\"axes.grid\"] = True\n\n#Reprodutibilidade\nnp.random.seed(42)\n\n#Caminho do arquivo\nCSV_PATH = \"spaceship.csv\"\n\ndf = pd.read_csv(CSV_PATH)\nprint(df.shape)\ndf.head()\n</code></pre> <pre><code>#Vis\u00e3o geral: tipos e faltantes\nprint(\"\\n=== info() ===\")\ndf.info()\n\nprint(\"\\n=== Missing values por coluna ===\")\nmissing_abs = df.isna().sum().sort_values(ascending=False)\nmissing_pct = (df.isna().mean()*100).sort_values(ascending=False)\ndisplay(pd.DataFrame({\"missing\": missing_abs, \"missing_%\": missing_pct.round(2)}))\n</code></pre> <pre><code>#Quebra Cabin em deck/num/side\ncabin = df[\"Cabin\"].astype(\"string\")\nparts = cabin.str.split(\"/\", expand=True)\ndf[\"CabinDeck\"] = parts[0]\ndf[\"CabinNum\"]  = pd.to_numeric(parts[1], errors=\"coerce\")\ndf[\"CabinSide\"] = parts[2]\n\n#Quebra PassengerId em grupo e \u00edndice no grupo\npid = df[\"PassengerId\"].astype(\"string\")\ngrp_pp = pid.str.split(\"_\", expand=True)\ndf[\"Group\"] = pd.to_numeric(grp_pp[0], errors=\"coerce\")  \ndf[\"PaxInGroup\"] = pd.to_numeric(grp_pp[1], errors=\"coerce\")\n\n#Transported -&gt; 0/1\ndf[\"Transported\"] = df[\"Transported\"].map({True:1, False:0, \"True\":1, \"False\":0}).astype(int)\n\n#Colunas que n\u00e3o vamos usar como features\ndrop_cols = [\"PassengerId\", \"Name\", \"Cabin\"] \n\n#Colunas num\u00e9ricas e categ\u00f3ricas\nnumeric_features = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Group\", \"CabinNum\", \"PaxInGroup\"]\ncategorical_features = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"CabinDeck\", \"CabinSide\"]\n\nprint(\"numeric_features:\", numeric_features)\nprint(\"categorical_features:\", categorical_features)\n\ndf_pre = df.drop(columns=drop_cols).copy()\ndf_pre.head()\n</code></pre> <pre><code>#Separa\u00e7\u00e3o X, y\ntarget = \"Transported\"\nX = df_pre.drop(columns=[target])\ny = df_pre[target].values\n</code></pre> <pre><code>#Sanitiza\u00e7\u00e3o (Corre\u00e7\u00e3o de erros no processamento)\nX = X.copy()\n\n#Booleans para strings\nfor col in [\"CryoSleep\", \"VIP\"]:\n    if col in X.columns:\n        X[col] = X[col].map({True: \"True\", False: \"False\"}).astype(\"object\")\n\n#CATEG\u00d3RICAS como 'object' e remover pd.NA\nfor c in categorical_features:\n    if c in X.columns:\n        X[c] = X[c].astype(\"object\")\n        mask = pd.isna(X[c])\n        if mask.any():\n            X.loc[mask, c] = np.nan\n\n#NUM\u00c9RICAS realmente num\u00e9ricas\nfor c in numeric_features:\n    if c in X.columns:\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n</code></pre> <pre><code># Num\u00e9ricas: imputar mediana + scaler (-1, 1)\nnum_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\",  MinMaxScaler(feature_range=(-1, 1))),\n])\n\n# Categ\u00f3ricas: imputar moda + OneHot\ncat_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\",  OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", num_pipe, numeric_features),\n        (\"cat\", cat_pipe, categorical_features),\n    ],\n    remainder=\"drop\",\n)\n\nX_proc = preprocessor.fit_transform(X)\n\nprint(\"X_proc shape:\", X_proc.shape)\n</code></pre> <pre><code>#Checagem de colunas ap\u00f3s OHE\nnum_names = numeric_features\ncat_names = preprocessor.named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out(categorical_features).tolist()\nfinal_feature_names = num_names + cat_names\n\nprint(\"Total de colunas ap\u00f3s OHE:\", len(final_feature_names))\n</code></pre> <pre><code>#Plotagem dos histogramas\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Antes\nX[\"Age\"].hist(ax=axes[0], bins=30, alpha=0.8)\naxes[0].set_title(\"Age \u2014 antes da escala\")\naxes[0].set_xlabel(\"Age\")\n\nX[\"FoodCourt\"].hist(ax=axes[1], bins=30, alpha=0.8)\naxes[1].set_title(\"FoodCourt \u2014 antes da escala\")\naxes[1].set_xlabel(\"FoodCourt\")\nplt.tight_layout()\nplt.show()\n\n# Depois\nage_idx = numeric_features.index(\"Age\")\nfood_idx = numeric_features.index(\"FoodCourt\")\n\nage_scaled   = X_proc[:, age_idx]\nfood_scaled  = X_proc[:, food_idx]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].hist(age_scaled, bins=30, alpha=0.8)\naxes[0].set_title(\"Age \u2014 depois (StandardScaler)\")\naxes[0].set_xlabel(\"z-score\")\n\naxes[1].hist(food_scaled, bins=30, alpha=0.8)\naxes[1].set_title(\"FoodCourt \u2014 depois (StandardScaler)\")\naxes[1].set_xlabel(\"z-score\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercicio2/main/","title":"Exercicio 2","text":""},{"location":"exercicio2/main/#deep-learning-perceptron","title":"Deep Learning \u2014 Perceptron","text":"<p>Autor: Caio Ortega Boa Disciplina: Deep Learning Per\u00edodo: 2025.1 Link do Reposit\u00f3rio</p>"},{"location":"exercicio2/main/#sumario","title":"Sum\u00e1rio","text":"<ul> <li>Perceptron Implementa\u00e7\u00e3o de um Perceptron</li> <li>Exerc\u00edcio 1: Treinamento de Perceptron com Converg\u00eancia </li> <li>Exerc\u00edcio 2: Treinamento de Perceptron sem Converg\u00eancia</li> </ul>"},{"location":"exercicio2/main/#perceptron-implementacao-de-um-perceptron","title":"Perceptron \u2014 Implementa\u00e7\u00e3o de um Perceptron","text":"<p>Objetivo. Implementa\u00e7\u00e3o de um Perceptron sem utiliza\u00e7\u00e3o de bibliotecas auxiliares e modelos prontos, apenas Numpy para c\u00e1lculos matriciais.</p>"},{"location":"exercicio2/main/#codigo","title":"C\u00f3digo","text":"<pre><code>import numpy as np\n\nclass Perceptron:\n    \"\"\"\n    Single-layer Perceptron implemented from scratch.\n    \"\"\"\n    def __init__(\n        self,\n        lr: float = 0.1,\n        max_epochs: int = 100,\n        random_state: int | None = None,\n        track_history: bool = True,\n    ):\n        self.lr = lr\n        self.max_epochs = max_epochs\n        self.random_state = random_state\n        self.track_history = track_history\n\n        self.w = None\n        self.b = 0.0\n        self.accuracy_history_ = []\n\n    def decision_function(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute raw scores (w\u00b7x + b).\n        X: shape (n_samples, n_features)\n        \"\"\"\n        if self.w is None:\n            raise ValueError(\"Model is not fitted yet.\")\n        return X @ self.w + self.b\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict labels in {-1, +1} using sign(w\u00b7x + b).\n        \"\"\"\n        scores = self.decision_function(X)\n        return np.where(scores &gt;= 0, 1, -1)\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; dict:\n        \"\"\"\n        Fit the perceptron on labels in {-1, +1}.\n        Stops early if an epoch completes with zero updates.\n        Records accuracy after each epoch.\n        Returns a training log with 'epochs_run' and 'converged' keys.\n        \"\"\"\n        rng = np.random.default_rng(self.random_state)\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.w = np.zeros(n_features, dtype=float)\n        self.b = 0.0\n        self.accuracy_history_ = []\n\n        indices = np.arange(n_samples)\n\n        converged = False\n        epochs_run = 0\n\n        for epoch in range(self.max_epochs):\n            epochs_run += 1\n            # Shuffle each epoch for robustness\n            rng.shuffle(indices)\n            updates = 0\n\n            for idx in indices:\n                x_i = X[idx]\n                y_i = y[idx]  # must be -1 or +1\n                margin = y_i * (np.dot(self.w, x_i) + self.b)\n                if margin &lt;= 0:\n                    # Misclassified -&gt; update\n                    self.w = self.w + self.lr * y_i * x_i\n                    self.b = self.b + self.lr * y_i\n                    updates += 1\n\n            # Accuracy after epoch\n            y_pred = self.predict(X)\n            acc = np.mean(y_pred == y)\n            self.accuracy_history_.append(acc)\n\n            if updates == 0:\n                converged = True\n                break\n\n        return {\"epochs_run\": epochs_run, \"converged\": converged}\n</code></pre>"},{"location":"exercicio2/main/#exercicio-1-treinamento-de-perceptron-com-convergencia","title":"Exerc\u00edcio 1 \u2014 Treinamento de Perceptron com Converg\u00eancia","text":"<p>Objetivo. Gerar um dataset 2d com clara distin\u00e7\u00e3o entre classes para realiza\u00e7\u00e3o do treinamento de um perceptron.</p>"},{"location":"exercicio2/main/#parametros-utilizados","title":"Par\u00e2metros utilizados","text":"<ul> <li>M\u00e9dias (\u03bcx, \u03bcy): (1.5,1.5), (5,5)</li> <li>Desvios (\u03c3x, \u03c3y): (0.5; 0), (0; 0.5)</li> </ul>"},{"location":"exercicio2/main/#visualizacao","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio2/main/#resultados","title":"Resultados","text":"<ul> <li>Final Weights [0.304 , 0.199]</li> <li>Final Bias -1.6</li> <li>Epochs 2</li> <li>Fina Accuracy 1.0</li> </ul>"},{"location":"exercicio2/main/#analise-dos-resultados","title":"An\u00e1lise dos Resultados","text":"<ul> <li>Alta Separabilidade A alta separabilidade dos dados gerados, apresentando classes extremamente bem definidas por uma \u00fanica linha de decis\u00e3o, s\u00e3o respons\u00e1veis pelo baixo n\u00famero de \u00e9pocas necess\u00e1rio para a convers\u00e3o. Isso se deve pois, conforme o treinamento ocorre e a linha se enviesa para os dados, o modelo passa a classificar todos os dados corretamente, ocasionando em sua converg\u00eancia.</li> </ul>"},{"location":"exercicio2/main/#codigo_1","title":"C\u00f3digo","text":"<pre><code>mean0 = np.array([1.5, 1.5])\ncov0  = np.array([[0.5, 0.0],[0.0, 0.5]])\n\nmean1 = np.array([5.0, 5.0])\ncov1  = np.array([[0.5, 0.0],[0.0, 0.5]])\n\nn_per_class = 1000\n\nX0 = rng.multivariate_normal(mean0, cov0, size=n_per_class)\nX1 = rng.multivariate_normal(mean1, cov1, size=n_per_class)\n\nX = np.vstack([X0, X1])\ny01 = np.hstack([np.zeros(n_per_class, dtype=int), np.ones(n_per_class, dtype=int)])\ny = np.where(y01 == 1, 1, -1)\n\nplt.figure()\nplt.scatter(X0[:, 0], X0[:, 1], label=\"Class 0\")\nplt.scatter(X1[:, 0], X1[:, 1], label=\"Class 1\")\nplt.title(\"Exercise 1: Generated Data\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Train perceptron\nclf = Perceptron(lr=0.1, max_epochs=100, random_state=42)\nlog = clf.fit(X, y)\n\n# Evaluate\ny_pred = clf.predict(X)\nacc = np.mean(y_pred == y)\nprint(\"Epochs run:\", log[\"epochs_run\"])\nprint(\"Converged:\", log[\"converged\"])\nprint(\"Final weights:\", clf.w)\nprint(\"Final bias:\", clf.b)\nprint(\"Final accuracy:\", acc)\n</code></pre>"},{"location":"exercicio2/main/#exercicio-2-treinamento-de-perceptron-sem-convergencia","title":"Exerc\u00edcio 2 \u2014 Treinamento de Perceptron sem Converg\u00eancia","text":"<p>Objetivo. Gerar um dataset 2d com baixa distin\u00e7\u00e3o entre classes para realiza\u00e7\u00e3o do treinamento de um perceptron.</p>"},{"location":"exercicio2/main/#parametros-utilizados_1","title":"Par\u00e2metros utilizados","text":"<ul> <li>M\u00e9dias (\u03bcx, \u03bcy): (3.0,3.0), (4.0,4.0)</li> <li>Desvios (\u03c3x, \u03c3y): (1.5; 0), (0; 1.5)</li> </ul>"},{"location":"exercicio2/main/#visualizacao_1","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio2/main/#resultados_1","title":"Resultados","text":"<ul> <li>Final Weights [-0.140 , 0.955]</li> <li>Final Bias -4.5</li> <li>Epochs 100</li> <li>Fina Accuracy 0.558</li> </ul>"},{"location":"exercicio2/main/#analise-dos-resultados_1","title":"An\u00e1lise dos Resultados","text":"<ul> <li>Baixa Separabilidade No seguinte dataset, havendo baixa separabilidade de dados, o modelo do perceptron n\u00e3o conseguiu convergir. Isso se deve a mistura e sobreposi\u00e7\u00e3o de dados que existe no dataset, impossibilitando que todas as classes sejam devidamente identificadas por uma \u00fanica linha de decis\u00e3o linear. Por conta disso o perceptron nunca ir\u00e1 encontrar os pesos ideais para o treinamento, n\u00e3o permitindo sua converg\u00eancia.</li> </ul>"},{"location":"exercicio2/main/#codigo_2","title":"C\u00f3digo","text":"<pre><code>mean0 = np.array([3.0, 3.0])\ncov0  = np.array([[1.5, 0.0],[0.0, 1.5]])\n\nmean1 = np.array([4.0, 4.0])\ncov1  = np.array([[1.5, 0.0],[0.0, 1.5]])\n\nn_per_class = 1000\n\nX0 = rng.multivariate_normal(mean0, cov0, size=n_per_class)\nX1 = rng.multivariate_normal(mean1, cov1, size=n_per_class)\n\nX = np.vstack([X0, X1])\ny01 = np.hstack([np.zeros(n_per_class, dtype=int), np.ones(n_per_class, dtype=int)])\ny = np.where(y01 == 1, 1, -1)\n\nplt.figure()\nplt.scatter(X0[:, 0], X0[:, 1], label=\"Class 0\")\nplt.scatter(X1[:, 0], X1[:, 1], label=\"Class 1\")\nplt.title(\"Exercise 2: Generated Data\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>clf = Perceptron(lr=0.1, max_epochs=100, random_state=123)\nlog = clf.fit(X, y)\n\ny_pred = clf.predict(X)\nacc = np.mean(y_pred == y)\n\nprint(\"Epochs run:\", log[\"epochs_run\"])\nprint(\"Converged:\", log[\"converged\"])\nprint(\"Final weights:\", clf.w)\nprint(\"Final bias:\", clf.b)\nprint(\"Final accuracy:\", acc)\n</code></pre>"},{"location":"exercicio3/main/","title":"Exercicio 3","text":""},{"location":"exercicio3/main/#deep-learning-multi-layer-perceptron-mlp","title":"Deep Learning \u2014 Multi-Layer Perceptron (MLP)","text":"<p>Autor: Caio Ortega Boa Disciplina: Deep Learning Per\u00edodo: 2025.1 Link do Reposit\u00f3rio</p>"},{"location":"exercicio3/main/#sumario","title":"Sum\u00e1rio","text":"<ul> <li>Exerc\u00edcio 1 C\u00e1lculo Manual de uma MLP</li> <li>MLP: rede com camadas ocultas <code>tanh</code> e sa\u00edda <code>softmax</code> </li> <li>Exerc\u00edcio 2: Classifica\u00e7\u00e3o bin\u00e1ria  </li> <li>Exerc\u00edcio 3: Classifica\u00e7\u00e3o multiclasse </li> <li>Exerc\u00edcio 4: Classifica\u00e7\u00e3o multiclasse (+ camadas ocultas)</li> </ul>"},{"location":"exercicio3/main/#exercicio-1-calculo-manual-de-uma-mlp","title":"Exerc\u00edcio 1 \u2014 C\u00e1lculo Manual de uma MLP","text":"<p>Descri\u00e7\u00e3o do modelo. MLP com 2 entradas, 1 camada oculta (2 neur\u00f4nios) e 1 neur\u00f4nio de sa\u00edda. Ativa\u00e7\u00f5es: <code>tanh</code> na oculta e na sa\u00edda. Loss: MSE.</p>"},{"location":"exercicio3/main/#1-dados-do-problema-preencher","title":"1) Dados do problema (preencher)","text":"<ul> <li>Entrada (vetor coluna): x = [ 0.5 , 0,2]</li> <li>Sa\u00edda desejada: y = 1.0 </li> <li>Pesos da camada oculta W1 = [[0.3 , -0.1][0.2, 0.4]]</li> <li>Vieses da camada oculta b1 = [0.1 , -0.2]</li> <li>Pesos da sa\u00edda W2 = [0.5, -0.3]</li> <li>Vi\u00e9s da sa\u00edda b2 = 0.2 </li> <li>Taxa de aprendizado: eta = 0.3</li> </ul>"},{"location":"exercicio3/main/#2-forward-pass","title":"2) Forward pass","text":"<ul> <li>Pr\u00e9-ativa\u00e7\u00f5es na oculta: z1 = W1 @ x + b1 = [0.27, -0.18]</li> <li>Ativa\u00e7\u00f5es na oculta: a1 = tanh(z1) = [0.2636, -0.1780]</li> <li>Pr\u00e9-ativa\u00e7\u00e3o na sa\u00edda: z2 = W2 @ a1 + b2 = 0.3852  </li> <li>Sa\u00edda da rede: y^ = tanh(z2) = 0.3672</li> </ul>"},{"location":"exercicio3/main/#3-loss-mse","title":"3) Loss (MSE)","text":"<ul> <li>L = (y - y<sup>)</sup>2 = 0.4003</li> </ul>"},{"location":"exercicio3/main/#4-backward-pass","title":"4) Backward pass","text":"<ul> <li>dl/dy^ = -2*(y - y^) = -1.2655  </li> <li>dy^/dz2 = 1 - tanh(z2)^2 = 0.8651</li> <li>dl/dz2 = -1.0948</li> <li>dL/dW2 = dl/dz2 * a1^T = [-0.2886, 0.1949]</li> <li>dL/db2 = dl/dz2 = -1.0948</li> <li>dL/da1 = W2^T * dl/dz2 = [-0.5474, 0.3180]</li> <li>da1/dz1 = 1 - tanh(z1)^2 = [0.9305, 0.9682]</li> <li>dL/dz1 = (W2^T*dl/dz2) * (1 - tanh(z1)^2) = [-0.5093, 0.3180]</li> <li>dL/dW1 = dL/dz1 * x^T = [[-0.2546, 0.1018][0.1590, -0.0636]]</li> <li>dL/db1 = dL/dz1 = [-0.5093, 0.3180]</li> </ul>"},{"location":"exercicio3/main/#5-atualizacao-dos-parametros-gradient-descent","title":"5) Atualiza\u00e7\u00e3o dos par\u00e2metros (Gradient Descent)","text":"<ul> <li>W2 = W2 - eta * dL/dW2 = [0.5865, -0.3584]</li> <li>b2 = b2 - eta * dL/db2 = 0.5284</li> <li>W1 = W1 - eta * dl/dW1 = [[0.3764, -0.1305][0.1522, 0.4190]]</li> <li>b1 = b1 - eta * dL/db1 = [0.2528, -0.2954]  </li> </ul>"},{"location":"exercicio3/main/#mlp","title":"MLP","text":"<p>Objetivo. Implementar uma MLP modular em NumPy, suportando: - Entrada gen\u00e9rica (<code>input_dim = n_features</code>); - N camadas ocultas (lista de larguras), ativa\u00e7\u00e3o <code>tanh</code> em todas as ocultas; - Camada de sa\u00edda <code>softmax</code> com <code>output_dim = n_classes</code>; - Loss: categorical cross-entropy; - Otimiza\u00e7\u00e3o: Gradient Descent;.</p> <p>Fluxo do treino. 1. Inicializa\u00e7\u00e3o dos par\u00e2metros; 2. Forward; 3. Loss; 4. Backward; 5. Gradient Descent;  </p> <pre><code># mlp.py\nfrom __future__ import annotations\nfrom typing import List, Optional\nimport numpy as np\n\nfrom utils import (\n    tanh, dtanh_from_a,\n    softmax, one_hot, cross_entropy, accuracy_score,\n    xavier_init,\n)\n\nclass MLP:\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_layers: List[int] = [16, 16],\n        output_dim: int = 2,        \n        lr: float = 0.05,\n        max_epochs: int = 500,\n        batch_size: Optional[int] = None,\n        random_state: Optional[int] = 42,\n        track_history: bool = True,\n    ):\n        self.input_dim = input_dim\n        self.hidden_layers = hidden_layers\n        self.output_dim = output_dim\n        self.lr = lr\n        self.max_epochs = max_epochs\n        self.batch_size = batch_size\n        self.random_state = random_state\n        self.track_history = track_history\n\n        self.params_ = None\n        self.loss_history_: List[float] = []\n        self.acc_history_: List[float] = []\n\n    # ---------- initialization ----------\n    def _init_params(self, rng: np.random.Generator) -&gt; None:\n        layer_sizes = [self.input_dim] + self.hidden_layers + [self.output_dim]\n        W, b = [], []\n        for l in range(1, len(layer_sizes)):\n            fan_in = layer_sizes[l-1]\n            fan_out = layer_sizes[l]\n            W_l = xavier_init(fan_in, fan_out, rng)\n            b_l = np.zeros((fan_out, 1))\n            W.append(W_l)\n            b.append(b_l)\n        self.params_ = {\"W\": W, \"b\": b}\n\n    # ---------- forward ----------\n    def _forward(self, X: np.ndarray):\n        W, B = self.params_[\"W\"], self.params_[\"b\"]\n        A = X.T  \n        caches = [{\"A\": A}]  \n\n        # hidden layers\n        for l in range(len(self.hidden_layers)):\n            Z = W[l] @ A + B[l]\n            A = tanh(Z)\n            caches.append({\"Z\": Z, \"A\": A})\n\n        # output layer (softmax)\n        ZL = W[-1] @ A + B[-1]\n        P = softmax(ZL, axis=0)\n        caches.append({\"Z\": ZL, \"A\": P})\n        return caches, P.T\n\n    # ---------- backward ----------\n    def _backward(self, caches, y: np.ndarray):\n        W = self.params_[\"W\"]\n        L = len(W)\n        m = y.shape[0]\n\n        A0 = caches[0][\"A\"]\n        A_list = [A0] + [c[\"A\"] for c in caches[1:]]\n\n        Y = one_hot(y.reshape(-1), self.output_dim).T\n        P = A_list[-1]\n\n        dZ = (P - Y) / m\n        dW = [None] * L\n        dB = [None] * L\n\n        # \u00faltima camada\n        A_prev = A_list[-2]\n        dW[L-1] = dZ @ A_prev.T\n        dB[L-1] = np.sum(dZ, axis=1, keepdims=True)\n\n        # ocultas\n        for l in reversed(range(L-1)):\n            dA = W[l+1].T @ dZ\n            A_l = A_list[l+1]\n            dZ = dA * dtanh_from_a(A_l)\n\n            A_prev = A_list[l]\n            dW[l] = dZ @ A_prev.T\n            dB[l] = np.sum(dZ, axis=1, keepdims=True)\n\n        return dW, dB\n\n    # ---------- update ----------\n    def _update(self, dW, dB, lr: float) -&gt; None:\n        for l in range(len(self.params_[\"W\"])):\n            self.params_[\"W\"][l] -= lr * dW[l]\n            self.params_[\"b\"][l] -= lr * dB[l]\n\n    # ---------- fit ----------\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        rng = np.random.default_rng(self.random_state)\n        self._init_params(rng)\n\n        m = X.shape[0]\n        batch_size = self.batch_size or m\n\n        for epoch in range(1, self.max_epochs + 1):\n            idx = rng.permutation(m)\n            X_shuf = X[idx]\n            y_shuf = y[idx]\n\n            for start in range(0, m, batch_size):\n                end = min(start + batch_size, m)\n                Xb = X_shuf[start:end]\n                yb = y_shuf[start:end]\n\n                caches, _ = self._forward(Xb)\n                dW, dB = self._backward(caches, yb)\n                self._update(dW, dB, self.lr)\n\n            if self.track_history:\n                P_full = self.predict_proba(X)\n                Y_full = one_hot(y, self.output_dim)\n                loss = cross_entropy(Y_full, P_full)\n                y_pred = np.argmax(P_full, axis=1)\n                acc = accuracy_score(y, y_pred)\n                self.loss_history_.append(loss)\n                self.acc_history_.append(acc)\n\n        return {\"epochs_run\": self.max_epochs}\n\n    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n        _, P = self._forward(X)\n        return P\n\n    def decision_function(self, X: np.ndarray) -&gt; np.ndarray:\n        W, B = self.params_[\"W\"], self.params_[\"b\"]\n        A = X.T\n        for l in range(len(self.hidden_layers)):\n            A = tanh(W[l] @ A + B[l])\n        ZL = W[-1] @ A + B[-1]\n        return ZL.T\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        P = self.predict_proba(X)\n        return np.argmax(P, axis=1)\n</code></pre> <p>Fun\u00e7\u00f5es Aux\u00edliares </p><pre><code># utils.py\nfrom __future__ import annotations\nimport numpy as np\n\n# -----------------------------\n# Ativa\u00e7\u00f5es e derivadas\n# -----------------------------\ndef tanh(z: np.ndarray) -&gt; np.ndarray:\n    return np.tanh(z)\n\ndef dtanh_from_a(a: np.ndarray) -&gt; np.ndarray:\n    return 1.0 - a**2\n\ndef softmax(Z: np.ndarray, axis: int = 0) -&gt; np.ndarray:\n    \"\"\"\n    Z: (K, m) -&gt; aplica softmax por coluna (axis=0).\n    Retorna prob. por classe, colunas somam 1.\n    \"\"\"\n    Z_shift = Z - np.max(Z, axis=axis, keepdims=True)\n    e = np.exp(Z_shift)\n    return e / np.sum(e, axis=axis, keepdims=True)\n\n# -----------------------------\n# Loss e m\u00e9tricas\n# -----------------------------\ndef bce_loss(y_true: np.ndarray, y_prob: np.ndarray, eps: float = 1e-12) -&gt; float:\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return float(-np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob)))\n\ndef cross_entropy(y_true_oh: np.ndarray, y_prob: np.ndarray, eps: float = 1e-12) -&gt; float:\n    \"\"\"\n    y_true_oh: (m, K) one-hot\n    y_prob   : (m, K) probabilidades (softmax)\n    \"\"\"\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return float(-np.mean(np.sum(y_true_oh * np.log(y_prob), axis=1)))\n\ndef accuracy_score(y_true: np.ndarray, y_pred_labels: np.ndarray) -&gt; float:\n    return float(np.mean(y_true == y_pred_labels))\n\n# -----------------------------\n# Split 80/20\n# -----------------------------\ndef train_test_split(X: np.ndarray, y: np.ndarray, test_size: float = 0.2, random_state: int = 42):\n    rng = np.random.default_rng(random_state)\n    m = X.shape[0]\n    idx = rng.permutation(m)\n    m_test = int(np.floor(test_size * m))\n    test_idx = idx[:m_test]\n    train_idx = idx[m_test:]\n    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n\n# -----------------------------\n# Inicializa\u00e7\u00f5es\n# -----------------------------\ndef xavier_init(fan_in: int, fan_out: int, rng: np.random.Generator) -&gt; np.ndarray:\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n    return rng.normal(0.0, std, size=(fan_out, fan_in))\n\n# -----------------------------\n# Helpers\n# -----------------------------\ndef one_hot(y: np.ndarray, K: int) -&gt; np.ndarray:\n    \"\"\"\n    y: (m,) com r\u00f3tulos inteiros [0..K-1]\n    retorna: (m, K) one-hot\n    \"\"\"\n    m = y.shape[0]\n    Y = np.zeros((m, K), dtype=float)\n    Y[np.arange(m), y.astype(int)] = 1.0\n    return Y\n</code></pre><p></p> <p>Pr\u00e9-processamento. - MinMax [-1, 1] nos atributos, para combinar com ativa\u00e7\u00e3o tanh nas ocultas.</p> <pre><code>from __future__ import annotations\nimport numpy as np\n\nclass MinMaxScaler:\n    \"\"\"\n    Escala os dados para o intervalo [-1, 1].\n    \"\"\"\n    def __init__(self):\n        self.min_: np.ndarray | None = None\n        self.max_: np.ndarray | None = None\n\n    def fit(self, X: np.ndarray) -&gt; \"MinMaxScaler\":\n        self.min_ = X.min(axis=0)\n        self.max_ = X.max(axis=0)\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        if self.min_ is None or self.max_ is None:\n            raise RuntimeError(\"Scaler n\u00e3o ajustado. Chame fit() antes de transform().\")\n\n        # normaliza para [0, 1]\n        X_norm = (X - self.min_) / (self.max_ - self.min_ + 1e-12)\n        # reescala para [-1, 1]\n        return 2.0 * X_norm - 1.0\n\n    def fit_transform(self, X: np.ndarray) -&gt; np.ndarray:\n        return self.fit(X).transform(X)\n</code></pre> <p>Gera\u00e7\u00e3o de Dados </p><pre><code>from __future__ import annotations\nimport numpy as np\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\n\ndef make_varying_classification(\n    n_samples: int,\n    n_classes: int,\n    n_features: int,\n    clusters_per_class,\n    class_sep: float = 1.2,           \n    flip_y: float = 0.2,\n    random_state: int = 42,\n    shuffle: bool = False,\n):\n    \"\"\"\n    Gera dados sint\u00e9ticos com n\u00ba de clusters vari\u00e1vel por classe,\n    usando make_classification de forma simplificada.\n\n    - Divide amostras de forma uniforme entre as classes\n    - n_informative = n_features\n    - n_redundant = 0\n    - class_sep e flip_y ajust\u00e1veis\n    \"\"\"\n    clusters_per_class = list(clusters_per_class)\n    if n_classes &lt; 2 or len(clusters_per_class) != n_classes:\n        raise ValueError(\"clusters_per_class deve ter n_classes elementos e n_classes &gt;= 2.\")\n\n    # Divide amostras uniformemente\n    base = n_samples // n_classes\n    counts = [base] * n_classes\n    for i in range(n_samples - base * n_classes):\n        counts[i] += 1\n\n    X_parts, y_parts = [], []\n    for c, m_c in enumerate(counts):\n        seed_c = (random_state + 10007 * (c + 1)) % (2**31 - 1)\n        X_c, _ = make_classification(\n            n_samples=m_c,\n            n_features=n_features,\n            n_informative=n_features,\n            n_redundant=0,\n            n_repeated=0,\n            n_classes=2,               \n            n_clusters_per_class=clusters_per_class[c],\n            weights=[1.0, 0.0],        \n            class_sep=class_sep,\n            flip_y=flip_y,\n            shuffle=True,\n            random_state=seed_c,\n        )\n        X_parts.append(X_c)\n        y_parts.append(np.full(m_c, c, dtype=int))\n\n    X = np.vstack(X_parts)\n    y = np.concatenate(y_parts)\n\n    if shuffle:\n        rng = np.random.default_rng(random_state)\n        idx = rng.permutation(X.shape[0])\n        X, y = X[idx], y[idx]\n\n    return X, y\n\ndef plot_classification_data(X: np.ndarray, y: np.ndarray, title: str = \"Synthetic Data\"):\n    \"\"\"\n    Plota os dados 2D gerados por make_varying_classification.\n\n    Par\u00e2metros:\n      X : np.ndarray (n_samples, 2) -&gt; features\n      y : np.ndarray (n_samples,)   -&gt; r\u00f3tulos (0, 1, ..., n_classes-1)\n      title : t\u00edtulo opcional do gr\u00e1fico\n    \"\"\"\n    if X.shape[1] != 2:\n        raise ValueError(\"O plot s\u00f3 funciona para n_features=2.\")\n\n    plt.figure(figsize=(6, 6))\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"tab10\", edgecolor=\"k\", s=40, alpha=0.8)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.title(title)\n    plt.legend(*scatter.legend_elements(), title=\"Classes\")\n    plt.grid(True, linestyle=\"--\", alpha=0.6)\n    plt.show()\n</code></pre><p></p>"},{"location":"exercicio3/main/#exercicio-2-classificacao-binaria","title":"Exerc\u00edcio 2 \u2014 Classifica\u00e7\u00e3o Bin\u00e1ria","text":"<p>Objetivo. Treinar uma MLP em um conjunto de dados bin\u00e1rios, com clusters assim\u00e9tricos e 2 features.</p>"},{"location":"exercicio3/main/#especificacao-dos-dados","title":"Especifica\u00e7\u00e3o dos Dados","text":"<ul> <li>Amostras: 1000  </li> <li>Classes: 2  </li> <li>Features: 2 </li> <li>Clusters por classe: <code>[2, 1]</code></li> </ul>"},{"location":"exercicio3/main/#resultados","title":"Resultados","text":"<p>Foram realizados testes com 1 camada oculta de profundidade 1, 1 camada oculta de profundidade 16 e 2 camadas ocultas de profundidade 16 para 500 \u00e9pocas.</p> <ul> <li>Train Loss: 0.4671, 0.4634, 0.2879  </li> <li>Train Accuracy: 0.7750, 0.7738, 0.8538  </li> <li>Test Loss: 0.4553, 0.4509, 0.2763</li> <li>Test Accuracy: 0.8000, 0.8050, 0.8500</li> </ul> <p> </p>"},{"location":"exercicio3/main/#exercicio-3-4-classificacao-multiclasse","title":"Exerc\u00edcio 3 / 4 \u2014 Classifica\u00e7\u00e3o Multiclasse","text":"<p>Objetivo. Treinar uma MLP em um conjunto de dados com 3 classes distintas, clusters assim\u00e9tricos e 4 features.</p>"},{"location":"exercicio3/main/#especificacao-dos-dados_1","title":"Especifica\u00e7\u00e3o dos Dados","text":"<ul> <li>Amostras: 1500  </li> <li>Classes: 3  </li> <li>Features: 4  </li> <li>Clusters por classe: <code>[2, 3, 4]</code> </li> </ul>"},{"location":"exercicio3/main/#resultados_1","title":"Resultados","text":"<p>Foram realizados testes com 1 camada oculta de profundidade 16, 2 camada oculta de profundidade 16 e 3 camadas ocultas de profundidade 16 para 500 \u00e9pocas.</p> <ul> <li>Train Loss: 0.7644, 0.5351, 0.4725  </li> <li>Train Accuracy: 0.6142, 0.7625, 0.7817  </li> <li>Test Loss: 0.7889, 0.6355, 0.5803</li> <li>Test Accuracy: 0.5933, 0.6767, 0.7200</li> </ul> <p> </p>"}]}