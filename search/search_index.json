{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#bem-vindoa-deep-learning-20251","title":"Bem-vindo(a) \u2014 Deep Learning (2025.1)","text":"<p>Autor: Caio Ortega Boa Turma: 2025.2 Reposit\u00f3rio: Este site re\u00fane todo o conte\u00fado produzido por Caio Ortega Boa na disciplina de Deep Learning \u2014 exerc\u00edcios, c\u00f3digos, an\u00e1lises, gr\u00e1ficos e anota\u00e7\u00f5es.</p>"},{"location":"#sobre-esta-pagina","title":"Sobre esta p\u00e1gina","text":"<p>Este \u00e9 o hub do projeto da mat\u00e9ria. Aqui voc\u00ea encontra: - vis\u00e3o geral do que foi feito; - links para notebooks/c\u00f3digos; - entregas da mat\u00e9ria;</p>"},{"location":"template/","title":"Template de Entrega","text":""},{"location":"template/#template-de-entrega","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"template/#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> <li>Maria Oliveira</li> <li>Grupo K<ul> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"template/#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"template/#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"template/#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"template/#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"template/#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"exercicio1/main/","title":"Exercicio 1","text":""},{"location":"exercicio1/main/#deep-learning-data","title":"Deep Learning \u2014 Data","text":"<p>Autor: Caio Ortega Boa Disciplina: Deep Learning Per\u00edodo: 2025.1 Link do Reposit\u00f3rio</p>"},{"location":"exercicio1/main/#sumario","title":"Sum\u00e1rio","text":"<ul> <li>Exerc\u00edcio 1: Separabilidade em 2D (dados sint\u00e9ticos gaussianos)  </li> <li>Exerc\u00edcio 2: N\u00e3o-linearidade em 5D e proje\u00e7\u00e3o PCA \u2192 2D  </li> <li>Exerc\u00edcio 3: Pr\u00e9-processamento do Spaceship Titanic (Kaggle) para MLP com <code>tanh</code></li> </ul>"},{"location":"exercicio1/main/#exercicio-1-class-separability-em-2d","title":"Exerc\u00edcio 1 \u2014 Class Separability em 2D","text":"<p>Objetivo. Explorar como a distribui\u00e7\u00e3o de quatro classes em 2D influencia a complexidade das fronteiras de decis\u00e3o que uma rede neural precisaria aprender.</p>"},{"location":"exercicio1/main/#parametros-utilizados","title":"Par\u00e2metros utilizados","text":"<ul> <li>M\u00e9dias (\u03bcx, \u03bcy): (2,3), (5,6), (8,1), (15,4)</li> <li>Desvios (\u03c3x, \u03c3y): (0,8; 2,5), (1,2; 1,9), (0,9; 0,9), (0,5; 2,0) </li> </ul>"},{"location":"exercicio1/main/#visualizacao","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio1/main/#analise-e-respostas","title":"An\u00e1lise e respostas","text":"<ul> <li>Distribui\u00e7\u00e3o e overlap: Classes 0 e 1 apresentam sobreposi\u00e7\u00e3o consider\u00e1vel enquanto as classes 1 e 2 apresentam leve sobreposi\u00e7\u00e3o; a Classe 3 est\u00e1 deslocada \u00e0 direita sem nenhuma sobreposi\u00e7\u00e3o.  </li> <li>Uma fronteira linear simples separa tudo? N\u00e3o. Com uma \u00fanica fronteira linear n\u00e3o \u00e9 poss\u00edvel separar todas as classes corretamente.  </li> <li>\u201cSketch\u201d das fronteiras que a rede aprenderia: Para separar de maneira eficiente todas as classes seriam necess\u00e1rias pelo menos 3 fronteiras lineares.</li> </ul>"},{"location":"exercicio1/main/#codigo","title":"C\u00f3digo","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Garante reprodutibilidade dos n\u00fameros aleat\u00f3rios\nnp.random.seed(42)\n\ndef draw_line(ax, point1, point2, style='--', color='k', lw=2):\n    \"\"\"\n    Desenha uma linha entre dois pontos no gr\u00e1fico.\n\n    Par\u00e2metros:\n    - ax: objeto matplotlib.axes onde a linha ser\u00e1 desenhada.\n    - point1: tupla (x1, y1) do primeiro ponto.\n    - point2: tupla (x2, y2) do segundo ponto.\n    - style: estilo da linha (default='--' tracejada).\n    - color: cor da linha (default='k' preto).\n    - lw: espessura da linha.\n    - label: legenda opcional para a linha.\n    \"\"\"\n    x_vals = [point1[0], point2[0]]\n    y_vals = [point1[1], point2[1]]\n    ax.plot(x_vals, y_vals, style, color=color, lw=lw)\n\n# Par\u00e2metros das distribui\u00e7\u00f5es gaussianas:\n# means = lista de tuplas (\u03bcx, \u03bcy) = m\u00e9dias em cada eixo\n# stds = lista de tuplas (\u03c3x, \u03c3y) = desvios em cada eixo\nmeans = [(2,3), (5,6), (8,1), (15,4)]\nstds = [(0.8,2.5), (1.2,1.9), (0.9,0.9), (0.5,2.0)]\n\n# Listas para acumular pontos (X) e r\u00f3tulos (y)\nX = []\ny = []\n\n# Para cada classe (0,1,2,3) gera 100 pontos 2D\n# np.random.normal aceita tuplas em loc/scale\n# loc=(\u03bcx, \u03bcy) =&gt; m\u00e9dia por eixo\n# scale=(\u03c3x, \u03c3y) =&gt; desvio por eixo\n# Isso equivale a gaussianas 2D com covari\u00e2ncia diagonal\nfor i, (mean, std) in enumerate(zip(means, stds)):\n    points = np.random.normal(loc=mean, scale=std, size=(100,2))\n    X.append(points)               # pontos da classe i\n    y.append(np.full(100, i))      # vetor [i, i, ..., i] (100 vezes)\n\n# Empilha todas as classes em um \u00fanico array\nX = np.vstack(X)  # shape (400, 2)\ny = np.hstack(y)  # shape (400,)\n\n# Cria o gr\u00e1fico de dispers\u00e3o\nfig, ax = plt.subplots(figsize=(8,6))\nfor i in range(4):\n    # Plota os pontos da classe i\n    ax.scatter(X[y==i,0], X[y==i,1], label=f'Class {i}', alpha=0.7)\n\n#Desenhando linhas arbitrarias de separa\u00e7\u00e3o\ndraw_line(ax, (5,-3), (2,15), style='-', color='purple')\ndraw_line(ax, (4.5,1.5), (12.5,5), style='-', color='purple')\ndraw_line(ax, (12.5,-3), (12.5,15), style='-', color='purple')\n\nax.legend()\nax.set_title(\"Synthetic 2D Gaussian Dataset\")\nax.set_xlabel(\"X1\")\nax.set_ylabel(\"X2\")\nplt.show()\n</code></pre>"},{"location":"exercicio1/main/#exercicio-2-nao-linearidade-em-5d-pca-5d-2d","title":"Exerc\u00edcio 2 \u2014 N\u00e3o-linearidade em 5D + PCA (5D \u2192 2D)","text":"<p>Objetivo. Criar dois grupos 5D com m\u00e9dias/covari\u00e2ncias especificadas e visualizar em 2D via PCA.</p>"},{"location":"exercicio1/main/#configuracao","title":"Configura\u00e7\u00e3o","text":"<ul> <li>Classe A: vetor de m\u00e9dia nulo; covari\u00e2ncias positivas entre algumas dimens\u00f5es.  </li> <li>Classe B: vetor de m\u00e9dia transladado (1,5 em todas as componentes); covari\u00e2ncias com sinais distintos, alterando forma e orienta\u00e7\u00e3o do grupo.</li> </ul>"},{"location":"exercicio1/main/#visualizacao_1","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio1/main/#analise-e-respostas_1","title":"An\u00e1lise e respostas","text":"<ul> <li>Rela\u00e7\u00e3o entre as classes (proje\u00e7\u00e3o 2D): Observa-se mistura parcial, embora possa se identificar certa separa\u00e7\u00e3o entre as nuvens.  </li> <li>Separabilidade linear: Embora possa ser observado uma distribui\u00e7\u00e3o com certa separa\u00e7\u00e3o na proje\u00e7\u00e3o 2d dos dados, uma separa\u00e7\u00e3o linear seria muito ineficiente para o caso proposto. Por haverem m\u00faltiplas dimens\u00f5es nos dados a separabilidade linear tenderia a perder muita informa\u00e7\u00e3o, por n\u00e3o haver um hiperplano perfeito capaz de separar as duas classes.</li> </ul>"},{"location":"exercicio1/main/#codigo_1","title":"C\u00f3digo","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Reprodutibilidade\nnp.random.seed(42)\n\n# -----------------------------\n# 1) Define par\u00e2metros 5D\n# -----------------------------\n# Classe A\nmu_A = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.3, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n], dtype=float)\n\n# Classe B\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigma_B = np.array([\n    [ 1.5, -0.7,  0.2,  0.0, 0.0],\n    [-0.7,  1.5,  0.4,  0.0, 0.0],\n    [ 0.2,  0.4,  1.5,  0.6, 0.0],\n    [ 0.0,  0.0,  0.6,  1.5, 0.3],\n    [ 0.0,  0.0,  0.0,  0.3, 1.5],\n], dtype=float)\n\n# -----------------------------\n# 2) Gera\u00e7\u00e3o dos dados (5D)\n# -----------------------------\nnA, nB = 500, 500\nXA = np.random.multivariate_normal(mean=mu_A, cov=Sigma_A, size=nA)\nXB = np.random.multivariate_normal(mean=mu_B, cov=Sigma_B, size=nB)\n\n# Empilha dados e r\u00f3tulos\nX_5d = np.vstack([XA, XB])         # (1000, 5)\ny    = np.array([0]*nA + [1]*nB)  # 0 = Classe A, 1 = Classe B\n\n# -----------------------------\n# 3) Redu\u00e7\u00e3o de dimensionalidade (PCA \u2192 2D)\n# -----------------------------\npca = PCA(n_components=2, random_state=42)\nX_2d = pca.fit_transform(X_5d)     # (1000, 2)\n\n# -----------------------------\n# 4) Visualiza\u00e7\u00e3o (apenas pontos)\n# -----------------------------\nplt.figure(figsize=(8, 6))\nplt.scatter(X_2d[y==0, 0], X_2d[y==0, 1], alpha=0.6, s=18, label=\"Class A\")\nplt.scatter(X_2d[y==1, 0], X_2d[y==1, 1], alpha=0.6, s=18, label=\"Class B\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Exercise 2 \u2014 PCA (5D \u2192 2D) scatter\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercicio1/main/#exercicio-3-spaceship-titanic-kaggle-pre-processamento-para-tanh","title":"Exerc\u00edcio 3 \u2014 Spaceship Titanic (Kaggle): Pr\u00e9-processamento para <code>tanh</code>","text":"<p>Objetivo. Preparar dados reais para uma MLP com <code>tanh</code>, assegurando entradas est\u00e1veis.</p>"},{"location":"exercicio1/main/#descricao-do-dataset","title":"Descri\u00e7\u00e3o do dataset","text":"<ul> <li>Objetivo do Dataset O Dataset simula um \"Titanic espacial\", que estaria lotado de passageiros e colidiu com uma anomalia espacial que trasnportou diversos passageiros para outra dimens\u00e3o. O objetivo do dataset \u00e9 descobrir quais passageiros teriam sido transportados para essa dimens\u00e3o alternativa baseado em seus dados.</li> <li>Alvo: <code>Transported</code> \u2014 indica se o passageiro foi transportado para outra dimens\u00e3o (bin\u00e1rio).  </li> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>, <code>CabinNum</code>, <code>Group</code>, <code>PaxInGroup</code>, <code>TotalSpend</code>.  </li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code>, <code>CabinDeck</code>, <code>CabinSide</code>.  </li> <li>Engenharia aplicada: A feature <code>Cabin</code> foi decomposta em <code>CabinDeck</code>, <code>CabinNum</code> e <code>CabinSide</code>;  <code>PassengerId</code> foi decomposto em <code>Group</code> e <code>PaxInGroup</code>; <code>Transported</code> foi convertido para 0/1.</li> </ul>"},{"location":"exercicio1/main/#faltantes","title":"Faltantes","text":"<ul> <li>Investiga\u00e7\u00e3o Todas as colunas, fora <code>PassengerId</code> e <code>Transported</code> possuiam dados faltantes.</li> <li>Num\u00e9ricas: imputa\u00e7\u00e3o pela mediana (robusta a outliers; preserva a posi\u00e7\u00e3o central).  </li> <li>Categ\u00f3ricas: imputa\u00e7\u00e3o pela moda (mant\u00e9m r\u00f3tulos conhecidos; evita categorias artificiais).  </li> </ul>"},{"location":"exercicio1/main/#tratamento-das-features","title":"Tratamento das Features","text":"<ul> <li>One-Hot Encoding Utilizado para tratamento de features categ\u00f3ricas.</li> <li>Normaliza\u00e7\u00e3o para <code>[-1, 1]</code>: Utilizado para tratamento de features num\u00e9ricas, de modo a acomodar os dados corretamente para a utiliza\u00e7\u00e3o da fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>. Se trata de uma boa pr\u00e1tica pois possibilita padroniza a escala das features e possibilita que a maior parte dos dados esteja na parte central da curva, otimizando o treinamento.</li> </ul>"},{"location":"exercicio1/main/#visualizacoes","title":"Visualiza\u00e7\u00f5es","text":"<p>Antes da transforma\u00e7\u00e3o </p> <p>Depois da transforma\u00e7\u00e3o </p>"},{"location":"exercicio1/main/#codigo_2","title":"C\u00f3digo","text":"<pre><code>#Imports e leitura\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n#Configs visuais\nplt.rcParams[\"figure.figsize\"] = (8, 5)\nplt.rcParams[\"axes.grid\"] = True\n\n#Reprodutibilidade\nnp.random.seed(42)\n\n#Caminho do arquivo\nCSV_PATH = \"spaceship.csv\"\n\ndf = pd.read_csv(CSV_PATH)\nprint(df.shape)\ndf.head()\n</code></pre> <pre><code>#Vis\u00e3o geral: tipos e faltantes\nprint(\"\\n=== info() ===\")\ndf.info()\n\nprint(\"\\n=== Missing values por coluna ===\")\nmissing_abs = df.isna().sum().sort_values(ascending=False)\nmissing_pct = (df.isna().mean()*100).sort_values(ascending=False)\ndisplay(pd.DataFrame({\"missing\": missing_abs, \"missing_%\": missing_pct.round(2)}))\n</code></pre> <pre><code>#Quebra Cabin em deck/num/side\ncabin = df[\"Cabin\"].astype(\"string\")\nparts = cabin.str.split(\"/\", expand=True)\ndf[\"CabinDeck\"] = parts[0]\ndf[\"CabinNum\"]  = pd.to_numeric(parts[1], errors=\"coerce\")\ndf[\"CabinSide\"] = parts[2]\n\n#Quebra PassengerId em grupo e \u00edndice no grupo\npid = df[\"PassengerId\"].astype(\"string\")\ngrp_pp = pid.str.split(\"_\", expand=True)\ndf[\"Group\"] = pd.to_numeric(grp_pp[0], errors=\"coerce\")  \ndf[\"PaxInGroup\"] = pd.to_numeric(grp_pp[1], errors=\"coerce\")\n\n#Transported -&gt; 0/1\ndf[\"Transported\"] = df[\"Transported\"].map({True:1, False:0, \"True\":1, \"False\":0}).astype(int)\n\n#Colunas que n\u00e3o vamos usar como features\ndrop_cols = [\"PassengerId\", \"Name\", \"Cabin\"] \n\n#Colunas num\u00e9ricas e categ\u00f3ricas\nnumeric_features = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Group\", \"CabinNum\", \"PaxInGroup\"]\ncategorical_features = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"CabinDeck\", \"CabinSide\"]\n\nprint(\"numeric_features:\", numeric_features)\nprint(\"categorical_features:\", categorical_features)\n\ndf_pre = df.drop(columns=drop_cols).copy()\ndf_pre.head()\n</code></pre> <pre><code>#Separa\u00e7\u00e3o X, y\ntarget = \"Transported\"\nX = df_pre.drop(columns=[target])\ny = df_pre[target].values\n</code></pre> <pre><code>#Sanitiza\u00e7\u00e3o (Corre\u00e7\u00e3o de erros no processamento)\nX = X.copy()\n\n#Booleans para strings\nfor col in [\"CryoSleep\", \"VIP\"]:\n    if col in X.columns:\n        X[col] = X[col].map({True: \"True\", False: \"False\"}).astype(\"object\")\n\n#CATEG\u00d3RICAS como 'object' e remover pd.NA\nfor c in categorical_features:\n    if c in X.columns:\n        X[c] = X[c].astype(\"object\")\n        mask = pd.isna(X[c])\n        if mask.any():\n            X.loc[mask, c] = np.nan\n\n#NUM\u00c9RICAS realmente num\u00e9ricas\nfor c in numeric_features:\n    if c in X.columns:\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n</code></pre> <pre><code># Num\u00e9ricas: imputar mediana + scaler (-1, 1)\nnum_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\",  MinMaxScaler(feature_range=(-1, 1))),\n])\n\n# Categ\u00f3ricas: imputar moda + OneHot\ncat_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\",  OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", num_pipe, numeric_features),\n        (\"cat\", cat_pipe, categorical_features),\n    ],\n    remainder=\"drop\",\n)\n\nX_proc = preprocessor.fit_transform(X)\n\nprint(\"X_proc shape:\", X_proc.shape)\n</code></pre> <pre><code>#Checagem de colunas ap\u00f3s OHE\nnum_names = numeric_features\ncat_names = preprocessor.named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out(categorical_features).tolist()\nfinal_feature_names = num_names + cat_names\n\nprint(\"Total de colunas ap\u00f3s OHE:\", len(final_feature_names))\n</code></pre> <pre><code>#Plotagem dos histogramas\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Antes\nX[\"Age\"].hist(ax=axes[0], bins=30, alpha=0.8)\naxes[0].set_title(\"Age \u2014 antes da escala\")\naxes[0].set_xlabel(\"Age\")\n\nX[\"FoodCourt\"].hist(ax=axes[1], bins=30, alpha=0.8)\naxes[1].set_title(\"FoodCourt \u2014 antes da escala\")\naxes[1].set_xlabel(\"FoodCourt\")\nplt.tight_layout()\nplt.show()\n\n# Depois\nage_idx = numeric_features.index(\"Age\")\nfood_idx = numeric_features.index(\"FoodCourt\")\n\nage_scaled   = X_proc[:, age_idx]\nfood_scaled  = X_proc[:, food_idx]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].hist(age_scaled, bins=30, alpha=0.8)\naxes[0].set_title(\"Age \u2014 depois (StandardScaler)\")\naxes[0].set_xlabel(\"z-score\")\n\naxes[1].hist(food_scaled, bins=30, alpha=0.8)\naxes[1].set_title(\"FoodCourt \u2014 depois (StandardScaler)\")\naxes[1].set_xlabel(\"z-score\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercicio2/main/","title":"Exercicio 2","text":""},{"location":"exercicio2/main/#deep-learning-perceptron","title":"Deep Learning \u2014 Perceptron","text":"<p>Autor: Caio Ortega Boa Disciplina: Deep Learning Per\u00edodo: 2025.1 Link do Reposit\u00f3rio</p>"},{"location":"exercicio2/main/#sumario","title":"Sum\u00e1rio","text":"<ul> <li>Perceptron Implementa\u00e7\u00e3o de um Perceptron</li> <li>Exerc\u00edcio 1: Treinamento de Perceptron com Converg\u00eancia </li> <li>Exerc\u00edcio 2: Treinamento de Perceptron sem Converg\u00eancia</li> </ul>"},{"location":"exercicio2/main/#perceptron-implementacao-de-um-perceptron","title":"Perceptron \u2014 Implementa\u00e7\u00e3o de um Perceptron","text":"<p>Objetivo. Implementa\u00e7\u00e3o de um Perceptron sem utiliza\u00e7\u00e3o de bibliotecas auxiliares e modelos prontos, apenas Numpy para c\u00e1lculos matriciais.</p>"},{"location":"exercicio2/main/#codigo","title":"C\u00f3digo","text":"<pre><code>import numpy as np\n\nclass Perceptron:\n    \"\"\"\n    Single-layer Perceptron implemented from scratch.\n    \"\"\"\n    def __init__(\n        self,\n        lr: float = 0.1,\n        max_epochs: int = 100,\n        random_state: int | None = None,\n        track_history: bool = True,\n    ):\n        self.lr = lr\n        self.max_epochs = max_epochs\n        self.random_state = random_state\n        self.track_history = track_history\n\n        self.w = None\n        self.b = 0.0\n        self.accuracy_history_ = []\n\n    def decision_function(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute raw scores (w\u00b7x + b).\n        X: shape (n_samples, n_features)\n        \"\"\"\n        if self.w is None:\n            raise ValueError(\"Model is not fitted yet.\")\n        return X @ self.w + self.b\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict labels in {-1, +1} using sign(w\u00b7x + b).\n        \"\"\"\n        scores = self.decision_function(X)\n        return np.where(scores &gt;= 0, 1, -1)\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; dict:\n        \"\"\"\n        Fit the perceptron on labels in {-1, +1}.\n        Stops early if an epoch completes with zero updates.\n        Records accuracy after each epoch.\n        Returns a training log with 'epochs_run' and 'converged' keys.\n        \"\"\"\n        rng = np.random.default_rng(self.random_state)\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.w = np.zeros(n_features, dtype=float)\n        self.b = 0.0\n        self.accuracy_history_ = []\n\n        indices = np.arange(n_samples)\n\n        converged = False\n        epochs_run = 0\n\n        for epoch in range(self.max_epochs):\n            epochs_run += 1\n            # Shuffle each epoch for robustness\n            rng.shuffle(indices)\n            updates = 0\n\n            for idx in indices:\n                x_i = X[idx]\n                y_i = y[idx]  # must be -1 or +1\n                margin = y_i * (np.dot(self.w, x_i) + self.b)\n                if margin &lt;= 0:\n                    # Misclassified -&gt; update\n                    self.w = self.w + self.lr * y_i * x_i\n                    self.b = self.b + self.lr * y_i\n                    updates += 1\n\n            # Accuracy after epoch\n            y_pred = self.predict(X)\n            acc = np.mean(y_pred == y)\n            self.accuracy_history_.append(acc)\n\n            if updates == 0:\n                converged = True\n                break\n\n        return {\"epochs_run\": epochs_run, \"converged\": converged}\n</code></pre>"},{"location":"exercicio2/main/#exercicio-1-treinamento-de-perceptron-com-convergencia","title":"Exerc\u00edcio 1 \u2014 Treinamento de Perceptron com Converg\u00eancia","text":"<p>Objetivo. Gerar um dataset 2d com clara distin\u00e7\u00e3o entre classes para realiza\u00e7\u00e3o do treinamento de um perceptron.</p>"},{"location":"exercicio2/main/#parametros-utilizados","title":"Par\u00e2metros utilizados","text":"<ul> <li>M\u00e9dias (\u03bcx, \u03bcy): (1.5,1.5), (5,5)</li> <li>Desvios (\u03c3x, \u03c3y): (0.5; 0), (0; 0.5)</li> </ul>"},{"location":"exercicio2/main/#visualizacao","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio2/main/#resultados","title":"Resultados","text":"<ul> <li>Final Weights [0.304 , 0.199]</li> <li>Final Bias -1.6</li> <li>Epochs 2</li> <li>Fina Accuracy 1.0</li> </ul>"},{"location":"exercicio2/main/#analise-dos-resultados","title":"An\u00e1lise dos Resultados","text":"<ul> <li>Alta Separabilidade A alta separabilidade dos dados gerados, apresentando classes extremamente bem definidas por uma \u00fanica linha de decis\u00e3o, s\u00e3o respons\u00e1veis pelo baixo n\u00famero de \u00e9pocas necess\u00e1rio para a convers\u00e3o. Isso se deve pois, conforme o treinamento ocorre e a linha se enviesa para os dados, o modelo passa a classificar todos os dados corretamente, ocasionando em sua converg\u00eancia.</li> </ul>"},{"location":"exercicio2/main/#codigo_1","title":"C\u00f3digo","text":"<pre><code>mean0 = np.array([1.5, 1.5])\ncov0  = np.array([[0.5, 0.0],[0.0, 0.5]])\n\nmean1 = np.array([5.0, 5.0])\ncov1  = np.array([[0.5, 0.0],[0.0, 0.5]])\n\nn_per_class = 1000\n\nX0 = rng.multivariate_normal(mean0, cov0, size=n_per_class)\nX1 = rng.multivariate_normal(mean1, cov1, size=n_per_class)\n\nX = np.vstack([X0, X1])\ny01 = np.hstack([np.zeros(n_per_class, dtype=int), np.ones(n_per_class, dtype=int)])\ny = np.where(y01 == 1, 1, -1)\n\nplt.figure()\nplt.scatter(X0[:, 0], X0[:, 1], label=\"Class 0\")\nplt.scatter(X1[:, 0], X1[:, 1], label=\"Class 1\")\nplt.title(\"Exercise 1: Generated Data\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Train perceptron\nclf = Perceptron(lr=0.1, max_epochs=100, random_state=42)\nlog = clf.fit(X, y)\n\n# Evaluate\ny_pred = clf.predict(X)\nacc = np.mean(y_pred == y)\nprint(\"Epochs run:\", log[\"epochs_run\"])\nprint(\"Converged:\", log[\"converged\"])\nprint(\"Final weights:\", clf.w)\nprint(\"Final bias:\", clf.b)\nprint(\"Final accuracy:\", acc)\n</code></pre>"},{"location":"exercicio2/main/#exercicio-2-treinamento-de-perceptron-sem-convergencia","title":"Exerc\u00edcio 2 \u2014 Treinamento de Perceptron sem Converg\u00eancia","text":"<p>Objetivo. Gerar um dataset 2d com baixa distin\u00e7\u00e3o entre classes para realiza\u00e7\u00e3o do treinamento de um perceptron.</p>"},{"location":"exercicio2/main/#parametros-utilizados_1","title":"Par\u00e2metros utilizados","text":"<ul> <li>M\u00e9dias (\u03bcx, \u03bcy): (3.0,3.0), (4.0,4.0)</li> <li>Desvios (\u03c3x, \u03c3y): (1.5; 0), (0; 1.5)</li> </ul>"},{"location":"exercicio2/main/#visualizacao_1","title":"Visualiza\u00e7\u00e3o","text":""},{"location":"exercicio2/main/#resultados_1","title":"Resultados","text":"<ul> <li>Final Weights [-0.140 , 0.955]</li> <li>Final Bias -4.5</li> <li>Epochs 100</li> <li>Fina Accuracy 0.558</li> </ul>"},{"location":"exercicio2/main/#analise-dos-resultados_1","title":"An\u00e1lise dos Resultados","text":"<ul> <li>Baixa Separabilidade No seguinte dataset, havendo baixa separabilidade de dados, o modelo do perceptron n\u00e3o conseguiu convergir. Isso se deve a mistura e sobreposi\u00e7\u00e3o de dados que existe no dataset, impossibilitando que todas as classes sejam devidamente identificadas por uma \u00fanica linha de decis\u00e3o linear. Por conta disso o perceptron nunca ir\u00e1 encontrar os pesos ideais para o treinamento, n\u00e3o permitindo sua converg\u00eancia.</li> </ul>"},{"location":"exercicio2/main/#codigo_2","title":"C\u00f3digo","text":"<pre><code>mean0 = np.array([3.0, 3.0])\ncov0  = np.array([[1.5, 0.0],[0.0, 1.5]])\n\nmean1 = np.array([4.0, 4.0])\ncov1  = np.array([[1.5, 0.0],[0.0, 1.5]])\n\nn_per_class = 1000\n\nX0 = rng.multivariate_normal(mean0, cov0, size=n_per_class)\nX1 = rng.multivariate_normal(mean1, cov1, size=n_per_class)\n\nX = np.vstack([X0, X1])\ny01 = np.hstack([np.zeros(n_per_class, dtype=int), np.ones(n_per_class, dtype=int)])\ny = np.where(y01 == 1, 1, -1)\n\nplt.figure()\nplt.scatter(X0[:, 0], X0[:, 1], label=\"Class 0\")\nplt.scatter(X1[:, 0], X1[:, 1], label=\"Class 1\")\nplt.title(\"Exercise 2: Generated Data\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>clf = Perceptron(lr=0.1, max_epochs=100, random_state=123)\nlog = clf.fit(X, y)\n\ny_pred = clf.predict(X)\nacc = np.mean(y_pred == y)\n\nprint(\"Epochs run:\", log[\"epochs_run\"])\nprint(\"Converged:\", log[\"converged\"])\nprint(\"Final weights:\", clf.w)\nprint(\"Final bias:\", clf.b)\nprint(\"Final accuracy:\", acc)\n</code></pre>"},{"location":"exercicio3/main/","title":"Exercicio 3","text":""},{"location":"exercicio3/main/#deep-learning-multi-layer-perceptron-mlp","title":"Deep Learning \u2014 Multi-Layer Perceptron (MLP)","text":"<p>Autor: Caio Ortega Boa Disciplina: Deep Learning Per\u00edodo: 2025.1 Link do Reposit\u00f3rio</p>"},{"location":"exercicio3/main/#sumario","title":"Sum\u00e1rio","text":"<ul> <li>Exerc\u00edcio 1 C\u00e1lculo Manual de uma MLP</li> <li>MLP: rede com camadas ocultas <code>tanh</code> e sa\u00edda <code>softmax</code> </li> <li>Exerc\u00edcio 2: Classifica\u00e7\u00e3o bin\u00e1ria  </li> <li>Exerc\u00edcio 3: Classifica\u00e7\u00e3o multiclasse </li> <li>Exerc\u00edcio 4: Classifica\u00e7\u00e3o multiclasse (+ camadas ocultas)</li> </ul>"},{"location":"exercicio3/main/#exercicio-1-calculo-manual-de-uma-mlp","title":"Exerc\u00edcio 1 \u2014 C\u00e1lculo Manual de uma MLP","text":"<p>Descri\u00e7\u00e3o do modelo. MLP com 2 entradas, 1 camada oculta (2 neur\u00f4nios) e 1 neur\u00f4nio de sa\u00edda. Ativa\u00e7\u00f5es: <code>tanh</code> na oculta e na sa\u00edda. Loss: MSE.</p>"},{"location":"exercicio3/main/#1-dados-do-problema-preencher","title":"1) Dados do problema (preencher)","text":"<ul> <li>Entrada (vetor coluna): x = [ 0.5 , 0,2]</li> <li>Sa\u00edda desejada: y = 1.0 </li> <li>Pesos da camada oculta W1 = [[0.3 , -0.1][0.2, 0.4]]</li> <li>Vieses da camada oculta b1 = [0.1 , -0.2]</li> <li>Pesos da sa\u00edda W2 = [0.5, -0.3]</li> <li>Vi\u00e9s da sa\u00edda b2 = 0.2 </li> <li>Taxa de aprendizado: eta = 0.3</li> </ul>"},{"location":"exercicio3/main/#2-forward-pass","title":"2) Forward pass","text":"<ul> <li>Pr\u00e9-ativa\u00e7\u00f5es na oculta: z1 = W1 @ x + b1 = [0.27, -0.18]</li> <li>Ativa\u00e7\u00f5es na oculta: a1 = tanh(z1) = [0.2636, -0.1780]</li> <li>Pr\u00e9-ativa\u00e7\u00e3o na sa\u00edda: z2 = W2 @ a1 + b2 = 0.3852  </li> <li>Sa\u00edda da rede: y^ = tanh(z2) = 0.3672</li> </ul>"},{"location":"exercicio3/main/#3-loss-mse","title":"3) Loss (MSE)","text":"<ul> <li>L = (y - y<sup>)</sup>2 = 0.4003</li> </ul>"},{"location":"exercicio3/main/#4-backward-pass","title":"4) Backward pass","text":"<ul> <li>dl/dy^ = -2*(y - y^) = -1.2655  </li> <li>dy^/dz2 = 1 - tanh(z2)^2 = 0.8651</li> <li>dl/dz2 = -1.0948</li> <li>dL/dW2 = dl/dz2 * a1^T = [-0.2886, 0.1949]</li> <li>dL/db2 = dl/dz2 = -1.0948</li> <li>dL/da1 = W2^T * dl/dz2 = [-0.5474, 0.3180]</li> <li>da1/dz1 = 1 - tanh(z1)^2 = [0.9305, 0.9682]</li> <li>dL/dz1 = (W2^T*dl/dz2) * (1 - tanh(z1)^2) = [-0.5093, 0.3180]</li> <li>dL/dW1 = dL/dz1 * x^T = [[-0.2546, 0.1018][0.1590, -0.0636]]</li> <li>dL/db1 = dL/dz1 = [-0.5093, 0.3180]</li> </ul>"},{"location":"exercicio3/main/#5-atualizacao-dos-parametros-gradient-descent","title":"5) Atualiza\u00e7\u00e3o dos par\u00e2metros (Gradient Descent)","text":"<ul> <li>W2 = W2 - eta * dL/dW2 = [0.5865, -0.3584]</li> <li>b2 = b2 - eta * dL/db2 = 0.5284</li> <li>W1 = W1 - eta * dl/dW1 = [[0.3764, -0.1305][0.1522, 0.4190]]</li> <li>b1 = b1 - eta * dL/db1 = [0.2528, -0.2954]  </li> </ul>"},{"location":"exercicio3/main/#mlp","title":"MLP","text":"<p>Objetivo. Implementar uma MLP modular em NumPy, suportando: - Entrada gen\u00e9rica (<code>input_dim = n_features</code>); - N camadas ocultas (lista de larguras), ativa\u00e7\u00e3o <code>tanh</code> em todas as ocultas; - Camada de sa\u00edda <code>softmax</code> com <code>output_dim = n_classes</code>; - Loss: categorical cross-entropy; - Otimiza\u00e7\u00e3o: Gradient Descent;.</p> <p>Fluxo do treino. 1. Inicializa\u00e7\u00e3o dos par\u00e2metros; 2. Forward; 3. Loss; 4. Backward; 5. Gradient Descent;  </p> <pre><code># mlp.py\nfrom __future__ import annotations\nfrom typing import List, Optional\nimport numpy as np\n\nfrom utils import (\n    tanh, dtanh_from_a,\n    softmax, one_hot, cross_entropy, accuracy_score,\n    xavier_init,\n)\n\nclass MLP:\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_layers: List[int] = [16, 16],\n        output_dim: int = 2,        \n        lr: float = 0.05,\n        max_epochs: int = 500,\n        batch_size: Optional[int] = None,\n        random_state: Optional[int] = 42,\n        track_history: bool = True,\n    ):\n        self.input_dim = input_dim\n        self.hidden_layers = hidden_layers\n        self.output_dim = output_dim\n        self.lr = lr\n        self.max_epochs = max_epochs\n        self.batch_size = batch_size\n        self.random_state = random_state\n        self.track_history = track_history\n\n        self.params_ = None\n        self.loss_history_: List[float] = []\n        self.acc_history_: List[float] = []\n\n    # ---------- initialization ----------\n    def _init_params(self, rng: np.random.Generator) -&gt; None:\n        layer_sizes = [self.input_dim] + self.hidden_layers + [self.output_dim]\n        W, b = [], []\n        for l in range(1, len(layer_sizes)):\n            fan_in = layer_sizes[l-1]\n            fan_out = layer_sizes[l]\n            W_l = xavier_init(fan_in, fan_out, rng)\n            b_l = np.zeros((fan_out, 1))\n            W.append(W_l)\n            b.append(b_l)\n        self.params_ = {\"W\": W, \"b\": b}\n\n    # ---------- forward ----------\n    def _forward(self, X: np.ndarray):\n        W, B = self.params_[\"W\"], self.params_[\"b\"]\n        A = X.T  \n        caches = [{\"A\": A}]  \n\n        # hidden layers\n        for l in range(len(self.hidden_layers)):\n            Z = W[l] @ A + B[l]\n            A = tanh(Z)\n            caches.append({\"Z\": Z, \"A\": A})\n\n        # output layer (softmax)\n        ZL = W[-1] @ A + B[-1]\n        P = softmax(ZL, axis=0)\n        caches.append({\"Z\": ZL, \"A\": P})\n        return caches, P.T\n\n    # ---------- backward ----------\n    def _backward(self, caches, y: np.ndarray):\n        W = self.params_[\"W\"]\n        L = len(W)\n        m = y.shape[0]\n\n        A0 = caches[0][\"A\"]\n        A_list = [A0] + [c[\"A\"] for c in caches[1:]]\n\n        Y = one_hot(y.reshape(-1), self.output_dim).T\n        P = A_list[-1]\n\n        dZ = (P - Y) / m\n        dW = [None] * L\n        dB = [None] * L\n\n        # \u00faltima camada\n        A_prev = A_list[-2]\n        dW[L-1] = dZ @ A_prev.T\n        dB[L-1] = np.sum(dZ, axis=1, keepdims=True)\n\n        # ocultas\n        for l in reversed(range(L-1)):\n            dA = W[l+1].T @ dZ\n            A_l = A_list[l+1]\n            dZ = dA * dtanh_from_a(A_l)\n\n            A_prev = A_list[l]\n            dW[l] = dZ @ A_prev.T\n            dB[l] = np.sum(dZ, axis=1, keepdims=True)\n\n        return dW, dB\n\n    # ---------- update ----------\n    def _update(self, dW, dB, lr: float) -&gt; None:\n        for l in range(len(self.params_[\"W\"])):\n            self.params_[\"W\"][l] -= lr * dW[l]\n            self.params_[\"b\"][l] -= lr * dB[l]\n\n    # ---------- fit ----------\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        rng = np.random.default_rng(self.random_state)\n        self._init_params(rng)\n\n        m = X.shape[0]\n        batch_size = self.batch_size or m\n\n        for epoch in range(1, self.max_epochs + 1):\n            idx = rng.permutation(m)\n            X_shuf = X[idx]\n            y_shuf = y[idx]\n\n            for start in range(0, m, batch_size):\n                end = min(start + batch_size, m)\n                Xb = X_shuf[start:end]\n                yb = y_shuf[start:end]\n\n                caches, _ = self._forward(Xb)\n                dW, dB = self._backward(caches, yb)\n                self._update(dW, dB, self.lr)\n\n            if self.track_history:\n                P_full = self.predict_proba(X)\n                Y_full = one_hot(y, self.output_dim)\n                loss = cross_entropy(Y_full, P_full)\n                y_pred = np.argmax(P_full, axis=1)\n                acc = accuracy_score(y, y_pred)\n                self.loss_history_.append(loss)\n                self.acc_history_.append(acc)\n\n        return {\"epochs_run\": self.max_epochs}\n\n    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n        _, P = self._forward(X)\n        return P\n\n    def decision_function(self, X: np.ndarray) -&gt; np.ndarray:\n        W, B = self.params_[\"W\"], self.params_[\"b\"]\n        A = X.T\n        for l in range(len(self.hidden_layers)):\n            A = tanh(W[l] @ A + B[l])\n        ZL = W[-1] @ A + B[-1]\n        return ZL.T\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        P = self.predict_proba(X)\n        return np.argmax(P, axis=1)\n</code></pre> <p>Fun\u00e7\u00f5es Aux\u00edliares </p><pre><code># utils.py\nfrom __future__ import annotations\nimport numpy as np\n\n# -----------------------------\n# Ativa\u00e7\u00f5es e derivadas\n# -----------------------------\ndef tanh(z: np.ndarray) -&gt; np.ndarray:\n    return np.tanh(z)\n\ndef dtanh_from_a(a: np.ndarray) -&gt; np.ndarray:\n    return 1.0 - a**2\n\ndef softmax(Z: np.ndarray, axis: int = 0) -&gt; np.ndarray:\n    \"\"\"\n    Z: (K, m) -&gt; aplica softmax por coluna (axis=0).\n    Retorna prob. por classe, colunas somam 1.\n    \"\"\"\n    Z_shift = Z - np.max(Z, axis=axis, keepdims=True)\n    e = np.exp(Z_shift)\n    return e / np.sum(e, axis=axis, keepdims=True)\n\n# -----------------------------\n# Loss e m\u00e9tricas\n# -----------------------------\ndef bce_loss(y_true: np.ndarray, y_prob: np.ndarray, eps: float = 1e-12) -&gt; float:\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return float(-np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob)))\n\ndef cross_entropy(y_true_oh: np.ndarray, y_prob: np.ndarray, eps: float = 1e-12) -&gt; float:\n    \"\"\"\n    y_true_oh: (m, K) one-hot\n    y_prob   : (m, K) probabilidades (softmax)\n    \"\"\"\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return float(-np.mean(np.sum(y_true_oh * np.log(y_prob), axis=1)))\n\ndef accuracy_score(y_true: np.ndarray, y_pred_labels: np.ndarray) -&gt; float:\n    return float(np.mean(y_true == y_pred_labels))\n\n# -----------------------------\n# Split 80/20\n# -----------------------------\ndef train_test_split(X: np.ndarray, y: np.ndarray, test_size: float = 0.2, random_state: int = 42):\n    rng = np.random.default_rng(random_state)\n    m = X.shape[0]\n    idx = rng.permutation(m)\n    m_test = int(np.floor(test_size * m))\n    test_idx = idx[:m_test]\n    train_idx = idx[m_test:]\n    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n\n# -----------------------------\n# Inicializa\u00e7\u00f5es\n# -----------------------------\ndef xavier_init(fan_in: int, fan_out: int, rng: np.random.Generator) -&gt; np.ndarray:\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n    return rng.normal(0.0, std, size=(fan_out, fan_in))\n\n# -----------------------------\n# Helpers\n# -----------------------------\ndef one_hot(y: np.ndarray, K: int) -&gt; np.ndarray:\n    \"\"\"\n    y: (m,) com r\u00f3tulos inteiros [0..K-1]\n    retorna: (m, K) one-hot\n    \"\"\"\n    m = y.shape[0]\n    Y = np.zeros((m, K), dtype=float)\n    Y[np.arange(m), y.astype(int)] = 1.0\n    return Y\n</code></pre><p></p> <p>Pr\u00e9-processamento. - MinMax [-1, 1] nos atributos, para combinar com ativa\u00e7\u00e3o tanh nas ocultas.</p> <pre><code>from __future__ import annotations\nimport numpy as np\n\nclass MinMaxScaler:\n    \"\"\"\n    Escala os dados para o intervalo [-1, 1].\n    \"\"\"\n    def __init__(self):\n        self.min_: np.ndarray | None = None\n        self.max_: np.ndarray | None = None\n\n    def fit(self, X: np.ndarray) -&gt; \"MinMaxScaler\":\n        self.min_ = X.min(axis=0)\n        self.max_ = X.max(axis=0)\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        if self.min_ is None or self.max_ is None:\n            raise RuntimeError(\"Scaler n\u00e3o ajustado. Chame fit() antes de transform().\")\n\n        # normaliza para [0, 1]\n        X_norm = (X - self.min_) / (self.max_ - self.min_ + 1e-12)\n        # reescala para [-1, 1]\n        return 2.0 * X_norm - 1.0\n\n    def fit_transform(self, X: np.ndarray) -&gt; np.ndarray:\n        return self.fit(X).transform(X)\n</code></pre> <p>Gera\u00e7\u00e3o de Dados </p><pre><code>from __future__ import annotations\nimport numpy as np\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\n\ndef make_varying_classification(\n    n_samples: int,\n    n_classes: int,\n    n_features: int,\n    clusters_per_class,\n    class_sep: float = 1.2,           \n    flip_y: float = 0.2,\n    random_state: int = 42,\n    shuffle: bool = False,\n):\n    \"\"\"\n    Gera dados sint\u00e9ticos com n\u00ba de clusters vari\u00e1vel por classe,\n    usando make_classification de forma simplificada.\n\n    - Divide amostras de forma uniforme entre as classes\n    - n_informative = n_features\n    - n_redundant = 0\n    - class_sep e flip_y ajust\u00e1veis\n    \"\"\"\n    clusters_per_class = list(clusters_per_class)\n    if n_classes &lt; 2 or len(clusters_per_class) != n_classes:\n        raise ValueError(\"clusters_per_class deve ter n_classes elementos e n_classes &gt;= 2.\")\n\n    # Divide amostras uniformemente\n    base = n_samples // n_classes\n    counts = [base] * n_classes\n    for i in range(n_samples - base * n_classes):\n        counts[i] += 1\n\n    X_parts, y_parts = [], []\n    for c, m_c in enumerate(counts):\n        seed_c = (random_state + 10007 * (c + 1)) % (2**31 - 1)\n        X_c, _ = make_classification(\n            n_samples=m_c,\n            n_features=n_features,\n            n_informative=n_features,\n            n_redundant=0,\n            n_repeated=0,\n            n_classes=2,               \n            n_clusters_per_class=clusters_per_class[c],\n            weights=[1.0, 0.0],        \n            class_sep=class_sep,\n            flip_y=flip_y,\n            shuffle=True,\n            random_state=seed_c,\n        )\n        X_parts.append(X_c)\n        y_parts.append(np.full(m_c, c, dtype=int))\n\n    X = np.vstack(X_parts)\n    y = np.concatenate(y_parts)\n\n    if shuffle:\n        rng = np.random.default_rng(random_state)\n        idx = rng.permutation(X.shape[0])\n        X, y = X[idx], y[idx]\n\n    return X, y\n\ndef plot_classification_data(X: np.ndarray, y: np.ndarray, title: str = \"Synthetic Data\"):\n    \"\"\"\n    Plota os dados 2D gerados por make_varying_classification.\n\n    Par\u00e2metros:\n      X : np.ndarray (n_samples, 2) -&gt; features\n      y : np.ndarray (n_samples,)   -&gt; r\u00f3tulos (0, 1, ..., n_classes-1)\n      title : t\u00edtulo opcional do gr\u00e1fico\n    \"\"\"\n    if X.shape[1] != 2:\n        raise ValueError(\"O plot s\u00f3 funciona para n_features=2.\")\n\n    plt.figure(figsize=(6, 6))\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"tab10\", edgecolor=\"k\", s=40, alpha=0.8)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.title(title)\n    plt.legend(*scatter.legend_elements(), title=\"Classes\")\n    plt.grid(True, linestyle=\"--\", alpha=0.6)\n    plt.show()\n</code></pre><p></p>"},{"location":"exercicio3/main/#exercicio-2-classificacao-binaria","title":"Exerc\u00edcio 2 \u2014 Classifica\u00e7\u00e3o Bin\u00e1ria","text":"<p>Objetivo. Treinar uma MLP em um conjunto de dados bin\u00e1rios, com clusters assim\u00e9tricos e 2 features.</p>"},{"location":"exercicio3/main/#especificacao-dos-dados","title":"Especifica\u00e7\u00e3o dos Dados","text":"<ul> <li>Amostras: 1000  </li> <li>Classes: 2  </li> <li>Features: 2 </li> <li>Clusters por classe: <code>[2, 1]</code></li> </ul>"},{"location":"exercicio3/main/#resultados","title":"Resultados","text":"<p>Foram realizados testes com 1 camada oculta de profundidade 1, 1 camada oculta de profundidade 16 e 2 camadas ocultas de profundidade 16 para 500 \u00e9pocas.</p> <ul> <li>Train Loss: 0.4671, 0.4634, 0.2879  </li> <li>Train Accuracy: 0.7750, 0.7738, 0.8538  </li> <li>Test Loss: 0.4553, 0.4509, 0.2763</li> <li>Test Accuracy: 0.8000, 0.8050, 0.8500</li> </ul> <p> </p>"},{"location":"exercicio3/main/#exercicio-3-4-classificacao-multiclasse","title":"Exerc\u00edcio 3 / 4 \u2014 Classifica\u00e7\u00e3o Multiclasse","text":"<p>Objetivo. Treinar uma MLP em um conjunto de dados com 3 classes distintas, clusters assim\u00e9tricos e 4 features.</p>"},{"location":"exercicio3/main/#especificacao-dos-dados_1","title":"Especifica\u00e7\u00e3o dos Dados","text":"<ul> <li>Amostras: 1500  </li> <li>Classes: 3  </li> <li>Features: 4  </li> <li>Clusters por classe: <code>[2, 3, 4]</code> </li> </ul>"},{"location":"exercicio3/main/#resultados_1","title":"Resultados","text":"<p>Foram realizados testes com 1 camada oculta de profundidade 16, 2 camada oculta de profundidade 16 e 3 camadas ocultas de profundidade 16 para 500 \u00e9pocas.</p> <ul> <li>Train Loss: 0.7644, 0.5351, 0.4725  </li> <li>Train Accuracy: 0.6142, 0.7625, 0.7817  </li> <li>Test Loss: 0.7889, 0.6355, 0.5803</li> <li>Test Accuracy: 0.5933, 0.6767, 0.7200</li> </ul> <p> </p>"}]}